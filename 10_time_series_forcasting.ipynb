{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 forcasting time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import urllib.request\n",
    "import random\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import string\n",
    "\n",
    "from helper_functions import create_tensorboard_callback, unzip_data, plot_loss_curves, compare_historys, calculate_results, plot_time_series\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras\n",
    "from keras.layers import TextVectorization\n",
    "from keras import layers\n",
    "from keras.layers import preprocessing\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\"\n",
    "# filename= \"BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\"\n",
    "# file = urllib.request.urlretrieve(url,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Currency</th>\n",
       "      <th>Closing Price (USD)</th>\n",
       "      <th>24h Open (USD)</th>\n",
       "      <th>24h High (USD)</th>\n",
       "      <th>24h Low (USD)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-10-01</th>\n",
       "      <td>BTC</td>\n",
       "      <td>123.65499</td>\n",
       "      <td>124.30466</td>\n",
       "      <td>124.75166</td>\n",
       "      <td>122.56349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-02</th>\n",
       "      <td>BTC</td>\n",
       "      <td>125.45500</td>\n",
       "      <td>123.65499</td>\n",
       "      <td>125.75850</td>\n",
       "      <td>123.63383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-03</th>\n",
       "      <td>BTC</td>\n",
       "      <td>108.58483</td>\n",
       "      <td>125.45500</td>\n",
       "      <td>125.66566</td>\n",
       "      <td>83.32833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-04</th>\n",
       "      <td>BTC</td>\n",
       "      <td>118.67466</td>\n",
       "      <td>108.58483</td>\n",
       "      <td>118.67500</td>\n",
       "      <td>107.05816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-05</th>\n",
       "      <td>BTC</td>\n",
       "      <td>121.33866</td>\n",
       "      <td>118.67466</td>\n",
       "      <td>121.93633</td>\n",
       "      <td>118.00566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Currency  Closing Price (USD)  24h Open (USD)  24h High (USD)  \\\n",
       "Date                                                                       \n",
       "2013-10-01      BTC            123.65499       124.30466       124.75166   \n",
       "2013-10-02      BTC            125.45500       123.65499       125.75850   \n",
       "2013-10-03      BTC            108.58483       125.45500       125.66566   \n",
       "2013-10-04      BTC            118.67466       108.58483       118.67500   \n",
       "2013-10-05      BTC            121.33866       118.67466       121.93633   \n",
       "\n",
       "            24h Low (USD)  \n",
       "Date                       \n",
       "2013-10-01      122.56349  \n",
       "2013-10-02      123.63383  \n",
       "2013-10-03       83.32833  \n",
       "2013-10-04      107.05816  \n",
       "2013-10-05      118.00566  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/BTC/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\",\n",
    "                 parse_dates=[\"Date\"],\n",
    "                 index_col=[\"Date\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-10-01</th>\n",
       "      <td>123.654990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-02</th>\n",
       "      <td>125.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-03</th>\n",
       "      <td>108.584830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-04</th>\n",
       "      <td>118.674660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-05</th>\n",
       "      <td>121.338660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-14</th>\n",
       "      <td>49764.132082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-15</th>\n",
       "      <td>50032.693137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-16</th>\n",
       "      <td>47885.625255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-17</th>\n",
       "      <td>45604.615754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-18</th>\n",
       "      <td>43144.471291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2787 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Price\n",
       "Date                    \n",
       "2013-10-01    123.654990\n",
       "2013-10-02    125.455000\n",
       "2013-10-03    108.584830\n",
       "2013-10-04    118.674660\n",
       "2013-10-05    121.338660\n",
       "...                  ...\n",
       "2021-05-14  49764.132082\n",
       "2021-05-15  50032.693137\n",
       "2021-05-16  47885.625255\n",
       "2021-05-17  45604.615754\n",
       "2021-05-18  43144.471291\n",
       "\n",
       "[2787 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin_prices = pd.DataFrame(\n",
    "    df[\"Closing Price (USD)\"]).rename(columns={\"Closing Price (USD)\": \"Price\"})\n",
    "bitcoin_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='Date'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bitcoin_prices.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating train and test sets for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([123.65499, 125.455  , 108.58483, 118.67466, 121.33866, 120.65533,\n",
       "       121.795  , 123.033  , 124.049  , 125.96116])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps = bitcoin_prices.index.to_numpy()\n",
    "btc_prices = bitcoin_prices.Price.to_numpy()\n",
    "timesteps[:10]\n",
    "btc_prices[:10]\n",
    "# fyi i think he doesnt see that btc_prices has extra whitespace..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2229, 558)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_size = int(0.8 * len(btc_prices))\n",
    "X_train, y_train = timesteps[:split_size], btc_prices[:split_size]\n",
    "\n",
    "X_test, y_test = timesteps[split_size:], btc_prices[split_size:]\n",
    "\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train, y_train, s=5, label=\"train data\")\n",
    "plt.scatter(X_test, y_test, s=5, label=\"train data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_time_series(X_train, y_train, label=\"train data\")\n",
    "plot_time_series(X_test, y_test, label=\"test data\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MASE funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASE implemented courtesy of sktime - https://github.com/alan-turing-institute/sktime/blob/ee7a06843a44f4aaec7582d847e36073a9ab0566/sktime/performance_metrics/forecasting/_functions.py#L16\n",
    "def mean_absolute_scaled_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "  Implement MASE (assuming no seasonality of data).\n",
    "  \"\"\"\n",
    "    mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "    # Find MAE of naive forecast (no seasonality)\n",
    "    mae_naive_no_season = tf.reduce_mean(tf.abs(\n",
    "        y_true[1:] -\n",
    "        y_true[:-1]))  # our seasonality is 1 day (hence the shifting of 1 day)\n",
    "\n",
    "    return mae / mae_naive_no_season\n",
    "\n",
    "\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    # Make sure float32 (for metric calculations)\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    # Calculate various metrics\n",
    "    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = tf.keras.metrics.mean_squared_error(\n",
    "        y_true,\n",
    "        y_pred)  # puts and emphasis on outliers (all errors get squared)\n",
    "    rmse = tf.sqrt(mse)\n",
    "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mase = mean_absolute_scaled_error(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"mae\": mae.numpy(),\n",
    "        \"mse\": mse.numpy(),\n",
    "        \"rmse\": rmse.numpy(),\n",
    "        \"mape\": mape.numpy(),\n",
    "        \"mase\": mase.numpy()\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## formating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 1\n",
    "WINDOW_SIZE = 7\n",
    "\n",
    "\n",
    "# Create function to label windowed data\n",
    "def get_labelled_windows(x, horizon=1):\n",
    "    \"\"\"\n",
    "  Creates labels for windowed dataset.\n",
    "\n",
    "  E.g. if horizon=1 (default)\n",
    "  Input: [1, 2, 3, 4, 5, 6] -> Output: ([1, 2, 3, 4, 5], [6])\n",
    "  \"\"\"\n",
    "    return x[:, :-horizon], x[:, -horizon:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 17:24:20.055756: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 2, 3, 4, 5, 6, 7]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[8]], dtype=int32)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_window, test_label = get_labelled_windows(\n",
    "    tf.expand_dims(tf.range(8) + 1, axis=0))\n",
    "test_window, test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to view NumPy arrays as windows\n",
    "def make_windows(x, window_size=7, horizon=1):\n",
    "    \"\"\"\n",
    "  Turns a 1D array into a 2D array of sequential windows of window_size.\n",
    "  \"\"\"\n",
    "    # 1. Create a window of specific window_size (add the horizon on the end for later labelling)\n",
    "    window_step = np.expand_dims(np.arange(window_size + horizon), axis=0)\n",
    "    # print(f\"Window step:\\n {window_step}\")\n",
    "\n",
    "    # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)\n",
    "    window_indexes = window_step + np.expand_dims(\n",
    "        np.arange(len(x) - (window_size + horizon - 1)),\n",
    "        axis=0).T  # create 2D array of windows of size window_size\n",
    "    # print(f\"Window indexes:\\n {window_indexes[:3], window_indexes[-3:], window_indexes.shape}\")\n",
    "\n",
    "    # 3. Index on the target array (time series) with 2D array of multiple window steps\n",
    "    windowed_array = x[window_indexes]\n",
    "\n",
    "    # 4. Get the labelled windows\n",
    "    windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
    "\n",
    "    return windows, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2780, 2780)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_windows, full_labels = make_windows(btc_prices)\n",
    "len(full_labels), len(full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123.65499 125.455   108.58483 118.67466 121.33866 120.65533 121.795  ] [123.033]\n",
      "[125.455   108.58483 118.67466 121.33866 120.65533 121.795   123.033  ] [124.049]\n",
      "[108.58483 118.67466 121.33866 120.65533 121.795   123.033   124.049  ] [125.96116]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(full_windows[i], full_labels[i])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## turning windows into train and test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below cell is dumb... you dont need this theres built in functions for this already...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the train/test splits\n",
    "def make_train_test_splits(windows, labels, test_split=0.2):\n",
    "    \"\"\"\n",
    "  Splits matching pairs of windows and labels into train and test splits.\n",
    "  \"\"\"\n",
    "    split_size = int(\n",
    "        len(windows) *\n",
    "        (1 - test_split))  # this will default to 80% train/20% test\n",
    "    train_windows = windows[:split_size]\n",
    "    train_labels = labels[:split_size]\n",
    "    test_windows = windows[split_size:]\n",
    "    test_labels = labels[split_size:]\n",
    "    return train_windows, test_windows, train_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(\n",
    "    full_windows, full_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([125.455  , 108.58483, 118.67466, 121.33866, 120.65533, 121.795  ,\n",
       "       123.033  ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_windows[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making modeling checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to implement a ModelCheckpoint callback with a specific filename\n",
    "def create_model_checkpoint(model_name, save_path=\"./saves/10/\"):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=save_path + \"/\" + model_name + \"/\" +\n",
    "        model_name,  # create filepath to save model\n",
    "        save_weights_only=True,\n",
    "        verbose=1,  # only output a limited amount of text\n",
    "        save_best_only=True)  # save only the best model to file\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_1 dense model window=7 horizon=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 1480.9569 - mae: 1480.9569\n",
      "Epoch 1: val_loss improved from inf to 2864.56836, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 2s 36ms/step - loss: 1072.0831 - mae: 1072.0831 - val_loss: 2864.5684 - val_mae: 2864.5684\n",
      "Epoch 2/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 329.6384 - mae: 329.6384\n",
      "Epoch 2: val_loss improved from 2864.56836 to 1169.84692, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 310.5162 - mae: 310.5162 - val_loss: 1169.8469 - val_mae: 1169.8469\n",
      "Epoch 3/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 148.3363 - mae: 148.3363\n",
      "Epoch 3: val_loss did not improve from 1169.84692\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 220.8676 - mae: 220.8676 - val_loss: 1181.7379 - val_mae: 1181.7379\n",
      "Epoch 4/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 262.2178 - mae: 262.2178\n",
      "Epoch 4: val_loss improved from 1169.84692 to 1110.31165, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 206.9082 - mae: 206.9082 - val_loss: 1110.3116 - val_mae: 1110.3116\n",
      "Epoch 5/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 216.0562 - mae: 216.0562\n",
      "Epoch 5: val_loss improved from 1110.31165 to 1080.25647, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 194.3800 - mae: 194.3800 - val_loss: 1080.2565 - val_mae: 1080.2565\n",
      "Epoch 6/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 138.0929 - mae: 138.0929\n",
      "Epoch 6: val_loss improved from 1080.25647 to 1040.06836, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 188.9075 - mae: 188.9075 - val_loss: 1040.0684 - val_mae: 1040.0684\n",
      "Epoch 7/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 192.7822 - mae: 192.7822\n",
      "Epoch 7: val_loss did not improve from 1040.06836\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 181.3098 - mae: 181.3098 - val_loss: 1049.7234 - val_mae: 1049.7234\n",
      "Epoch 8/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 146.9946 - mae: 146.9946\n",
      "Epoch 8: val_loss improved from 1040.06836 to 993.97101, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 178.8671 - mae: 178.8671 - val_loss: 993.9710 - val_mae: 993.9710\n",
      "Epoch 9/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 142.0897 - mae: 142.0897\n",
      "Epoch 9: val_loss did not improve from 993.97101\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 175.2147 - mae: 175.2147 - val_loss: 1007.8943 - val_mae: 1007.8943\n",
      "Epoch 10/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 182.3484 - mae: 182.3484\n",
      "Epoch 10: val_loss improved from 993.97101 to 954.58777, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 171.0658 - mae: 171.0658 - val_loss: 954.5878 - val_mae: 954.5878\n",
      "Epoch 11/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 178.5079 - mae: 178.5079\n",
      "Epoch 11: val_loss improved from 954.58777 to 931.82324, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 166.4776 - mae: 166.4776 - val_loss: 931.8232 - val_mae: 931.8232\n",
      "Epoch 12/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 148.8941 - mae: 148.8941\n",
      "Epoch 12: val_loss did not improve from 931.82324\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 162.8522 - mae: 162.8522 - val_loss: 971.9766 - val_mae: 971.9766\n",
      "Epoch 13/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 164.4052 - mae: 164.4052\n",
      "Epoch 13: val_loss did not improve from 931.82324\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 163.1517 - mae: 163.1517 - val_loss: 958.7745 - val_mae: 958.7745\n",
      "Epoch 14/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 171.8526 - mae: 171.8526\n",
      "Epoch 14: val_loss improved from 931.82324 to 900.54211, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 161.9840 - mae: 161.9840 - val_loss: 900.5421 - val_mae: 900.5421\n",
      "Epoch 15/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 165.6743 - mae: 165.6743\n",
      "Epoch 15: val_loss improved from 900.54211 to 869.22345, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 156.0288 - mae: 156.0288 - val_loss: 869.2234 - val_mae: 869.2234\n",
      "Epoch 16/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 138.7087 - mae: 138.7087\n",
      "Epoch 16: val_loss improved from 869.22345 to 865.95630, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 151.8102 - mae: 151.8102 - val_loss: 865.9563 - val_mae: 865.9563\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 151.2904 - mae: 151.2904\n",
      "Epoch 17: val_loss improved from 865.95630 to 860.19745, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 151.2904 - mae: 151.2904 - val_loss: 860.1974 - val_mae: 860.1974\n",
      "Epoch 18/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 160.0712 - mae: 160.0712\n",
      "Epoch 18: val_loss improved from 860.19745 to 829.31250, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 152.2160 - mae: 152.2160 - val_loss: 829.3125 - val_mae: 829.3125\n",
      "Epoch 19/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 84.2455 - mae: 84.2455\n",
      "Epoch 19: val_loss improved from 829.31250 to 826.10052, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 146.5785 - mae: 146.5785 - val_loss: 826.1005 - val_mae: 826.1005\n",
      "Epoch 20/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 152.0137 - mae: 152.0137\n",
      "Epoch 20: val_loss improved from 826.10052 to 789.61676, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 144.4882 - mae: 144.4882 - val_loss: 789.6168 - val_mae: 789.6168\n",
      "Epoch 21/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 170.0505 - mae: 170.0505\n",
      "Epoch 21: val_loss did not improve from 789.61676\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 145.0762 - mae: 145.0762 - val_loss: 789.8429 - val_mae: 789.8429\n",
      "Epoch 22/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 186.4529 - mae: 186.4529\n",
      "Epoch 22: val_loss did not improve from 789.61676\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 140.3383 - mae: 140.3383 - val_loss: 793.5374 - val_mae: 793.5374\n",
      "Epoch 23/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 142.0635 - mae: 142.0635\n",
      "Epoch 23: val_loss did not improve from 789.61676\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 142.0707 - mae: 142.0707 - val_loss: 798.9828 - val_mae: 798.9828\n",
      "Epoch 24/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 160.5919 - mae: 160.5919\n",
      "Epoch 24: val_loss improved from 789.61676 to 748.52271, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 137.1058 - mae: 137.1058 - val_loss: 748.5227 - val_mae: 748.5227\n",
      "Epoch 25/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 140.8167 - mae: 140.8167\n",
      "Epoch 25: val_loss improved from 748.52271 to 742.48682, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 134.4694 - mae: 134.4694 - val_loss: 742.4868 - val_mae: 742.4868\n",
      "Epoch 26/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 152.8577 - mae: 152.8577\n",
      "Epoch 26: val_loss did not improve from 742.48682\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 140.3569 - mae: 140.3569 - val_loss: 792.7679 - val_mae: 792.7679\n",
      "Epoch 27/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 210.3034 - mae: 210.3034\n",
      "Epoch 27: val_loss improved from 742.48682 to 726.98273, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 137.7043 - mae: 137.7043 - val_loss: 726.9827 - val_mae: 726.9827\n",
      "Epoch 28/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 137.1462 - mae: 137.1462\n",
      "Epoch 28: val_loss did not improve from 726.98273\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 131.8039 - mae: 131.8039 - val_loss: 727.8396 - val_mae: 727.8396\n",
      "Epoch 29/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 126.9727 - mae: 126.9727\n",
      "Epoch 29: val_loss did not improve from 726.98273\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 134.3283 - mae: 134.3283 - val_loss: 742.5387 - val_mae: 742.5387\n",
      "Epoch 30/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 133.6407 - mae: 133.6407\n",
      "Epoch 30: val_loss did not improve from 726.98273\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 135.5005 - mae: 135.5005 - val_loss: 792.0652 - val_mae: 792.0652\n",
      "Epoch 31/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 145.6892 - mae: 145.6892\n",
      "Epoch 31: val_loss did not improve from 726.98273\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 137.0882 - mae: 137.0882 - val_loss: 806.3241 - val_mae: 806.3241\n",
      "Epoch 32/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 131.9913 - mae: 131.9913\n",
      "Epoch 32: val_loss did not improve from 726.98273\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 131.4058 - mae: 131.4058 - val_loss: 727.2214 - val_mae: 727.2214\n",
      "Epoch 33/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 162.3430 - mae: 162.3430\n",
      "Epoch 33: val_loss did not improve from 726.98273\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 126.8670 - mae: 126.8670 - val_loss: 730.3666 - val_mae: 730.3666\n",
      "Epoch 34/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 156.5593 - mae: 156.5593\n",
      "Epoch 34: val_loss improved from 726.98273 to 692.56775, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 128.2992 - mae: 128.2992 - val_loss: 692.5677 - val_mae: 692.5677\n",
      "Epoch 35/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 109.2976 - mae: 109.2976\n",
      "Epoch 35: val_loss improved from 692.56775 to 674.90094, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 125.9683 - mae: 125.9683 - val_loss: 674.9009 - val_mae: 674.9009\n",
      "Epoch 36/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 162.7300 - mae: 162.7300\n",
      "Epoch 36: val_loss did not improve from 674.90094\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 128.3577 - mae: 128.3577 - val_loss: 754.1307 - val_mae: 754.1307\n",
      "Epoch 37/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 167.1469 - mae: 167.1469\n",
      "Epoch 37: val_loss improved from 674.90094 to 658.89288, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 124.8522 - mae: 124.8522 - val_loss: 658.8929 - val_mae: 658.8929\n",
      "Epoch 38/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 112.5567 - mae: 112.5567\n",
      "Epoch 38: val_loss improved from 658.89288 to 654.61859, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 122.4071 - mae: 122.4071 - val_loss: 654.6186 - val_mae: 654.6186\n",
      "Epoch 39/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 120.5464 - mae: 120.5464\n",
      "Epoch 39: val_loss improved from 654.61859 to 651.79150, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 121.1386 - mae: 121.1386 - val_loss: 651.7915 - val_mae: 651.7915\n",
      "Epoch 40/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 120.3487 - mae: 120.3487\n",
      "Epoch 40: val_loss improved from 651.79150 to 643.89514, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 119.9954 - mae: 119.9954 - val_loss: 643.8951 - val_mae: 643.8951\n",
      "Epoch 41/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 113.0480 - mae: 113.0480\n",
      "Epoch 41: val_loss improved from 643.89514 to 642.36823, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 121.1582 - mae: 121.1582 - val_loss: 642.3682 - val_mae: 642.3682\n",
      "Epoch 42/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 123.8860 - mae: 123.8860\n",
      "Epoch 42: val_loss improved from 642.36823 to 635.56946, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 126.0648 - mae: 126.0648 - val_loss: 635.5695 - val_mae: 635.5695\n",
      "Epoch 43/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 98.0462 - mae: 98.0462\n",
      "Epoch 43: val_loss did not improve from 635.56946\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 117.7680 - mae: 117.7680 - val_loss: 643.5865 - val_mae: 643.5865\n",
      "Epoch 44/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 118.1492 - mae: 118.1492\n",
      "Epoch 44: val_loss improved from 635.56946 to 632.18579, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 118.4803 - mae: 118.4803 - val_loss: 632.1858 - val_mae: 632.1858\n",
      "Epoch 45/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 116.2156 - mae: 116.2156\n",
      "Epoch 45: val_loss improved from 632.18579 to 625.12000, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 117.0774 - mae: 117.0774 - val_loss: 625.1200 - val_mae: 625.1200\n",
      "Epoch 46/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 119.3327 - mae: 119.3327\n",
      "Epoch 46: val_loss did not improve from 625.12000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 116.6028 - mae: 116.6028 - val_loss: 635.1314 - val_mae: 635.1314\n",
      "Epoch 47/100\n",
      " 2/18 [==>...........................] - ETA: 1s - loss: 127.2564 - mae: 127.2564\n",
      "Epoch 47: val_loss did not improve from 625.12000\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 118.8597 - mae: 118.8597 - val_loss: 759.5991 - val_mae: 759.5991\n",
      "Epoch 48/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 128.2945 - mae: 128.2945\n",
      "Epoch 48: val_loss improved from 625.12000 to 618.53632, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 117.0659 - mae: 117.0659 - val_loss: 618.5363 - val_mae: 618.5363\n",
      "Epoch 49/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 153.4968 - mae: 153.4968\n",
      "Epoch 49: val_loss improved from 618.53632 to 613.68085, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 116.1795 - mae: 116.1795 - val_loss: 613.6808 - val_mae: 613.6808\n",
      "Epoch 50/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 137.7577 - mae: 137.7577\n",
      "Epoch 50: val_loss did not improve from 613.68085\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 119.0951 - mae: 119.0951 - val_loss: 662.9153 - val_mae: 662.9153\n",
      "Epoch 51/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 103.8338 - mae: 103.8338\n",
      "Epoch 51: val_loss did not improve from 613.68085\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 115.5859 - mae: 115.5859 - val_loss: 614.8344 - val_mae: 614.8344\n",
      "Epoch 52/100\n",
      " 8/18 [============>.................] - ETA: 0s - loss: 116.2778 - mae: 116.2778\n",
      "Epoch 52: val_loss did not improve from 613.68085\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 114.5797 - mae: 114.5797 - val_loss: 617.1028 - val_mae: 617.1028\n",
      "Epoch 53/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 114.6784 - mae: 114.6784\n",
      "Epoch 53: val_loss did not improve from 613.68085\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 116.9908 - mae: 116.9908 - val_loss: 630.9307 - val_mae: 630.9307\n",
      "Epoch 54/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 123.6946 - mae: 123.6946\n",
      "Epoch 54: val_loss did not improve from 613.68085\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 123.1606 - mae: 123.1606 - val_loss: 744.7484 - val_mae: 744.7484\n",
      "Epoch 55/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 153.1590 - mae: 153.1590\n",
      "Epoch 55: val_loss improved from 613.68085 to 603.81525, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 117.3389 - mae: 117.3389 - val_loss: 603.8152 - val_mae: 603.8152\n",
      "Epoch 56/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 109.8947 - mae: 109.8947\n",
      "Epoch 56: val_loss did not improve from 603.81525\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 112.5389 - mae: 112.5389 - val_loss: 800.5353 - val_mae: 800.5353\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 130.3298 - mae: 130.3298\n",
      "Epoch 57: val_loss did not improve from 603.81525\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 130.3298 - mae: 130.3298 - val_loss: 684.1401 - val_mae: 684.1400\n",
      "Epoch 58/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 99.7992 - mae: 99.7992\n",
      "Epoch 58: val_loss improved from 603.81525 to 595.44592, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 116.7921 - mae: 116.7921 - val_loss: 595.4459 - val_mae: 595.4459\n",
      "Epoch 59/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 109.6013 - mae: 109.6013\n",
      "Epoch 59: val_loss improved from 595.44592 to 593.74792, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 113.1199 - mae: 113.1199 - val_loss: 593.7479 - val_mae: 593.7479\n",
      "Epoch 60/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 111.7136 - mae: 111.7136\n",
      "Epoch 60: val_loss did not improve from 593.74792\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 111.0139 - mae: 111.0139 - val_loss: 627.1496 - val_mae: 627.1496\n",
      "Epoch 61/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 113.1488 - mae: 113.1488\n",
      "Epoch 61: val_loss improved from 593.74792 to 591.10455, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 112.0132 - mae: 112.0132 - val_loss: 591.1046 - val_mae: 591.1046\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 108.9394 - mae: 108.9394\n",
      "Epoch 62: val_loss improved from 591.10455 to 590.02319, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 108.9394 - mae: 108.9394 - val_loss: 590.0232 - val_mae: 590.0232\n",
      "Epoch 63/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 111.1031 - mae: 111.1031\n",
      "Epoch 63: val_loss did not improve from 590.02319\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 111.0107 - mae: 111.0107 - val_loss: 594.0451 - val_mae: 594.0451\n",
      "Epoch 64/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 91.5219 - mae: 91.5219\n",
      "Epoch 64: val_loss did not improve from 590.02319\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.5979 - mae: 109.5979 - val_loss: 616.4185 - val_mae: 616.4185\n",
      "Epoch 65/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 130.0057 - mae: 130.0057\n",
      "Epoch 65: val_loss did not improve from 590.02319\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 113.5300 - mae: 113.5300 - val_loss: 600.8667 - val_mae: 600.8667\n",
      "Epoch 66/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 108.1314 - mae: 108.1314\n",
      "Epoch 66: val_loss improved from 590.02319 to 582.74762, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 110.6634 - mae: 110.6634 - val_loss: 582.7476 - val_mae: 582.7476\n",
      "Epoch 67/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 108.0609 - mae: 108.0609\n",
      "Epoch 67: val_loss did not improve from 582.74762\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 107.3947 - mae: 107.3947 - val_loss: 614.4897 - val_mae: 614.4897\n",
      "Epoch 68/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 103.6216 - mae: 103.6216\n",
      "Epoch 68: val_loss did not improve from 582.74762\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 113.7425 - mae: 113.7425 - val_loss: 630.6083 - val_mae: 630.6083\n",
      "Epoch 69/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 126.5624 - mae: 126.5624\n",
      "Epoch 69: val_loss did not improve from 582.74762\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 128.9089 - mae: 128.9089 - val_loss: 665.5047 - val_mae: 665.5047\n",
      "Epoch 70/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 127.1333 - mae: 127.1333\n",
      "Epoch 70: val_loss improved from 582.74762 to 582.10980, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 126.1567 - mae: 126.1567 - val_loss: 582.1098 - val_mae: 582.1098\n",
      "Epoch 71/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 86.1032 - mae: 86.1032\n",
      "Epoch 71: val_loss did not improve from 582.10980\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 111.9837 - mae: 111.9837 - val_loss: 592.0446 - val_mae: 592.0446\n",
      "Epoch 72/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 99.9894 - mae: 99.9894\n",
      "Epoch 72: val_loss improved from 582.10980 to 579.16058, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.2626 - mae: 109.2626 - val_loss: 579.1606 - val_mae: 579.1606\n",
      "Epoch 73/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 112.6534 - mae: 112.6534\n",
      "Epoch 73: val_loss did not improve from 579.16058\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 111.6250 - mae: 111.6250 - val_loss: 584.9088 - val_mae: 584.9088\n",
      "Epoch 74/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 110.0319 - mae: 110.0319\n",
      "Epoch 74: val_loss improved from 579.16058 to 576.73108, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 110.3606 - mae: 110.3606 - val_loss: 576.7311 - val_mae: 576.7311\n",
      "Epoch 75/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 108.0098 - mae: 108.0098\n",
      "Epoch 75: val_loss did not improve from 576.73108\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.9806 - mae: 109.9806 - val_loss: 655.4922 - val_mae: 655.4922\n",
      "Epoch 76/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 85.8504 - mae: 85.8504\n",
      "Epoch 76: val_loss improved from 576.73108 to 575.16473, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 114.1273 - mae: 114.1273 - val_loss: 575.1647 - val_mae: 575.1647\n",
      "Epoch 77/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 111.5859 - mae: 111.5859\n",
      "Epoch 77: val_loss improved from 575.16473 to 574.61633, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.5698 - mae: 109.5698 - val_loss: 574.6163 - val_mae: 574.6163\n",
      "Epoch 78/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 108.8558 - mae: 108.8558\n",
      "Epoch 78: val_loss improved from 574.61633 to 574.39716, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.7014 - mae: 109.7014 - val_loss: 574.3972 - val_mae: 574.3972\n",
      "Epoch 79/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 107.8456 - mae: 107.8456\n",
      "Epoch 79: val_loss did not improve from 574.39716\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 108.1629 - mae: 108.1629 - val_loss: 608.6507 - val_mae: 608.6507\n",
      "Epoch 80/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 116.9706 - mae: 116.9706\n",
      "Epoch 80: val_loss did not improve from 574.39716\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 108.8919 - mae: 108.8919 - val_loss: 622.7358 - val_mae: 622.7358\n",
      "Epoch 81/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 108.3272 - mae: 108.3272\n",
      "Epoch 81: val_loss did not improve from 574.39716\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 108.3903 - mae: 108.3903 - val_loss: 577.8203 - val_mae: 577.8203\n",
      "Epoch 82/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 194.0309 - mae: 194.0309\n",
      "Epoch 82: val_loss did not improve from 574.39716\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 116.8874 - mae: 116.8874 - val_loss: 577.9376 - val_mae: 577.9376\n",
      "Epoch 83/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 114.1982 - mae: 114.1982\n",
      "Epoch 83: val_loss did not improve from 574.39716\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.0638 - mae: 111.0638 - val_loss: 622.7242 - val_mae: 622.7242\n",
      "Epoch 84/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 113.0111 - mae: 113.0111\n",
      "Epoch 84: val_loss did not improve from 574.39716\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.0221 - mae: 110.0221 - val_loss: 580.6626 - val_mae: 580.6626\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 106.4355 - mae: 106.4355\n",
      "Epoch 85: val_loss improved from 574.39716 to 574.02240, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 106.4355 - mae: 106.4355 - val_loss: 574.0224 - val_mae: 574.0224\n",
      "Epoch 86/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 106.5646 - mae: 106.5646\n",
      "Epoch 86: val_loss did not improve from 574.02240\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 108.2495 - mae: 108.2495 - val_loss: 586.3257 - val_mae: 586.3257\n",
      "Epoch 87/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 123.0835 - mae: 123.0835\n",
      "Epoch 87: val_loss did not improve from 574.02240\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 106.5864 - mae: 106.5864 - val_loss: 587.6724 - val_mae: 587.6724\n",
      "Epoch 88/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 131.5909 - mae: 131.5909\n",
      "Epoch 88: val_loss did not improve from 574.02240\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 107.1115 - mae: 107.1115 - val_loss: 575.1793 - val_mae: 575.1793\n",
      "Epoch 89/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 102.5411 - mae: 102.5411\n",
      "Epoch 89: val_loss improved from 574.02240 to 572.01886, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 107.6800 - mae: 107.6800 - val_loss: 572.0189 - val_mae: 572.0189\n",
      "Epoch 90/100\n",
      " 8/18 [============>.................] - ETA: 0s - loss: 97.7145 - mae: 97.7145  \n",
      "Epoch 90: val_loss did not improve from 572.01886\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 107.4490 - mae: 107.4490 - val_loss: 616.9901 - val_mae: 616.9901\n",
      "Epoch 91/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 94.8114 - mae: 94.8114\n",
      "Epoch 91: val_loss did not improve from 572.01886\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 113.3806 - mae: 113.3806 - val_loss: 673.1776 - val_mae: 673.1776\n",
      "Epoch 92/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 102.5437 - mae: 102.5437\n",
      "Epoch 92: val_loss did not improve from 572.01886\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8854 - mae: 109.8854 - val_loss: 576.1896 - val_mae: 576.1896\n",
      "Epoch 93/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 107.3270 - mae: 107.3270\n",
      "Epoch 93: val_loss did not improve from 572.01886\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.0561 - mae: 112.0561 - val_loss: 575.1873 - val_mae: 575.1873\n",
      "Epoch 94/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 138.6943 - mae: 138.6943\n",
      "Epoch 94: val_loss did not improve from 572.01886\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 116.5994 - mae: 116.5994 - val_loss: 597.3276 - val_mae: 597.3276\n",
      "Epoch 95/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 139.6256 - mae: 139.6256\n",
      "Epoch 95: val_loss improved from 572.01886 to 570.35779, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 107.1522 - mae: 107.1522 - val_loss: 570.3578 - val_mae: 570.3578\n",
      "Epoch 96/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 112.7007 - mae: 112.7007\n",
      "Epoch 96: val_loss did not improve from 570.35779\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 113.5096 - mae: 113.5096 - val_loss: 657.5661 - val_mae: 657.5661\n",
      "Epoch 97/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 111.8258 - mae: 111.8258\n",
      "Epoch 97: val_loss did not improve from 570.35779\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.7690 - mae: 109.7690 - val_loss: 570.3743 - val_mae: 570.3743\n",
      "Epoch 98/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 112.3156 - mae: 112.3156\n",
      "Epoch 98: val_loss did not improve from 570.35779\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 106.5497 - mae: 106.5497 - val_loss: 574.5588 - val_mae: 574.5588\n",
      "Epoch 99/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 150.5217 - mae: 150.5217\n",
      "Epoch 99: val_loss improved from 570.35779 to 567.40308, saving model to ./saves/10//model_1_dense/model_1_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 106.2938 - mae: 106.2938 - val_loss: 567.4031 - val_mae: 567.4031\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 114.0317 - mae: 114.0317\n",
      "Epoch 100: val_loss did not improve from 567.40308\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 114.0317 - mae: 114.0317 - val_loss: 684.9349 - val_mae: 684.9349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12fd2ca00>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_1 = tf.keras.Sequential([\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(HORIZON, activation=\"linear\"),\n",
    "],\n",
    "                              name=\"model_1_dense\")\n",
    "\n",
    "model_1.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mae\"])\n",
    "\n",
    "model_1.fit(train_windows,\n",
    "            train_labels,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_1.name)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 684.9349 - mae: 684.9349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[684.9349365234375, 684.9349365234375]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(test_windows, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 567.4031 - mae: 567.4031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[567.403076171875, 567.403076171875]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use best weights checkpoint\n",
    "model_1.load_weights(f\"./saves/10/model_1_dense/{model_1.name}\")\n",
    "model_1.evaluate(test_windows, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds(model, input_data):\n",
    "    \"\"\"\n",
    "  Uses model to make predictions on input_data.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  model: trained model \n",
    "  input_data: windowed input data (same kind of data model was trained on)\n",
    "\n",
    "  Returns model predictions on input_data.\n",
    "  \"\"\"\n",
    "    forecast = model.predict(input_data)\n",
    "    return tf.squeeze(forecast)  # return 1D array of predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([8873.294, 8808.056, 9052.397, 8785.889, 8769.793], dtype=float32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_preds = make_preds(model_1, test_windows)\n",
    "model_1_preds[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_2 dense (window=30 horizon=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 30\n",
    "full_windows, full_labels = make_windows(btc_prices, WINDOW_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2205, 552, 2205, 552)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(\n",
    "    windows=full_windows, labels=full_labels)\n",
    "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1/18 [>.............................] - ETA: 8s - loss: 3155.7153 - mae: 3155.7153\n",
      "Epoch 1: val_loss improved from inf to 3181.58008, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 1s 13ms/step - loss: 1103.6805 - mae: 1103.6805 - val_loss: 3181.5801 - val_mae: 3181.5801\n",
      "Epoch 2/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 570.0815 - mae: 570.0815\n",
      "Epoch 2: val_loss improved from 3181.58008 to 2121.22681, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 376.8144 - mae: 376.8144 - val_loss: 2121.2268 - val_mae: 2121.2268\n",
      "Epoch 3/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 303.7289 - mae: 303.7289\n",
      "Epoch 3: val_loss improved from 2121.22681 to 1423.40552, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 293.2628 - mae: 293.2628 - val_loss: 1423.4055 - val_mae: 1423.4055\n",
      "Epoch 4/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 204.9104 - mae: 204.9104\n",
      "Epoch 4: val_loss improved from 1423.40552 to 1302.53564, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 255.5238 - mae: 255.5238 - val_loss: 1302.5356 - val_mae: 1302.5356\n",
      "Epoch 5/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 176.4881 - mae: 176.4881\n",
      "Epoch 5: val_loss improved from 1302.53564 to 1247.62000, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 226.9384 - mae: 226.9384 - val_loss: 1247.6200 - val_mae: 1247.6200\n",
      "Epoch 6/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 278.9613 - mae: 278.9613\n",
      "Epoch 6: val_loss did not improve from 1247.62000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 233.8872 - mae: 233.8872 - val_loss: 1468.7207 - val_mae: 1468.7207\n",
      "Epoch 7/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 234.7271 - mae: 234.7271\n",
      "Epoch 7: val_loss improved from 1247.62000 to 1213.93408, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 228.8888 - mae: 228.8888 - val_loss: 1213.9341 - val_mae: 1213.9341\n",
      "Epoch 8/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 203.6734 - mae: 203.6734\n",
      "Epoch 8: val_loss improved from 1213.93408 to 1180.23022, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 203.2960 - mae: 203.2960 - val_loss: 1180.2302 - val_mae: 1180.2302\n",
      "Epoch 9/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 209.9207 - mae: 209.9207\n",
      "Epoch 9: val_loss improved from 1180.23022 to 1099.94336, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 201.1635 - mae: 201.1635 - val_loss: 1099.9434 - val_mae: 1099.9434\n",
      "Epoch 10/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 205.1043 - mae: 205.1043\n",
      "Epoch 10: val_loss improved from 1099.94336 to 1077.69824, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 200.3080 - mae: 200.3080 - val_loss: 1077.6982 - val_mae: 1077.6982\n",
      "Epoch 11/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 187.2179 - mae: 187.2179\n",
      "Epoch 11: val_loss did not improve from 1077.69824\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 187.7879 - mae: 187.7879 - val_loss: 1132.0039 - val_mae: 1132.0039\n",
      "Epoch 12/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 167.2506 - mae: 167.2506\n",
      "Epoch 12: val_loss improved from 1077.69824 to 1025.67529, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 189.1347 - mae: 189.1347 - val_loss: 1025.6753 - val_mae: 1025.6753\n",
      "Epoch 13/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 178.0204 - mae: 178.0204\n",
      "Epoch 13: val_loss did not improve from 1025.67529\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 177.9176 - mae: 177.9176 - val_loss: 1142.0967 - val_mae: 1142.0967\n",
      "Epoch 14/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 257.8706 - mae: 257.8706\n",
      "Epoch 14: val_loss improved from 1025.67529 to 964.01758, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 175.5695 - mae: 175.5695 - val_loss: 964.0176 - val_mae: 964.0176\n",
      "Epoch 15/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 213.4360 - mae: 213.4360\n",
      "Epoch 15: val_loss improved from 964.01758 to 962.34943, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 173.0461 - mae: 173.0461 - val_loss: 962.3494 - val_mae: 962.3494\n",
      "Epoch 16/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 175.8551 - mae: 175.8551\n",
      "Epoch 16: val_loss improved from 962.34943 to 945.87329, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 174.9064 - mae: 174.9064 - val_loss: 945.8733 - val_mae: 945.8733\n",
      "Epoch 17/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 165.4715 - mae: 165.4715\n",
      "Epoch 17: val_loss did not improve from 945.87329\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 167.7596 - mae: 167.7596 - val_loss: 1214.3800 - val_mae: 1214.3800\n",
      "Epoch 18/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 244.3707 - mae: 244.3707\n",
      "Epoch 18: val_loss improved from 945.87329 to 918.84314, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 170.7821 - mae: 170.7821 - val_loss: 918.8431 - val_mae: 918.8431\n",
      "Epoch 19/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 158.6599 - mae: 158.6599\n",
      "Epoch 19: val_loss did not improve from 918.84314\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 159.2714 - mae: 159.2714 - val_loss: 1148.6594 - val_mae: 1148.6594\n",
      "Epoch 20/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 214.3724 - mae: 214.3724\n",
      "Epoch 20: val_loss did not improve from 918.84314\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 175.0837 - mae: 175.0837 - val_loss: 1042.6504 - val_mae: 1042.6505\n",
      "Epoch 21/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 168.9013 - mae: 168.9013\n",
      "Epoch 21: val_loss improved from 918.84314 to 909.95758, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 167.2992 - mae: 167.2992 - val_loss: 909.9576 - val_mae: 909.9576\n",
      "Epoch 22/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 153.2213 - mae: 153.2213\n",
      "Epoch 22: val_loss did not improve from 909.95758\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 156.0068 - mae: 156.0068 - val_loss: 920.1380 - val_mae: 920.1380\n",
      "Epoch 23/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 206.3829 - mae: 206.3829\n",
      "Epoch 23: val_loss did not improve from 909.95758\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 158.1315 - mae: 158.1315 - val_loss: 937.5298 - val_mae: 937.5298\n",
      "Epoch 24/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 184.0647 - mae: 184.0647\n",
      "Epoch 24: val_loss improved from 909.95758 to 863.55878, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 153.6561 - mae: 153.6561 - val_loss: 863.5588 - val_mae: 863.5588\n",
      "Epoch 25/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 147.9819 - mae: 147.9819\n",
      "Epoch 25: val_loss improved from 863.55878 to 858.37939, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 151.4257 - mae: 151.4257 - val_loss: 858.3794 - val_mae: 858.3794\n",
      "Epoch 26/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 149.4272 - mae: 149.4272\n",
      "Epoch 26: val_loss improved from 858.37939 to 831.31226, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 149.6821 - mae: 149.6821 - val_loss: 831.3123 - val_mae: 831.3123\n",
      "Epoch 27/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 114.2038 - mae: 114.2038\n",
      "Epoch 27: val_loss improved from 831.31226 to 817.83636, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 151.6652 - mae: 151.6652 - val_loss: 817.8364 - val_mae: 817.8364\n",
      "Epoch 28/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 134.8527 - mae: 134.8527\n",
      "Epoch 28: val_loss did not improve from 817.83636\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 146.7748 - mae: 146.7748 - val_loss: 903.3092 - val_mae: 903.3092\n",
      "Epoch 29/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 212.0378 - mae: 212.0378\n",
      "Epoch 29: val_loss did not improve from 817.83636\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 152.0451 - mae: 152.0451 - val_loss: 843.6157 - val_mae: 843.6157\n",
      "Epoch 30/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 148.8664 - mae: 148.8664\n",
      "Epoch 30: val_loss did not improve from 817.83636\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 146.0337 - mae: 146.0337 - val_loss: 1264.0682 - val_mae: 1264.0682\n",
      "Epoch 31/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 226.5333 - mae: 226.5333\n",
      "Epoch 31: val_loss improved from 817.83636 to 790.40820, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 157.5262 - mae: 157.5262 - val_loss: 790.4082 - val_mae: 790.4082\n",
      "Epoch 32/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 171.7727 - mae: 171.7727\n",
      "Epoch 32: val_loss did not improve from 790.40820\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 160.5433 - mae: 160.5433 - val_loss: 889.0128 - val_mae: 889.0128\n",
      "Epoch 33/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 164.6284 - mae: 164.6284\n",
      "Epoch 33: val_loss did not improve from 790.40820\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 160.0627 - mae: 160.0627 - val_loss: 888.3783 - val_mae: 888.3783\n",
      "Epoch 34/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 170.1308 - mae: 170.1308\n",
      "Epoch 34: val_loss did not improve from 790.40820\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 149.4562 - mae: 149.4562 - val_loss: 791.4919 - val_mae: 791.4919\n",
      "Epoch 35/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 161.3385 - mae: 161.3385\n",
      "Epoch 35: val_loss did not improve from 790.40820\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 147.2432 - mae: 147.2432 - val_loss: 790.9368 - val_mae: 790.9368\n",
      "Epoch 36/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 142.2173 - mae: 142.2173\n",
      "Epoch 36: val_loss did not improve from 790.40820\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 138.6483 - mae: 138.6483 - val_loss: 875.7810 - val_mae: 875.7810\n",
      "Epoch 37/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 135.2435 - mae: 135.2435\n",
      "Epoch 37: val_loss improved from 790.40820 to 756.27069, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 145.7626 - mae: 145.7626 - val_loss: 756.2707 - val_mae: 756.2707\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 143.3800 - mae: 143.3800\n",
      "Epoch 38: val_loss did not improve from 756.27069\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 143.3800 - mae: 143.3800 - val_loss: 791.3947 - val_mae: 791.3947\n",
      "Epoch 39/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 143.2235 - mae: 143.2235\n",
      "Epoch 39: val_loss improved from 756.27069 to 739.51935, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 143.8972 - mae: 143.8972 - val_loss: 739.5193 - val_mae: 739.5193\n",
      "Epoch 40/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 107.4986 - mae: 107.4986\n",
      "Epoch 40: val_loss improved from 739.51935 to 733.48242, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 138.4830 - mae: 138.4830 - val_loss: 733.4824 - val_mae: 733.4824\n",
      "Epoch 41/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 106.8405 - mae: 106.8405\n",
      "Epoch 41: val_loss did not improve from 733.48242\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 133.0356 - mae: 133.0356 - val_loss: 742.5055 - val_mae: 742.5055\n",
      "Epoch 42/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 110.8060 - mae: 110.8060\n",
      "Epoch 42: val_loss did not improve from 733.48242\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 133.7653 - mae: 133.7653 - val_loss: 734.5653 - val_mae: 734.5653\n",
      "Epoch 43/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 122.4083 - mae: 122.4083\n",
      "Epoch 43: val_loss did not improve from 733.48242\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 130.5385 - mae: 130.5385 - val_loss: 745.8775 - val_mae: 745.8775\n",
      "Epoch 44/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 150.2895 - mae: 150.2895\n",
      "Epoch 44: val_loss improved from 733.48242 to 715.54474, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 140.6675 - mae: 140.6675 - val_loss: 715.5447 - val_mae: 715.5447\n",
      "Epoch 45/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 133.7572 - mae: 133.7572\n",
      "Epoch 45: val_loss did not improve from 715.54474\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 129.7149 - mae: 129.7149 - val_loss: 741.0615 - val_mae: 741.0615\n",
      "Epoch 46/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 113.8661 - mae: 113.8661\n",
      "Epoch 46: val_loss improved from 715.54474 to 708.23083, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 131.3867 - mae: 131.3867 - val_loss: 708.2308 - val_mae: 708.2308\n",
      "Epoch 47/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 135.0482 - mae: 135.0482\n",
      "Epoch 47: val_loss did not improve from 708.23083\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 130.7836 - mae: 130.7836 - val_loss: 729.6307 - val_mae: 729.6307\n",
      "Epoch 48/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 100.9996 - mae: 100.9996\n",
      "Epoch 48: val_loss improved from 708.23083 to 701.77972, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 130.4883 - mae: 130.4883 - val_loss: 701.7797 - val_mae: 701.7797\n",
      "Epoch 49/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 117.2810 - mae: 117.2810\n",
      "Epoch 49: val_loss did not improve from 701.77972\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 130.6793 - mae: 130.6793 - val_loss: 729.0025 - val_mae: 729.0025\n",
      "Epoch 50/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 130.2310 - mae: 130.2310\n",
      "Epoch 50: val_loss did not improve from 701.77972\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 143.5242 - mae: 143.5242 - val_loss: 876.3940 - val_mae: 876.3940\n",
      "Epoch 51/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 128.7739 - mae: 128.7739\n",
      "Epoch 51: val_loss did not improve from 701.77972\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 134.8563 - mae: 134.8563 - val_loss: 951.7553 - val_mae: 951.7553\n",
      "Epoch 52/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 180.3630 - mae: 180.3630\n",
      "Epoch 52: val_loss improved from 701.77972 to 688.46729, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 146.2550 - mae: 146.2550 - val_loss: 688.4673 - val_mae: 688.4673\n",
      "Epoch 53/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 142.1683 - mae: 142.1683\n",
      "Epoch 53: val_loss did not improve from 688.46729\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 148.8852 - mae: 148.8852 - val_loss: 692.6426 - val_mae: 692.6426\n",
      "Epoch 54/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 126.5676 - mae: 126.5676\n",
      "Epoch 54: val_loss did not improve from 688.46729\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 127.1406 - mae: 127.1406 - val_loss: 942.5145 - val_mae: 942.5145\n",
      "Epoch 55/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 151.7277 - mae: 151.7277\n",
      "Epoch 55: val_loss did not improve from 688.46729\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 157.0194 - mae: 157.0194 - val_loss: 806.5245 - val_mae: 806.5245\n",
      "Epoch 56/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 154.8647 - mae: 154.8647\n",
      "Epoch 56: val_loss did not improve from 688.46729\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 153.3818 - mae: 153.3818 - val_loss: 830.0060 - val_mae: 830.0060\n",
      "Epoch 57/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 141.0849 - mae: 141.0849\n",
      "Epoch 57: val_loss did not improve from 688.46729\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 138.5562 - mae: 138.5562 - val_loss: 704.6841 - val_mae: 704.6840\n",
      "Epoch 58/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 119.7076 - mae: 119.7076\n",
      "Epoch 58: val_loss did not improve from 688.46729\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 124.9342 - mae: 124.9342 - val_loss: 724.2948 - val_mae: 724.2948\n",
      "Epoch 59/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 122.3371 - mae: 122.3371\n",
      "Epoch 59: val_loss did not improve from 688.46729\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 152.5578 - mae: 152.5578 - val_loss: 691.7300 - val_mae: 691.7300\n",
      "Epoch 60/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 133.2516 - mae: 133.2516\n",
      "Epoch 60: val_loss improved from 688.46729 to 681.13379, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 133.1704 - mae: 133.1704 - val_loss: 681.1338 - val_mae: 681.1338\n",
      "Epoch 61/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 125.4348 - mae: 125.4348\n",
      "Epoch 61: val_loss improved from 681.13379 to 671.30853, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 122.9823 - mae: 122.9823 - val_loss: 671.3085 - val_mae: 671.3085\n",
      "Epoch 62/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 90.9096 - mae: 90.9096\n",
      "Epoch 62: val_loss did not improve from 671.30853\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 125.0752 - mae: 125.0752 - val_loss: 715.7697 - val_mae: 715.7697\n",
      "Epoch 63/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 118.1585 - mae: 118.1585\n",
      "Epoch 63: val_loss improved from 671.30853 to 655.87683, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 129.6375 - mae: 129.6375 - val_loss: 655.8768 - val_mae: 655.8768\n",
      "Epoch 64/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 96.3308 - mae: 96.3308\n",
      "Epoch 64: val_loss did not improve from 655.87683\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 126.9402 - mae: 126.9402 - val_loss: 684.7199 - val_mae: 684.7199\n",
      "Epoch 65/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 122.5146 - mae: 122.5146\n",
      "Epoch 65: val_loss did not improve from 655.87683\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 130.9189 - mae: 130.9189 - val_loss: 814.3670 - val_mae: 814.3670\n",
      "Epoch 66/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 154.2527 - mae: 154.2527\n",
      "Epoch 66: val_loss improved from 655.87683 to 649.52631, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 127.9876 - mae: 127.9876 - val_loss: 649.5263 - val_mae: 649.5263\n",
      "Epoch 67/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 122.3211 - mae: 122.3211\n",
      "Epoch 67: val_loss did not improve from 649.52631\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 122.6437 - mae: 122.6437 - val_loss: 677.6421 - val_mae: 677.6421\n",
      "Epoch 68/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 99.9166 - mae: 99.9166\n",
      "Epoch 68: val_loss did not improve from 649.52631\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 123.4308 - mae: 123.4308 - val_loss: 660.6889 - val_mae: 660.6889\n",
      "Epoch 69/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 120.8623 - mae: 120.8623\n",
      "Epoch 69: val_loss did not improve from 649.52631\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 124.9196 - mae: 124.9196 - val_loss: 668.4088 - val_mae: 668.4088\n",
      "Epoch 70/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 134.3681 - mae: 134.3681\n",
      "Epoch 70: val_loss did not improve from 649.52631\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 129.1557 - mae: 129.1557 - val_loss: 678.7157 - val_mae: 678.7157\n",
      "Epoch 71/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 117.0613 - mae: 117.0613\n",
      "Epoch 71: val_loss improved from 649.52631 to 645.83191, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 125.4806 - mae: 125.4806 - val_loss: 645.8319 - val_mae: 645.8319\n",
      "Epoch 72/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 79.8163 - mae: 79.8163\n",
      "Epoch 72: val_loss did not improve from 645.83191\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 118.9381 - mae: 118.9381 - val_loss: 669.4525 - val_mae: 669.4525\n",
      "Epoch 73/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 168.5979 - mae: 168.5979\n",
      "Epoch 73: val_loss did not improve from 645.83191\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 119.6861 - mae: 119.6861 - val_loss: 657.4070 - val_mae: 657.4070\n",
      "Epoch 74/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 121.4834 - mae: 121.4834\n",
      "Epoch 74: val_loss did not improve from 645.83191\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 124.2419 - mae: 124.2419 - val_loss: 701.4592 - val_mae: 701.4592\n",
      "Epoch 75/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 122.7700 - mae: 122.7700\n",
      "Epoch 75: val_loss improved from 645.83191 to 641.46655, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 132.2549 - mae: 132.2549 - val_loss: 641.4666 - val_mae: 641.4666\n",
      "Epoch 76/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 136.4985 - mae: 136.4985\n",
      "Epoch 76: val_loss did not improve from 641.46655\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 136.2269 - mae: 136.2269 - val_loss: 663.3177 - val_mae: 663.3177\n",
      "Epoch 77/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 70.2387 - mae: 70.2387\n",
      "Epoch 77: val_loss did not improve from 641.46655\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 144.3198 - mae: 144.3198 - val_loss: 825.4052 - val_mae: 825.4052\n",
      "Epoch 78/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 163.9270 - mae: 163.9270\n",
      "Epoch 78: val_loss did not improve from 641.46655\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 138.7704 - mae: 138.7704 - val_loss: 1053.3165 - val_mae: 1053.3165\n",
      "Epoch 79/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 217.7661 - mae: 217.7661\n",
      "Epoch 79: val_loss did not improve from 641.46655\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 127.5366 - mae: 127.5366 - val_loss: 693.8936 - val_mae: 693.8936\n",
      "Epoch 80/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 115.3182 - mae: 115.3182\n",
      "Epoch 80: val_loss did not improve from 641.46655\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 124.9497 - mae: 124.9497 - val_loss: 786.9943 - val_mae: 786.9943\n",
      "Epoch 81/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 149.3777 - mae: 149.3777\n",
      "Epoch 81: val_loss did not improve from 641.46655\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 128.9613 - mae: 128.9613 - val_loss: 742.4802 - val_mae: 742.4802\n",
      "Epoch 82/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 108.8978 - mae: 108.8978\n",
      "Epoch 82: val_loss did not improve from 641.46655\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 123.6898 - mae: 123.6898 - val_loss: 996.8735 - val_mae: 996.8735\n",
      "Epoch 83/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 127.4874 - mae: 127.4874\n",
      "Epoch 83: val_loss did not improve from 641.46655\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 147.0655 - mae: 147.0655 - val_loss: 790.3773 - val_mae: 790.3773\n",
      "Epoch 84/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 137.6364 - mae: 137.6364\n",
      "Epoch 84: val_loss did not improve from 641.46655\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 127.4559 - mae: 127.4559 - val_loss: 668.4062 - val_mae: 668.4062\n",
      "Epoch 85/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 105.9858 - mae: 105.9858\n",
      "Epoch 85: val_loss improved from 641.46655 to 623.88885, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 130.3040 - mae: 130.3040 - val_loss: 623.8889 - val_mae: 623.8889\n",
      "Epoch 86/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 114.9762 - mae: 114.9762\n",
      "Epoch 86: val_loss did not improve from 623.88885\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 117.0889 - mae: 117.0889 - val_loss: 771.4780 - val_mae: 771.4780\n",
      "Epoch 87/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 153.0044 - mae: 153.0044\n",
      "Epoch 87: val_loss did not improve from 623.88885\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 125.8295 - mae: 125.8295 - val_loss: 640.6691 - val_mae: 640.6691\n",
      "Epoch 88/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 131.0308 - mae: 131.0308\n",
      "Epoch 88: val_loss did not improve from 623.88885\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 121.7971 - mae: 121.7971 - val_loss: 663.4254 - val_mae: 663.4254\n",
      "Epoch 89/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 108.0953 - mae: 108.0953\n",
      "Epoch 89: val_loss did not improve from 623.88885\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 119.7855 - mae: 119.7855 - val_loss: 794.8613 - val_mae: 794.8613\n",
      "Epoch 90/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 139.9039 - mae: 139.9039\n",
      "Epoch 90: val_loss improved from 623.88885 to 617.23828, saving model to ./saves/10//model_2_dense/model_2_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 123.2577 - mae: 123.2577 - val_loss: 617.2383 - val_mae: 617.2383\n",
      "Epoch 91/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 115.8409 - mae: 115.8409\n",
      "Epoch 91: val_loss did not improve from 617.23828\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 115.9472 - mae: 115.9472 - val_loss: 637.8149 - val_mae: 637.8149\n",
      "Epoch 92/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 127.0064 - mae: 127.0064\n",
      "Epoch 92: val_loss did not improve from 617.23828\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 123.7312 - mae: 123.7312 - val_loss: 702.3494 - val_mae: 702.3494\n",
      "Epoch 93/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 156.7625 - mae: 156.7625\n",
      "Epoch 93: val_loss did not improve from 617.23828\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 133.1283 - mae: 133.1283 - val_loss: 751.8823 - val_mae: 751.8823\n",
      "Epoch 94/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 143.8646 - mae: 143.8646\n",
      "Epoch 94: val_loss did not improve from 617.23828\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 143.0884 - mae: 143.0884 - val_loss: 686.4375 - val_mae: 686.4375\n",
      "Epoch 95/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 146.5923 - mae: 146.5923\n",
      "Epoch 95: val_loss did not improve from 617.23828\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 121.7379 - mae: 121.7379 - val_loss: 640.4175 - val_mae: 640.4175\n",
      "Epoch 96/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 97.2365 - mae: 97.2365\n",
      "Epoch 96: val_loss did not improve from 617.23828\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 116.6439 - mae: 116.6439 - val_loss: 737.2883 - val_mae: 737.2883\n",
      "Epoch 97/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 142.7553 - mae: 142.7553\n",
      "Epoch 97: val_loss did not improve from 617.23828\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 122.4588 - mae: 122.4588 - val_loss: 686.6940 - val_mae: 686.6940\n",
      "Epoch 98/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 123.2071 - mae: 123.2071\n",
      "Epoch 98: val_loss did not improve from 617.23828\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 126.9721 - mae: 126.9721 - val_loss: 688.3503 - val_mae: 688.3503\n",
      "Epoch 99/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 147.0521 - mae: 147.0521\n",
      "Epoch 99: val_loss did not improve from 617.23828\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 126.8950 - mae: 126.8950 - val_loss: 757.6239 - val_mae: 757.6239\n",
      "Epoch 100/100\n",
      " 6/18 [=========>....................] - ETA: 0s - loss: 122.2032 - mae: 122.2032\n",
      "Epoch 100: val_loss did not improve from 617.23828\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 119.1689 - mae: 119.1689 - val_loss: 624.3747 - val_mae: 624.3747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x131c15940>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_2 = tf.keras.Sequential([\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(HORIZON, activation=\"linear\"),\n",
    "],\n",
    "                              name=\"model_2_dense\")\n",
    "\n",
    "model_2.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mae\"])\n",
    "\n",
    "model_2.fit(train_windows,\n",
    "            train_labels,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_2.name)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_3 dense window = 30 horizon = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2751, 2751)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 30\n",
    "HORIZON = 7\n",
    "full_windows, full_labels = make_windows(btc_prices,\n",
    "                                         WINDOW_SIZE,\n",
    "                                         horizon=HORIZON)\n",
    "len(full_windows), len(full_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(\n",
    "    windows=full_windows, labels=full_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1/18 [>.............................] - ETA: 10s - loss: 2623.6948 - mae: 2623.6948\n",
      "Epoch 1: val_loss improved from inf to 2606.49707, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 1s 13ms/step - loss: 1182.5363 - mae: 1182.5363 - val_loss: 2606.4971 - val_mae: 2606.4971\n",
      "Epoch 2/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 393.9960 - mae: 393.9960\n",
      "Epoch 2: val_loss improved from 2606.49707 to 2305.15210, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 476.1976 - mae: 476.1976 - val_loss: 2305.1521 - val_mae: 2305.1521\n",
      "Epoch 3/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 393.1249 - mae: 393.1249\n",
      "Epoch 3: val_loss improved from 2305.15210 to 2139.01318, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 385.9485 - mae: 385.9485 - val_loss: 2139.0132 - val_mae: 2139.0132\n",
      "Epoch 4/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 425.0312 - mae: 425.0312\n",
      "Epoch 4: val_loss improved from 2139.01318 to 1751.91309, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 356.8289 - mae: 356.8289 - val_loss: 1751.9131 - val_mae: 1751.9131\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 332.2337 - mae: 332.2337\n",
      "Epoch 5: val_loss did not improve from 1751.91309\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 332.2337 - mae: 332.2337 - val_loss: 1886.3054 - val_mae: 1886.3054\n",
      "Epoch 6/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 323.5077 - mae: 323.5077\n",
      "Epoch 6: val_loss improved from 1751.91309 to 1749.49756, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 319.7218 - mae: 319.7218 - val_loss: 1749.4976 - val_mae: 1749.4976\n",
      "Epoch 7/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 310.1715 - mae: 310.1715\n",
      "Epoch 7: val_loss improved from 1749.49756 to 1588.48413, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 304.5758 - mae: 304.5758 - val_loss: 1588.4841 - val_mae: 1588.4841\n",
      "Epoch 8/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 303.9942 - mae: 303.9942\n",
      "Epoch 8: val_loss improved from 1588.48413 to 1544.70117, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 300.2604 - mae: 300.2604 - val_loss: 1544.7012 - val_mae: 1544.7012\n",
      "Epoch 9/100\n",
      " 8/18 [============>.................] - ETA: 0s - loss: 302.3026 - mae: 302.3026\n",
      "Epoch 9: val_loss did not improve from 1544.70117\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 285.8708 - mae: 285.8708 - val_loss: 1668.4807 - val_mae: 1668.4807\n",
      "Epoch 10/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 243.6809 - mae: 243.6809\n",
      "Epoch 10: val_loss improved from 1544.70117 to 1476.99597, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 291.7064 - mae: 291.7064 - val_loss: 1476.9960 - val_mae: 1476.9960\n",
      "Epoch 11/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 275.8801 - mae: 275.8801\n",
      "Epoch 11: val_loss improved from 1476.99597 to 1459.67712, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 277.3025 - mae: 277.3025 - val_loss: 1459.6771 - val_mae: 1459.6771\n",
      "Epoch 12/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 201.2061 - mae: 201.2061\n",
      "Epoch 12: val_loss did not improve from 1459.67712\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 270.3451 - mae: 270.3451 - val_loss: 1588.5077 - val_mae: 1588.5077\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 269.6240 - mae: 269.6240\n",
      "Epoch 13: val_loss improved from 1459.67712 to 1439.38696, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 269.6240 - mae: 269.6240 - val_loss: 1439.3870 - val_mae: 1439.3870\n",
      "Epoch 14/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 248.7767 - mae: 248.7767\n",
      "Epoch 14: val_loss improved from 1439.38696 to 1398.34131, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 267.2040 - mae: 267.2040 - val_loss: 1398.3413 - val_mae: 1398.3413\n",
      "Epoch 15/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 258.9367 - mae: 258.9367\n",
      "Epoch 15: val_loss did not improve from 1398.34131\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 265.8263 - mae: 265.8263 - val_loss: 1474.7148 - val_mae: 1474.7148\n",
      "Epoch 16/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 248.3364 - mae: 248.3364\n",
      "Epoch 16: val_loss did not improve from 1398.34131\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 258.8197 - mae: 258.8197 - val_loss: 1417.8021 - val_mae: 1417.8021\n",
      "Epoch 17/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 295.4612 - mae: 295.4612\n",
      "Epoch 17: val_loss did not improve from 1398.34131\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 256.7180 - mae: 256.7180 - val_loss: 1576.7810 - val_mae: 1576.7810\n",
      "Epoch 18/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 299.0149 - mae: 299.0149\n",
      "Epoch 18: val_loss did not improve from 1398.34131\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 258.2173 - mae: 258.2173 - val_loss: 1576.3285 - val_mae: 1576.3285\n",
      "Epoch 19/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 246.1076 - mae: 246.1076\n",
      "Epoch 19: val_loss did not improve from 1398.34131\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 254.3356 - mae: 254.3356 - val_loss: 1664.1791 - val_mae: 1664.1791\n",
      "Epoch 20/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 249.3377 - mae: 249.3377\n",
      "Epoch 20: val_loss did not improve from 1398.34131\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 262.3531 - mae: 262.3531 - val_loss: 1460.7168 - val_mae: 1460.7168\n",
      "Epoch 21/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 308.3447 - mae: 308.3447\n",
      "Epoch 21: val_loss did not improve from 1398.34131\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 250.4540 - mae: 250.4540 - val_loss: 1471.0217 - val_mae: 1471.0217\n",
      "Epoch 22/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 327.6442 - mae: 327.6442\n",
      "Epoch 22: val_loss improved from 1398.34131 to 1367.50964, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 249.0190 - mae: 249.0190 - val_loss: 1367.5096 - val_mae: 1367.5096\n",
      "Epoch 23/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 264.0450 - mae: 264.0450\n",
      "Epoch 23: val_loss did not improve from 1367.50964\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 246.5446 - mae: 246.5446 - val_loss: 1423.4144 - val_mae: 1423.4144\n",
      "Epoch 24/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 322.6127 - mae: 322.6127\n",
      "Epoch 24: val_loss improved from 1367.50964 to 1336.67908, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 268.3145 - mae: 268.3145 - val_loss: 1336.6791 - val_mae: 1336.6791\n",
      "Epoch 25/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 250.7342 - mae: 250.7342\n",
      "Epoch 25: val_loss did not improve from 1336.67908\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 251.3288 - mae: 251.3288 - val_loss: 1612.9016 - val_mae: 1612.9016\n",
      "Epoch 26/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 245.3216 - mae: 245.3216\n",
      "Epoch 26: val_loss did not improve from 1336.67908\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 245.7135 - mae: 245.7135 - val_loss: 1445.5674 - val_mae: 1445.5674\n",
      "Epoch 27/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 222.2545 - mae: 222.2545\n",
      "Epoch 27: val_loss did not improve from 1336.67908\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 240.2547 - mae: 240.2547 - val_loss: 1375.8777 - val_mae: 1375.8777\n",
      "Epoch 28/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 217.6341 - mae: 217.6341\n",
      "Epoch 28: val_loss did not improve from 1336.67908\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 238.4504 - mae: 238.4504 - val_loss: 1451.9979 - val_mae: 1451.9979\n",
      "Epoch 29/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 274.3709 - mae: 274.3709\n",
      "Epoch 29: val_loss did not improve from 1336.67908\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 244.4701 - mae: 244.4701 - val_loss: 1359.6332 - val_mae: 1359.6332\n",
      "Epoch 30/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 288.0367 - mae: 288.0367\n",
      "Epoch 30: val_loss did not improve from 1336.67908\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 245.5454 - mae: 245.5454 - val_loss: 1357.9799 - val_mae: 1357.9799\n",
      "Epoch 31/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 202.3163 - mae: 202.3163\n",
      "Epoch 31: val_loss improved from 1336.67908 to 1315.18604, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 237.9380 - mae: 237.9380 - val_loss: 1315.1860 - val_mae: 1315.1860\n",
      "Epoch 32/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 238.8667 - mae: 238.8667\n",
      "Epoch 32: val_loss did not improve from 1315.18604\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 238.3227 - mae: 238.3227 - val_loss: 1317.1594 - val_mae: 1317.1594\n",
      "Epoch 33/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 223.4189 - mae: 223.4189\n",
      "Epoch 33: val_loss did not improve from 1315.18604\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 235.1629 - mae: 235.1629 - val_loss: 1316.7069 - val_mae: 1316.7069\n",
      "Epoch 34/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 237.6431 - mae: 237.6431\n",
      "Epoch 34: val_loss did not improve from 1315.18604\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 238.3905 - mae: 238.3905 - val_loss: 1389.2145 - val_mae: 1389.2145\n",
      "Epoch 35/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 211.6744 - mae: 211.6744\n",
      "Epoch 35: val_loss did not improve from 1315.18604\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 242.7024 - mae: 242.7024 - val_loss: 1402.7910 - val_mae: 1402.7910\n",
      "Epoch 36/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 223.4623 - mae: 223.4623\n",
      "Epoch 36: val_loss did not improve from 1315.18604\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 238.3835 - mae: 238.3835 - val_loss: 1346.8407 - val_mae: 1346.8407\n",
      "Epoch 37/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 218.4336 - mae: 218.4336\n",
      "Epoch 37: val_loss did not improve from 1315.18604\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 248.2945 - mae: 248.2945 - val_loss: 1382.5356 - val_mae: 1382.5356\n",
      "Epoch 38/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 219.6097 - mae: 219.6097\n",
      "Epoch 38: val_loss did not improve from 1315.18604\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 234.4650 - mae: 234.4650 - val_loss: 1380.0208 - val_mae: 1380.0208\n",
      "Epoch 39/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 149.4015 - mae: 149.4015\n",
      "Epoch 39: val_loss did not improve from 1315.18604\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 234.1275 - mae: 234.1275 - val_loss: 1396.3613 - val_mae: 1396.3613\n",
      "Epoch 40/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 227.1000 - mae: 227.1000\n",
      "Epoch 40: val_loss did not improve from 1315.18604\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 235.7254 - mae: 235.7254 - val_loss: 1519.3610 - val_mae: 1519.3610\n",
      "Epoch 41/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 205.1889 - mae: 205.1889\n",
      "Epoch 41: val_loss did not improve from 1315.18604\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 237.5792 - mae: 237.5792 - val_loss: 1352.1157 - val_mae: 1352.1157\n",
      "Epoch 42/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 174.9081 - mae: 174.9081\n",
      "Epoch 42: val_loss improved from 1315.18604 to 1299.79126, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 237.0258 - mae: 237.0258 - val_loss: 1299.7913 - val_mae: 1299.7913\n",
      "Epoch 43/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 270.7555 - mae: 270.7555\n",
      "Epoch 43: val_loss improved from 1299.79126 to 1283.27014, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 231.1354 - mae: 231.1354 - val_loss: 1283.2701 - val_mae: 1283.2701\n",
      "Epoch 44/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 230.6788 - mae: 230.6788\n",
      "Epoch 44: val_loss did not improve from 1283.27014\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 236.3535 - mae: 236.3535 - val_loss: 1291.6652 - val_mae: 1291.6652\n",
      "Epoch 45/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 260.0365 - mae: 260.0365\n",
      "Epoch 45: val_loss did not improve from 1283.27014\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 229.2268 - mae: 229.2268 - val_loss: 1399.8079 - val_mae: 1399.8079\n",
      "Epoch 46/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 240.2190 - mae: 240.2190\n",
      "Epoch 46: val_loss did not improve from 1283.27014\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 236.3663 - mae: 236.3663 - val_loss: 1309.8562 - val_mae: 1309.8562\n",
      "Epoch 47/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 188.1602 - mae: 188.1602\n",
      "Epoch 47: val_loss did not improve from 1283.27014\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 232.8303 - mae: 232.8303 - val_loss: 1372.8463 - val_mae: 1372.8463\n",
      "Epoch 48/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 252.8699 - mae: 252.8699\n",
      "Epoch 48: val_loss did not improve from 1283.27014\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 230.7030 - mae: 230.7030 - val_loss: 1343.0537 - val_mae: 1343.0537\n",
      "Epoch 49/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 292.6802 - mae: 292.6802\n",
      "Epoch 49: val_loss did not improve from 1283.27014\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 241.7802 - mae: 241.7802 - val_loss: 1305.1300 - val_mae: 1305.1300\n",
      "Epoch 50/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 249.6160 - mae: 249.6160\n",
      "Epoch 50: val_loss improved from 1283.27014 to 1279.53442, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 233.6826 - mae: 233.6826 - val_loss: 1279.5344 - val_mae: 1279.5344\n",
      "Epoch 51/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 232.3320 - mae: 232.3320\n",
      "Epoch 51: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 235.3612 - mae: 235.3612 - val_loss: 1420.5796 - val_mae: 1420.5796\n",
      "Epoch 52/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 159.5363 - mae: 159.5363\n",
      "Epoch 52: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 229.5099 - mae: 229.5099 - val_loss: 1299.3629 - val_mae: 1299.3629\n",
      "Epoch 53/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 178.7182 - mae: 178.7182\n",
      "Epoch 53: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 229.7111 - mae: 229.7111 - val_loss: 1286.2351 - val_mae: 1286.2351\n",
      "Epoch 54/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 264.3500 - mae: 264.3500\n",
      "Epoch 54: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 231.1671 - mae: 231.1671 - val_loss: 1280.5417 - val_mae: 1280.5417\n",
      "Epoch 55/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 296.0294 - mae: 296.0294\n",
      "Epoch 55: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 230.1860 - mae: 230.1860 - val_loss: 1289.4261 - val_mae: 1289.4261\n",
      "Epoch 56/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 173.3751 - mae: 173.3751\n",
      "Epoch 56: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 228.6671 - mae: 228.6671 - val_loss: 1315.5708 - val_mae: 1315.5708\n",
      "Epoch 57/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 228.7077 - mae: 228.7077\n",
      "Epoch 57: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 230.7840 - mae: 230.7840 - val_loss: 1326.3097 - val_mae: 1326.3097\n",
      "Epoch 58/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 226.9610 - mae: 226.9610\n",
      "Epoch 58: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 227.5970 - mae: 227.5970 - val_loss: 1310.5953 - val_mae: 1310.5953\n",
      "Epoch 59/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 180.5334 - mae: 180.5334\n",
      "Epoch 59: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 228.3210 - mae: 228.3210 - val_loss: 1286.7811 - val_mae: 1286.7811\n",
      "Epoch 60/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 224.6225 - mae: 224.6225\n",
      "Epoch 60: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 230.6091 - mae: 230.6091 - val_loss: 1288.0806 - val_mae: 1288.0806\n",
      "Epoch 61/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 225.2689 - mae: 225.2689\n",
      "Epoch 61: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 226.6780 - mae: 226.6780 - val_loss: 1374.4509 - val_mae: 1374.4509\n",
      "Epoch 62/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 332.4377 - mae: 332.4377\n",
      "Epoch 62: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 231.6242 - mae: 231.6242 - val_loss: 1290.2930 - val_mae: 1290.2930\n",
      "Epoch 63/100\n",
      " 6/18 [=========>....................] - ETA: 0s - loss: 221.1360 - mae: 221.1360\n",
      "Epoch 63: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 233.8660 - mae: 233.8660 - val_loss: 1327.8748 - val_mae: 1327.8748\n",
      "Epoch 64/100\n",
      " 7/18 [==========>...................] - ETA: 0s - loss: 265.5041 - mae: 265.5041\n",
      "Epoch 64: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 225.8528 - mae: 225.8528 - val_loss: 1280.2709 - val_mae: 1280.2709\n",
      "Epoch 65/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 237.3920 - mae: 237.3920\n",
      "Epoch 65: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 229.4352 - mae: 229.4352 - val_loss: 1324.1899 - val_mae: 1324.1899\n",
      "Epoch 66/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 171.6862 - mae: 171.6862\n",
      "Epoch 66: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 225.8249 - mae: 225.8249 - val_loss: 1542.0150 - val_mae: 1542.0150\n",
      "Epoch 67/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 251.4068 - mae: 251.4068\n",
      "Epoch 67: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 237.4485 - mae: 237.4485 - val_loss: 1318.3943 - val_mae: 1318.3943\n",
      "Epoch 68/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 199.7139 - mae: 199.7139\n",
      "Epoch 68: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 227.6932 - mae: 227.6932 - val_loss: 1311.2524 - val_mae: 1311.2524\n",
      "Epoch 69/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 220.1765 - mae: 220.1765\n",
      "Epoch 69: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 225.3952 - mae: 225.3952 - val_loss: 1495.3613 - val_mae: 1495.3613\n",
      "Epoch 70/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 280.6599 - mae: 280.6599\n",
      "Epoch 70: val_loss did not improve from 1279.53442\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 229.2415 - mae: 229.2415 - val_loss: 1314.1410 - val_mae: 1314.1410\n",
      "Epoch 71/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 195.9930 - mae: 195.9930\n",
      "Epoch 71: val_loss improved from 1279.53442 to 1258.14832, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 224.5727 - mae: 224.5727 - val_loss: 1258.1483 - val_mae: 1258.1483\n",
      "Epoch 72/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 262.0898 - mae: 262.0898\n",
      "Epoch 72: val_loss improved from 1258.14832 to 1245.29565, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 227.0043 - mae: 227.0043 - val_loss: 1245.2957 - val_mae: 1245.2957\n",
      "Epoch 73/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 249.1324 - mae: 249.1324\n",
      "Epoch 73: val_loss did not improve from 1245.29565\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 222.8278 - mae: 222.8278 - val_loss: 1270.6742 - val_mae: 1270.6742\n",
      "Epoch 74/100\n",
      " 6/18 [=========>....................] - ETA: 0s - loss: 214.3616 - mae: 214.3616\n",
      "Epoch 74: val_loss did not improve from 1245.29565\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 224.3815 - mae: 224.3815 - val_loss: 1402.2083 - val_mae: 1402.2083\n",
      "Epoch 75/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 225.8356 - mae: 225.8356\n",
      "Epoch 75: val_loss did not improve from 1245.29565\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 225.4684 - mae: 225.4684 - val_loss: 1345.1549 - val_mae: 1345.1549\n",
      "Epoch 76/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 213.7949 - mae: 213.7949\n",
      "Epoch 76: val_loss did not improve from 1245.29565\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 226.9647 - mae: 226.9647 - val_loss: 1333.5864 - val_mae: 1333.5864\n",
      "Epoch 77/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 210.5345 - mae: 210.5345\n",
      "Epoch 77: val_loss did not improve from 1245.29565\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 227.0477 - mae: 227.0477 - val_loss: 1251.5227 - val_mae: 1251.5227\n",
      "Epoch 78/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 188.1850 - mae: 188.1850\n",
      "Epoch 78: val_loss did not improve from 1245.29565\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 225.4304 - mae: 225.4304 - val_loss: 1279.8044 - val_mae: 1279.8044\n",
      "Epoch 79/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 202.5717 - mae: 202.5717\n",
      "Epoch 79: val_loss did not improve from 1245.29565\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 226.8488 - mae: 226.8488 - val_loss: 1337.9700 - val_mae: 1337.9700\n",
      "Epoch 80/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 209.7766 - mae: 209.7766\n",
      "Epoch 80: val_loss did not improve from 1245.29565\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 227.8309 - mae: 227.8309 - val_loss: 1301.1176 - val_mae: 1301.1176\n",
      "Epoch 81/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 226.6908 - mae: 226.6908\n",
      "Epoch 81: val_loss improved from 1245.29565 to 1238.77820, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 222.8103 - mae: 222.8103 - val_loss: 1238.7782 - val_mae: 1238.7782\n",
      "Epoch 82/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 215.6367 - mae: 215.6367\n",
      "Epoch 82: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 221.6927 - mae: 221.6927 - val_loss: 1254.5577 - val_mae: 1254.5577\n",
      "Epoch 83/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 215.3052 - mae: 215.3052\n",
      "Epoch 83: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 222.2941 - mae: 222.2941 - val_loss: 1312.1594 - val_mae: 1312.1594\n",
      "Epoch 84/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 226.4648 - mae: 226.4648\n",
      "Epoch 84: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 225.7592 - mae: 225.7592 - val_loss: 1275.3396 - val_mae: 1275.3396\n",
      "Epoch 85/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 264.5125 - mae: 264.5125\n",
      "Epoch 85: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 222.7701 - mae: 222.7701 - val_loss: 1246.6150 - val_mae: 1246.6150\n",
      "Epoch 86/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 226.0112 - mae: 226.0112\n",
      "Epoch 86: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 223.8966 - mae: 223.8966 - val_loss: 1293.1306 - val_mae: 1293.1306\n",
      "Epoch 87/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 249.4889 - mae: 249.4889\n",
      "Epoch 87: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 230.4208 - mae: 230.4208 - val_loss: 1371.4434 - val_mae: 1371.4434\n",
      "Epoch 88/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 220.1933 - mae: 220.1933\n",
      "Epoch 88: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 228.1883 - mae: 228.1883 - val_loss: 1506.8027 - val_mae: 1506.8027\n",
      "Epoch 89/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 230.4716 - mae: 230.4716\n",
      "Epoch 89: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 230.2472 - mae: 230.2472 - val_loss: 1355.8489 - val_mae: 1355.8489\n",
      "Epoch 90/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 255.9240 - mae: 255.9240\n",
      "Epoch 90: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 240.0667 - mae: 240.0667 - val_loss: 1468.7733 - val_mae: 1468.7733\n",
      "Epoch 91/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 220.6098 - mae: 220.6098\n",
      "Epoch 91: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 238.7536 - mae: 238.7536 - val_loss: 1347.7236 - val_mae: 1347.7236\n",
      "Epoch 92/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 217.4705 - mae: 217.4705\n",
      "Epoch 92: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 224.9981 - mae: 224.9981 - val_loss: 1268.7018 - val_mae: 1268.7018\n",
      "Epoch 93/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 237.0133 - mae: 237.0133\n",
      "Epoch 93: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 226.7187 - mae: 226.7187 - val_loss: 1341.9410 - val_mae: 1341.9410\n",
      "Epoch 94/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 254.8148 - mae: 254.8148\n",
      "Epoch 94: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 225.7565 - mae: 225.7565 - val_loss: 1317.2158 - val_mae: 1317.2158\n",
      "Epoch 95/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 256.4658 - mae: 256.4658\n",
      "Epoch 95: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 222.3831 - mae: 222.3831 - val_loss: 1267.4240 - val_mae: 1267.4240\n",
      "Epoch 96/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 183.9421 - mae: 183.9421\n",
      "Epoch 96: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 226.0979 - mae: 226.0979 - val_loss: 1289.6829 - val_mae: 1289.6829\n",
      "Epoch 97/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 248.5155 - mae: 248.5155\n",
      "Epoch 97: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 223.5443 - mae: 223.5443 - val_loss: 1401.8895 - val_mae: 1401.8895\n",
      "Epoch 98/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 219.5942 - mae: 219.5942\n",
      "Epoch 98: val_loss did not improve from 1238.77820\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 224.8748 - mae: 224.8748 - val_loss: 1243.4143 - val_mae: 1243.4143\n",
      "Epoch 99/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 169.8210 - mae: 169.8210\n",
      "Epoch 99: val_loss improved from 1238.77820 to 1235.78284, saving model to ./saves/10//model_3_dense/model_3_dense\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 218.7591 - mae: 218.7591 - val_loss: 1235.7828 - val_mae: 1235.7828\n",
      "Epoch 100/100\n",
      " 3/18 [====>.........................] - ETA: 1s - loss: 195.4841 - mae: 195.4841\n",
      "Epoch 100: val_loss did not improve from 1235.78284\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 220.2290 - mae: 220.2290 - val_loss: 1357.6012 - val_mae: 1357.6012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x131d02c70>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_3 = tf.keras.Sequential([\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(HORIZON),\n",
    "],\n",
    "                              name=\"model_3_dense\")\n",
    "\n",
    "model_3.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mae\"])\n",
    "\n",
    "model_3.fit(train_windows,\n",
    "            train_labels,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_3.name)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_4 Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 1\n",
    "WINDOW_SIZE = 7\n",
    "\n",
    "full_windows, full_labels = make_windows(btc_prices,\n",
    "                                         WINDOW_SIZE,\n",
    "                                         horizon=HORIZON)\n",
    "\n",
    "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(\n",
    "    windows=full_windows, labels=full_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_windows[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before we pass our data to conv1d layer we have to reshape it to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=float64, numpy=\n",
       "array([123.65499, 125.455  , 108.58483, 118.67466, 121.33866, 120.65533,\n",
       "       121.795  ])>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant(train_windows[0])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_dims_layer = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: [123.65499 125.455   108.58483 118.67466 121.33866 120.65533 121.795  ]\n",
      "original: (7,)\n",
      "expanded shape: (7, 1)\n",
      "original with expanded shape: [[123.65499]\n",
      " [125.455  ]\n",
      " [108.58483]\n",
      " [118.67466]\n",
      " [121.33866]\n",
      " [120.65533]\n",
      " [121.795  ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"original: {x}\")\n",
    "print(f\"original: {x.shape}\")\n",
    "print(f\"expanded shape: {expand_dims_layer(x).shape}\")\n",
    "print(f\"original with expanded shape: {expand_dims_layer(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 2049.0298 - mae: 2049.0298\n",
      "Epoch 1: val_loss improved from inf to 2620.25610, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2049.0298 - mae: 2049.0298 - val_loss: 2620.2561 - val_mae: 2620.2561\n",
      "Epoch 2/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 359.0080 - mae: 359.0080\n",
      "Epoch 2: val_loss improved from 2620.25610 to 1424.57959, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 318.4798 - mae: 318.4798 - val_loss: 1424.5796 - val_mae: 1424.5796\n",
      "Epoch 3/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 186.7054 - mae: 186.7054\n",
      "Epoch 3: val_loss improved from 1424.57959 to 1026.67651, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 215.4766 - mae: 215.4766 - val_loss: 1026.6765 - val_mae: 1026.6765\n",
      "Epoch 4/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 226.3113 - mae: 226.3113\n",
      "Epoch 4: val_loss improved from 1026.67651 to 988.21246, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 182.8083 - mae: 182.8083 - val_loss: 988.2125 - val_mae: 988.2125\n",
      "Epoch 5/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 186.2432 - mae: 186.2432\n",
      "Epoch 5: val_loss did not improve from 988.21246\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 176.5701 - mae: 176.5701 - val_loss: 992.7846 - val_mae: 992.7846\n",
      "Epoch 6/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 130.3551 - mae: 130.3551\n",
      "Epoch 6: val_loss improved from 988.21246 to 970.04651, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 172.8204 - mae: 172.8204 - val_loss: 970.0465 - val_mae: 970.0465\n",
      "Epoch 7/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 212.3408 - mae: 212.3408\n",
      "Epoch 7: val_loss improved from 970.04651 to 951.90399, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 171.2348 - mae: 171.2348 - val_loss: 951.9040 - val_mae: 951.9040\n",
      "Epoch 8/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 168.1665 - mae: 168.1665\n",
      "Epoch 8: val_loss improved from 951.90399 to 948.46625, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 168.4142 - mae: 168.4142 - val_loss: 948.4662 - val_mae: 948.4662\n",
      "Epoch 9/100\n",
      " 7/18 [==========>...................] - ETA: 0s - loss: 157.9747 - mae: 157.9747\n",
      "Epoch 9: val_loss improved from 948.46625 to 947.56555, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 166.3161 - mae: 166.3161 - val_loss: 947.5656 - val_mae: 947.5656\n",
      "Epoch 10/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 178.1192 - mae: 178.1192\n",
      "Epoch 10: val_loss improved from 947.56555 to 918.73395, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 163.7345 - mae: 163.7345 - val_loss: 918.7339 - val_mae: 918.7339\n",
      "Epoch 11/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 161.9961 - mae: 161.9961\n",
      "Epoch 11: val_loss improved from 918.73395 to 897.09015, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 161.2245 - mae: 161.2245 - val_loss: 897.0901 - val_mae: 897.0901\n",
      "Epoch 12/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 159.6643 - mae: 159.6643\n",
      "Epoch 12: val_loss did not improve from 897.09015\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 159.5175 - mae: 159.5175 - val_loss: 951.7358 - val_mae: 951.7358\n",
      "Epoch 13/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 157.1346 - mae: 157.1346\n",
      "Epoch 13: val_loss improved from 897.09015 to 888.49872, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 157.3886 - mae: 157.3886 - val_loss: 888.4987 - val_mae: 888.4987\n",
      "Epoch 14/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 151.2538 - mae: 151.2538\n",
      "Epoch 14: val_loss did not improve from 888.49872\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 154.0532 - mae: 154.0532 - val_loss: 893.4568 - val_mae: 893.4568\n",
      "Epoch 15/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 156.9896 - mae: 156.9896\n",
      "Epoch 15: val_loss improved from 888.49872 to 870.39355, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 152.6875 - mae: 152.6875 - val_loss: 870.3936 - val_mae: 870.3936\n",
      "Epoch 16/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 140.4766 - mae: 140.4766\n",
      "Epoch 16: val_loss improved from 870.39355 to 843.34430, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 149.5638 - mae: 149.5638 - val_loss: 843.3443 - val_mae: 843.3443\n",
      "Epoch 17/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 144.0055 - mae: 144.0055\n",
      "Epoch 17: val_loss improved from 843.34430 to 823.69391, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 148.7675 - mae: 148.7675 - val_loss: 823.6939 - val_mae: 823.6939\n",
      "Epoch 18/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 150.0906 - mae: 150.0906\n",
      "Epoch 18: val_loss improved from 823.69391 to 822.73242, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 145.5192 - mae: 145.5192 - val_loss: 822.7324 - val_mae: 822.7324\n",
      "Epoch 19/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 79.6949 - mae: 79.6949\n",
      "Epoch 19: val_loss did not improve from 822.73242\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 142.6375 - mae: 142.6375 - val_loss: 825.2520 - val_mae: 825.2520\n",
      "Epoch 20/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 152.5802 - mae: 152.5802\n",
      "Epoch 20: val_loss improved from 822.73242 to 805.63129, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 143.0133 - mae: 143.0133 - val_loss: 805.6313 - val_mae: 805.6313\n",
      "Epoch 21/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 200.3138 - mae: 200.3138\n",
      "Epoch 21: val_loss improved from 805.63129 to 773.42194, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 142.0460 - mae: 142.0460 - val_loss: 773.4219 - val_mae: 773.4220\n",
      "Epoch 22/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 186.4044 - mae: 186.4044\n",
      "Epoch 22: val_loss did not improve from 773.42194\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 140.9260 - mae: 140.9260 - val_loss: 774.9353 - val_mae: 774.9353\n",
      "Epoch 23/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 135.0119 - mae: 135.0119\n",
      "Epoch 23: val_loss improved from 773.42194 to 765.44543, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 136.1732 - mae: 136.1732 - val_loss: 765.4454 - val_mae: 765.4454\n",
      "Epoch 24/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 152.2098 - mae: 152.2098\n",
      "Epoch 24: val_loss improved from 765.44543 to 749.10492, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 135.9409 - mae: 135.9409 - val_loss: 749.1049 - val_mae: 749.1049\n",
      "Epoch 25/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 137.5601 - mae: 137.5601\n",
      "Epoch 25: val_loss improved from 749.10492 to 736.03937, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 133.2695 - mae: 133.2695 - val_loss: 736.0394 - val_mae: 736.0394\n",
      "Epoch 26/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 143.0708 - mae: 143.0708\n",
      "Epoch 26: val_loss did not improve from 736.03937\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 134.3420 - mae: 134.3420 - val_loss: 745.0227 - val_mae: 745.0227\n",
      "Epoch 27/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 198.3149 - mae: 198.3149\n",
      "Epoch 27: val_loss improved from 736.03937 to 718.80646, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 132.3421 - mae: 132.3421 - val_loss: 718.8065 - val_mae: 718.8065\n",
      "Epoch 28/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 131.5713 - mae: 131.5713\n",
      "Epoch 28: val_loss improved from 718.80646 to 710.89282, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 128.3956 - mae: 128.3956 - val_loss: 710.8928 - val_mae: 710.8928\n",
      "Epoch 29/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 125.2612 - mae: 125.2612\n",
      "Epoch 29: val_loss improved from 710.89282 to 701.36011, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 127.3645 - mae: 127.3645 - val_loss: 701.3601 - val_mae: 701.3601\n",
      "Epoch 30/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 144.2406 - mae: 144.2406\n",
      "Epoch 30: val_loss improved from 701.36011 to 699.55109, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 129.0611 - mae: 129.0611 - val_loss: 699.5511 - val_mae: 699.5511\n",
      "Epoch 31/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 142.6100 - mae: 142.6100\n",
      "Epoch 31: val_loss did not improve from 699.55109\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 124.8978 - mae: 124.8978 - val_loss: 733.0269 - val_mae: 733.0269\n",
      "Epoch 32/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 120.1888 - mae: 120.1888\n",
      "Epoch 32: val_loss did not improve from 699.55109\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 126.2197 - mae: 126.2197 - val_loss: 707.6859 - val_mae: 707.6859\n",
      "Epoch 33/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 155.2891 - mae: 155.2891\n",
      "Epoch 33: val_loss did not improve from 699.55109\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 123.4207 - mae: 123.4207 - val_loss: 710.4606 - val_mae: 710.4606\n",
      "Epoch 34/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 144.2326 - mae: 144.2326\n",
      "Epoch 34: val_loss improved from 699.55109 to 688.42944, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 124.2652 - mae: 124.2652 - val_loss: 688.4294 - val_mae: 688.4294\n",
      "Epoch 35/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 127.2584 - mae: 127.2584\n",
      "Epoch 35: val_loss improved from 688.42944 to 670.90308, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 123.8085 - mae: 123.8085 - val_loss: 670.9031 - val_mae: 670.9031\n",
      "Epoch 36/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 162.6347 - mae: 162.6347\n",
      "Epoch 36: val_loss did not improve from 670.90308\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 125.4418 - mae: 125.4418 - val_loss: 695.2070 - val_mae: 695.2070\n",
      "Epoch 37/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 154.2522 - mae: 154.2522\n",
      "Epoch 37: val_loss improved from 670.90308 to 651.99170, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 123.3902 - mae: 123.3902 - val_loss: 651.9917 - val_mae: 651.9917\n",
      "Epoch 38/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 116.8206 - mae: 116.8206\n",
      "Epoch 38: val_loss improved from 651.99170 to 648.61859, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 121.7854 - mae: 121.7854 - val_loss: 648.6186 - val_mae: 648.6186\n",
      "Epoch 39/100\n",
      " 8/18 [============>.................] - ETA: 0s - loss: 126.5574 - mae: 126.5574\n",
      "Epoch 39: val_loss did not improve from 648.61859\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 121.5752 - mae: 121.5752 - val_loss: 677.8600 - val_mae: 677.8600\n",
      "Epoch 40/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 120.1151 - mae: 120.1151\n",
      "Epoch 40: val_loss improved from 648.61859 to 647.46667, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 118.4716 - mae: 118.4716 - val_loss: 647.4667 - val_mae: 647.4667\n",
      "Epoch 41/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 110.8851 - mae: 110.8851\n",
      "Epoch 41: val_loss improved from 647.46667 to 634.25720, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 118.6408 - mae: 118.6408 - val_loss: 634.2572 - val_mae: 634.2572\n",
      "Epoch 42/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 116.0503 - mae: 116.0503\n",
      "Epoch 42: val_loss improved from 634.25720 to 631.30676, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 121.6200 - mae: 121.6200 - val_loss: 631.3068 - val_mae: 631.3068\n",
      "Epoch 43/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 93.2355 - mae: 93.2355\n",
      "Epoch 43: val_loss did not improve from 631.30676\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 117.1722 - mae: 117.1722 - val_loss: 670.0583 - val_mae: 670.0583\n",
      "Epoch 44/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 75.7415 - mae: 75.7415\n",
      "Epoch 44: val_loss improved from 631.30676 to 631.09106, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 120.0227 - mae: 120.0227 - val_loss: 631.0911 - val_mae: 631.0911\n",
      "Epoch 45/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 81.3602 - mae: 81.3602\n",
      "Epoch 45: val_loss improved from 631.09106 to 628.35413, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 116.8647 - mae: 116.8647 - val_loss: 628.3541 - val_mae: 628.3541\n",
      "Epoch 46/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 89.9004 - mae: 89.9004\n",
      "Epoch 46: val_loss did not improve from 628.35413\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 115.7931 - mae: 115.7931 - val_loss: 637.2686 - val_mae: 637.2686\n",
      "Epoch 47/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 105.1946 - mae: 105.1946\n",
      "Epoch 47: val_loss did not improve from 628.35413\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 118.6641 - mae: 118.6641 - val_loss: 773.7511 - val_mae: 773.7511\n",
      "Epoch 48/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 132.1949 - mae: 132.1949\n",
      "Epoch 48: val_loss did not improve from 628.35413\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 117.0159 - mae: 117.0159 - val_loss: 644.0920 - val_mae: 644.0920\n",
      "Epoch 49/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 162.6833 - mae: 162.6833\n",
      "Epoch 49: val_loss improved from 628.35413 to 611.08606, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 119.1105 - mae: 119.1105 - val_loss: 611.0861 - val_mae: 611.0861\n",
      "Epoch 50/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 134.4451 - mae: 134.4451\n",
      "Epoch 50: val_loss did not improve from 611.08606\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.2818 - mae: 113.2818 - val_loss: 629.6525 - val_mae: 629.6525\n",
      "Epoch 51/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 96.2452 - mae: 96.2452\n",
      "Epoch 51: val_loss did not improve from 611.08606\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 113.4122 - mae: 113.4122 - val_loss: 612.9730 - val_mae: 612.9730\n",
      "Epoch 52/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 124.8449 - mae: 124.8449\n",
      "Epoch 52: val_loss did not improve from 611.08606\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 114.3442 - mae: 114.3442 - val_loss: 617.8738 - val_mae: 617.8738\n",
      "Epoch 53/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 64.6456 - mae: 64.6456\n",
      "Epoch 53: val_loss did not improve from 611.08606\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 116.8485 - mae: 116.8485 - val_loss: 619.0039 - val_mae: 619.0039\n",
      "Epoch 54/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 105.9290 - mae: 105.9290\n",
      "Epoch 54: val_loss did not improve from 611.08606\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 118.8198 - mae: 118.8198 - val_loss: 674.5536 - val_mae: 674.5536\n",
      "Epoch 55/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 141.3692 - mae: 141.3692\n",
      "Epoch 55: val_loss improved from 611.08606 to 598.65015, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 113.7905 - mae: 113.7905 - val_loss: 598.6501 - val_mae: 598.6501\n",
      "Epoch 56/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 110.9205 - mae: 110.9205\n",
      "Epoch 56: val_loss did not improve from 598.65015\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.6876 - mae: 112.6876 - val_loss: 777.4764 - val_mae: 777.4764\n",
      "Epoch 57/100\n",
      " 8/18 [============>.................] - ETA: 0s - loss: 130.5990 - mae: 130.5990\n",
      "Epoch 57: val_loss did not improve from 598.65015\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 131.6736 - mae: 131.6736 - val_loss: 703.3051 - val_mae: 703.3051\n",
      "Epoch 58/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 123.7315 - mae: 123.7315\n",
      "Epoch 58: val_loss did not improve from 598.65015\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 123.5477 - mae: 123.5477 - val_loss: 685.3852 - val_mae: 685.3853\n",
      "Epoch 59/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 83.7497 - mae: 83.7497\n",
      "Epoch 59: val_loss did not improve from 598.65015\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.6713 - mae: 112.6713 - val_loss: 608.9357 - val_mae: 608.9357\n",
      "Epoch 60/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 96.2830 - mae: 96.2830\n",
      "Epoch 60: val_loss did not improve from 598.65015\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 113.3037 - mae: 113.3037 - val_loss: 668.0377 - val_mae: 668.0377\n",
      "Epoch 61/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 150.0503 - mae: 150.0503\n",
      "Epoch 61: val_loss did not improve from 598.65015\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 115.6404 - mae: 115.6404 - val_loss: 621.4360 - val_mae: 621.4360\n",
      "Epoch 62/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 84.5631 - mae: 84.5631\n",
      "Epoch 62: val_loss did not improve from 598.65015\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.6976 - mae: 111.6976 - val_loss: 627.3445 - val_mae: 627.3445\n",
      "Epoch 63/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 102.3530 - mae: 102.3530\n",
      "Epoch 63: val_loss improved from 598.65015 to 594.27209, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 116.2712 - mae: 116.2712 - val_loss: 594.2721 - val_mae: 594.2721\n",
      "Epoch 64/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 90.9079 - mae: 90.9079\n",
      "Epoch 64: val_loss did not improve from 594.27209\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 114.1165 - mae: 114.1165 - val_loss: 673.4912 - val_mae: 673.4912\n",
      "Epoch 65/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 142.6182 - mae: 142.6182\n",
      "Epoch 65: val_loss did not improve from 594.27209\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 116.5615 - mae: 116.5615 - val_loss: 597.5427 - val_mae: 597.5427\n",
      "Epoch 66/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 119.5144 - mae: 119.5144\n",
      "Epoch 66: val_loss improved from 594.27209 to 593.42047, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 112.5825 - mae: 112.5825 - val_loss: 593.4205 - val_mae: 593.4205\n",
      "Epoch 67/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 91.0558 - mae: 91.0558\n",
      "Epoch 67: val_loss did not improve from 593.42047\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 108.5709 - mae: 108.5709 - val_loss: 610.6395 - val_mae: 610.6395\n",
      "Epoch 68/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 66.7781 - mae: 66.7781\n",
      "Epoch 68: val_loss did not improve from 593.42047\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 114.0238 - mae: 114.0238 - val_loss: 698.4660 - val_mae: 698.4660\n",
      "Epoch 69/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 109.1329 - mae: 109.1329\n",
      "Epoch 69: val_loss did not improve from 593.42047\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 128.5439 - mae: 128.5439 - val_loss: 691.7802 - val_mae: 691.7802\n",
      "Epoch 70/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 134.7210 - mae: 134.7210\n",
      "Epoch 70: val_loss improved from 593.42047 to 583.40417, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 119.8453 - mae: 119.8453 - val_loss: 583.4042 - val_mae: 583.4042\n",
      "Epoch 71/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 89.3376 - mae: 89.3376\n",
      "Epoch 71: val_loss did not improve from 583.40417\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.8937 - mae: 111.8937 - val_loss: 614.8133 - val_mae: 614.8133\n",
      "Epoch 72/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 105.6095 - mae: 105.6095\n",
      "Epoch 72: val_loss did not improve from 583.40417\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.5224 - mae: 111.5224 - val_loss: 614.3988 - val_mae: 614.3988\n",
      "Epoch 73/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 122.1941 - mae: 122.1941\n",
      "Epoch 73: val_loss did not improve from 583.40417\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.2351 - mae: 111.2351 - val_loss: 612.3290 - val_mae: 612.3290\n",
      "Epoch 74/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 167.8013 - mae: 167.8013\n",
      "Epoch 74: val_loss did not improve from 583.40417\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.5117 - mae: 110.5117 - val_loss: 593.8785 - val_mae: 593.8785\n",
      "Epoch 75/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 127.7992 - mae: 127.7992\n",
      "Epoch 75: val_loss did not improve from 583.40417\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.5684 - mae: 110.5684 - val_loss: 613.3543 - val_mae: 613.3543\n",
      "Epoch 76/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 81.4476 - mae: 81.4476\n",
      "Epoch 76: val_loss improved from 583.40417 to 579.91461, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8843 - mae: 109.8843 - val_loss: 579.9146 - val_mae: 579.9146\n",
      "Epoch 77/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 84.6901 - mae: 84.6901\n",
      "Epoch 77: val_loss improved from 579.91461 to 577.59296, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.8530 - mae: 110.8530 - val_loss: 577.5930 - val_mae: 577.5930\n",
      "Epoch 78/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 111.2206 - mae: 111.2206\n",
      "Epoch 78: val_loss improved from 577.59296 to 577.51642, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 110.4087 - mae: 110.4087 - val_loss: 577.5164 - val_mae: 577.5164\n",
      "Epoch 79/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 105.1688 - mae: 105.1688\n",
      "Epoch 79: val_loss did not improve from 577.51642\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.5126 - mae: 110.5126 - val_loss: 610.6715 - val_mae: 610.6715\n",
      "Epoch 80/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 118.4534 - mae: 118.4534\n",
      "Epoch 80: val_loss did not improve from 577.51642\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.1653 - mae: 109.1653 - val_loss: 606.2216 - val_mae: 606.2216\n",
      "Epoch 81/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 98.3286 - mae: 98.3286\n",
      "Epoch 81: val_loss did not improve from 577.51642\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.0062 - mae: 110.0062 - val_loss: 578.4462 - val_mae: 578.4462\n",
      "Epoch 82/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 199.8544 - mae: 199.8544\n",
      "Epoch 82: val_loss did not improve from 577.51642\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 116.0274 - mae: 116.0274 - val_loss: 593.3561 - val_mae: 593.3561\n",
      "Epoch 83/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 115.2648 - mae: 115.2648\n",
      "Epoch 83: val_loss did not improve from 577.51642\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.0755 - mae: 111.0755 - val_loss: 626.1759 - val_mae: 626.1759\n",
      "Epoch 84/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 113.2475 - mae: 113.2475\n",
      "Epoch 84: val_loss did not improve from 577.51642\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.5437 - mae: 110.5437 - val_loss: 578.0347 - val_mae: 578.0347\n",
      "Epoch 85/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 85.7405 - mae: 85.7405\n",
      "Epoch 85: val_loss did not improve from 577.51642\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 107.7697 - mae: 107.7697 - val_loss: 597.1945 - val_mae: 597.1945\n",
      "Epoch 86/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 110.4536 - mae: 110.4536\n",
      "Epoch 86: val_loss did not improve from 577.51642\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.0191 - mae: 109.0191 - val_loss: 610.4032 - val_mae: 610.4032\n",
      "Epoch 87/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 124.0580 - mae: 124.0580\n",
      "Epoch 87: val_loss improved from 577.51642 to 574.62909, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.7870 - mae: 109.7870 - val_loss: 574.6291 - val_mae: 574.6291\n",
      "Epoch 88/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 125.4508 - mae: 125.4508\n",
      "Epoch 88: val_loss did not improve from 574.62909\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 108.7128 - mae: 108.7128 - val_loss: 602.5320 - val_mae: 602.5320\n",
      "Epoch 89/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 107.7674 - mae: 107.7674\n",
      "Epoch 89: val_loss improved from 574.62909 to 573.17084, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.1752 - mae: 110.1752 - val_loss: 573.1708 - val_mae: 573.1708\n",
      "Epoch 90/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 122.1744 - mae: 122.1744\n",
      "Epoch 90: val_loss did not improve from 573.17084\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 108.1406 - mae: 108.1406 - val_loss: 673.6931 - val_mae: 673.6931\n",
      "Epoch 91/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 103.8296 - mae: 103.8296\n",
      "Epoch 91: val_loss did not improve from 573.17084\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 112.7602 - mae: 112.7602 - val_loss: 617.6549 - val_mae: 617.6549\n",
      "Epoch 92/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 96.9641 - mae: 96.9641\n",
      "Epoch 92: val_loss did not improve from 573.17084\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.2951 - mae: 109.2951 - val_loss: 579.0809 - val_mae: 579.0809\n",
      "Epoch 93/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 109.6812 - mae: 109.6812\n",
      "Epoch 93: val_loss did not improve from 573.17084\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 114.6005 - mae: 114.6005 - val_loss: 618.2094 - val_mae: 618.2094\n",
      "Epoch 94/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 146.2949 - mae: 146.2949\n",
      "Epoch 94: val_loss did not improve from 573.17084\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.7752 - mae: 109.7752 - val_loss: 573.8305 - val_mae: 573.8305\n",
      "Epoch 95/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 136.7010 - mae: 136.7010\n",
      "Epoch 95: val_loss did not improve from 573.17084\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 107.2119 - mae: 107.2119 - val_loss: 584.3721 - val_mae: 584.3721\n",
      "Epoch 96/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 115.8556 - mae: 115.8556\n",
      "Epoch 96: val_loss did not improve from 573.17084\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.3409 - mae: 111.3409 - val_loss: 607.4700 - val_mae: 607.4700\n",
      "Epoch 97/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 105.3350 - mae: 105.3350\n",
      "Epoch 97: val_loss did not improve from 573.17084\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.1084 - mae: 110.1084 - val_loss: 578.9484 - val_mae: 578.9484\n",
      "Epoch 98/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 117.5409 - mae: 117.5409\n",
      "Epoch 98: val_loss improved from 573.17084 to 570.19977, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 108.9533 - mae: 108.9533 - val_loss: 570.1998 - val_mae: 570.1998\n",
      "Epoch 99/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 150.5092 - mae: 150.5092\n",
      "Epoch 99: val_loss improved from 570.19977 to 569.72717, saving model to ./saves/10//model_4_conv1d/model_4_conv1d\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 107.5394 - mae: 107.5394 - val_loss: 569.7272 - val_mae: 569.7272\n",
      "Epoch 100/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 115.8756 - mae: 115.8756\n",
      "Epoch 100: val_loss did not improve from 569.72717\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.4472 - mae: 112.4472 - val_loss: 660.8001 - val_mae: 660.8001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x131e04e20>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_4 = tf.keras.Sequential([\n",
    "    layers.Lambda(lambda x: tf.expand_dims(x, axis=1)),\n",
    "    layers.Conv1D(\n",
    "        128, activation=\"relu\", kernel_size=5, strides=1, padding=\"causal\"),\n",
    "    layers.Dense(HORIZON),\n",
    "],\n",
    "                              name=\"model_4_conv1d\")\n",
    "\n",
    "model_4.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mae\"])\n",
    "\n",
    "model_4.fit(train_windows,\n",
    "            train_labels,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_4.name)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_conv1d\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda_1 (Lambda)           (None, 1, 7)              0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1, 128)            4608      \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1, 1)              129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 3ms/step - loss: 660.8000 - mae: 660.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[660.800048828125, 660.800048828125]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.evaluate(test_windows, test_labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_5 RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 2739.9277 - mae: 2739.9277 \n",
      "Epoch 1: val_loss improved from inf to 10974.82812, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 3s 23ms/step - loss: 2613.0273 - mae: 2613.0273 - val_loss: 10974.8281 - val_mae: 10974.8281\n",
      "Epoch 2/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 1136.1042 - mae: 1136.1042\n",
      "Epoch 2: val_loss improved from 10974.82812 to 1085.40137, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 921.3350 - mae: 921.3350 - val_loss: 1085.4014 - val_mae: 1085.4014\n",
      "Epoch 3/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 281.2699 - mae: 281.2699\n",
      "Epoch 3: val_loss did not improve from 1085.40137\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 252.5538 - mae: 252.5538 - val_loss: 1306.4399 - val_mae: 1306.4399\n",
      "Epoch 4/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 199.1075 - mae: 199.1075\n",
      "Epoch 4: val_loss improved from 1085.40137 to 1044.07605, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 197.3025 - mae: 197.3025 - val_loss: 1044.0760 - val_mae: 1044.0760\n",
      "Epoch 5/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 191.6133 - mae: 191.6133\n",
      "Epoch 5: val_loss improved from 1044.07605 to 1028.07654, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 185.6659 - mae: 185.6659 - val_loss: 1028.0765 - val_mae: 1028.0765\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 186.5072 - mae: 186.5072\n",
      "Epoch 6: val_loss improved from 1028.07654 to 1013.93298, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 186.5072 - mae: 186.5072 - val_loss: 1013.9330 - val_mae: 1013.9330\n",
      "Epoch 7/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 187.6513 - mae: 187.6513\n",
      "Epoch 7: val_loss did not improve from 1013.93298\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 181.1198 - mae: 181.1198 - val_loss: 1016.8678 - val_mae: 1016.8678\n",
      "Epoch 8/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 175.4173 - mae: 175.4173\n",
      "Epoch 8: val_loss improved from 1013.93298 to 997.67291, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 177.4033 - mae: 177.4033 - val_loss: 997.6729 - val_mae: 997.6729\n",
      "Epoch 9/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 170.3052 - mae: 170.3052\n",
      "Epoch 9: val_loss did not improve from 997.67291\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 174.5674 - mae: 174.5674 - val_loss: 1004.3951 - val_mae: 1004.3951\n",
      "Epoch 10/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 170.2465 - mae: 170.2465\n",
      "Epoch 10: val_loss improved from 997.67291 to 988.49451, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 174.3211 - mae: 174.3211 - val_loss: 988.4945 - val_mae: 988.4945\n",
      "Epoch 11/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 172.9184 - mae: 172.9184\n",
      "Epoch 11: val_loss improved from 988.49451 to 960.60846, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 172.8688 - mae: 172.8688 - val_loss: 960.6085 - val_mae: 960.6085\n",
      "Epoch 12/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 172.7365 - mae: 172.7365\n",
      "Epoch 12: val_loss did not improve from 960.60846\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 170.8699 - mae: 170.8699 - val_loss: 969.4772 - val_mae: 969.4772\n",
      "Epoch 13/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 163.1050 - mae: 163.1050\n",
      "Epoch 13: val_loss did not improve from 960.60846\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 167.7195 - mae: 167.7195 - val_loss: 975.2131 - val_mae: 975.2131\n",
      "Epoch 14/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 163.8171 - mae: 163.8171\n",
      "Epoch 14: val_loss did not improve from 960.60846\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 165.4856 - mae: 165.4856 - val_loss: 966.2065 - val_mae: 966.2064\n",
      "Epoch 15/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 163.4140 - mae: 163.4140\n",
      "Epoch 15: val_loss improved from 960.60846 to 915.96405, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 164.8363 - mae: 164.8363 - val_loss: 915.9641 - val_mae: 915.9641\n",
      "Epoch 16/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 158.6144 - mae: 158.6144\n",
      "Epoch 16: val_loss improved from 915.96405 to 895.01947, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 159.5708 - mae: 159.5708 - val_loss: 895.0195 - val_mae: 895.0195\n",
      "Epoch 17/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 156.3764 - mae: 156.3764\n",
      "Epoch 17: val_loss did not improve from 895.01947\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 156.8147 - mae: 156.8147 - val_loss: 935.3296 - val_mae: 935.3296\n",
      "Epoch 18/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 162.6768 - mae: 162.6768\n",
      "Epoch 18: val_loss improved from 895.01947 to 887.02759, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 155.5172 - mae: 155.5172 - val_loss: 887.0276 - val_mae: 887.0276\n",
      "Epoch 19/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 149.7607 - mae: 149.7607\n",
      "Epoch 19: val_loss did not improve from 887.02759\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 150.9999 - mae: 150.9999 - val_loss: 890.7158 - val_mae: 890.7158\n",
      "Epoch 20/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 152.8429 - mae: 152.8429\n",
      "Epoch 20: val_loss improved from 887.02759 to 824.42084, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 150.1642 - mae: 150.1642 - val_loss: 824.4208 - val_mae: 824.4208\n",
      "Epoch 21/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 152.1221 - mae: 152.1221\n",
      "Epoch 21: val_loss did not improve from 824.42084\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 147.2578 - mae: 147.2578 - val_loss: 836.8435 - val_mae: 836.8435\n",
      "Epoch 22/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 155.5347 - mae: 155.5347\n",
      "Epoch 22: val_loss improved from 824.42084 to 797.60895, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 143.9372 - mae: 143.9372 - val_loss: 797.6089 - val_mae: 797.6089\n",
      "Epoch 23/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 135.2749 - mae: 135.2749\n",
      "Epoch 23: val_loss did not improve from 797.60895\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 140.5100 - mae: 140.5100 - val_loss: 814.5417 - val_mae: 814.5417\n",
      "Epoch 24/100\n",
      " 7/18 [==========>...................] - ETA: 0s - loss: 130.0733 - mae: 130.0733\n",
      "Epoch 24: val_loss improved from 797.60895 to 776.46704, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 139.0534 - mae: 139.0534 - val_loss: 776.4670 - val_mae: 776.4670\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 135.7394 - mae: 135.7394\n",
      "Epoch 25: val_loss improved from 776.46704 to 749.10925, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 135.7394 - mae: 135.7394 - val_loss: 749.1093 - val_mae: 749.1093\n",
      "Epoch 26/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 136.9653 - mae: 136.9653\n",
      "Epoch 26: val_loss improved from 749.10925 to 742.49091, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 134.6257 - mae: 134.6257 - val_loss: 742.4909 - val_mae: 742.4909\n",
      "Epoch 27/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 135.5645 - mae: 135.5645\n",
      "Epoch 27: val_loss improved from 742.49091 to 723.25220, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 132.4399 - mae: 132.4399 - val_loss: 723.2522 - val_mae: 723.2522\n",
      "Epoch 28/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 135.6123 - mae: 135.6123\n",
      "Epoch 28: val_loss improved from 723.25220 to 710.67194, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 130.2139 - mae: 130.2139 - val_loss: 710.6719 - val_mae: 710.6719\n",
      "Epoch 29/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 132.2103 - mae: 132.2103\n",
      "Epoch 29: val_loss improved from 710.67194 to 704.59204, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 128.9356 - mae: 128.9356 - val_loss: 704.5920 - val_mae: 704.5920\n",
      "Epoch 30/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 123.5088 - mae: 123.5088\n",
      "Epoch 30: val_loss did not improve from 704.59204\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 129.2298 - mae: 129.2298 - val_loss: 887.4506 - val_mae: 887.4506\n",
      "Epoch 31/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 139.0517 - mae: 139.0517\n",
      "Epoch 31: val_loss improved from 704.59204 to 690.97540, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 137.7021 - mae: 137.7021 - val_loss: 690.9754 - val_mae: 690.9754\n",
      "Epoch 32/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 127.1904 - mae: 127.1904\n",
      "Epoch 32: val_loss improved from 690.97540 to 678.02362, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 125.5663 - mae: 125.5663 - val_loss: 678.0236 - val_mae: 678.0236\n",
      "Epoch 33/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 127.9161 - mae: 127.9161\n",
      "Epoch 33: val_loss improved from 678.02362 to 672.62598, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 124.4403 - mae: 124.4403 - val_loss: 672.6260 - val_mae: 672.6260\n",
      "Epoch 34/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 121.8626 - mae: 121.8626\n",
      "Epoch 34: val_loss improved from 672.62598 to 668.42163, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 121.8499 - mae: 121.8499 - val_loss: 668.4216 - val_mae: 668.4216\n",
      "Epoch 35/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 120.2346 - mae: 120.2346\n",
      "Epoch 35: val_loss improved from 668.42163 to 663.79150, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 120.7474 - mae: 120.7474 - val_loss: 663.7915 - val_mae: 663.7915\n",
      "Epoch 36/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 117.7734 - mae: 117.7734\n",
      "Epoch 36: val_loss did not improve from 663.79150\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 124.2259 - mae: 124.2259 - val_loss: 672.4855 - val_mae: 672.4855\n",
      "Epoch 37/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 112.9086 - mae: 112.9086\n",
      "Epoch 37: val_loss improved from 663.79150 to 657.25134, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 120.3318 - mae: 120.3318 - val_loss: 657.2513 - val_mae: 657.2513\n",
      "Epoch 38/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 124.1544 - mae: 124.1544\n",
      "Epoch 38: val_loss improved from 657.25134 to 639.55139, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 118.7870 - mae: 118.7870 - val_loss: 639.5514 - val_mae: 639.5514\n",
      "Epoch 39/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 123.7264 - mae: 123.7264\n",
      "Epoch 39: val_loss did not improve from 639.55139\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 120.1358 - mae: 120.1358 - val_loss: 702.6637 - val_mae: 702.6636\n",
      "Epoch 40/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 120.2491 - mae: 120.2491\n",
      "Epoch 40: val_loss did not improve from 639.55139\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 119.4465 - mae: 119.4465 - val_loss: 643.6772 - val_mae: 643.6772\n",
      "Epoch 41/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 125.9245 - mae: 125.9245\n",
      "Epoch 41: val_loss did not improve from 639.55139\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 119.9490 - mae: 119.9490 - val_loss: 659.7174 - val_mae: 659.7174\n",
      "Epoch 42/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 117.1859 - mae: 117.1859\n",
      "Epoch 42: val_loss improved from 639.55139 to 638.72943, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 117.7047 - mae: 117.7047 - val_loss: 638.7294 - val_mae: 638.7294\n",
      "Epoch 43/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 115.8807 - mae: 115.8807\n",
      "Epoch 43: val_loss improved from 638.72943 to 622.00244, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 116.1518 - mae: 116.1518 - val_loss: 622.0024 - val_mae: 622.0024\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 115.7007 - mae: 115.7007\n",
      "Epoch 44: val_loss did not improve from 622.00244\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 115.7007 - mae: 115.7007 - val_loss: 627.8473 - val_mae: 627.8473\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 115.2886 - mae: 115.2886\n",
      "Epoch 45: val_loss improved from 622.00244 to 617.48285, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 115.2886 - mae: 115.2886 - val_loss: 617.4828 - val_mae: 617.4828\n",
      "Epoch 46/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 119.0361 - mae: 119.0361\n",
      "Epoch 46: val_loss did not improve from 617.48285\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 115.0116 - mae: 115.0116 - val_loss: 617.8051 - val_mae: 617.8051\n",
      "Epoch 47/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 110.2414 - mae: 110.2414\n",
      "Epoch 47: val_loss improved from 617.48285 to 612.78766, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 118.9199 - mae: 118.9199 - val_loss: 612.7877 - val_mae: 612.7877\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 117.6975 - mae: 117.6975\n",
      "Epoch 48: val_loss did not improve from 612.78766\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 117.6975 - mae: 117.6975 - val_loss: 613.9109 - val_mae: 613.9109\n",
      "Epoch 49/100\n",
      " 8/18 [============>.................] - ETA: 0s - loss: 120.9530 - mae: 120.9530\n",
      "Epoch 49: val_loss did not improve from 612.78766\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 114.3017 - mae: 114.3017 - val_loss: 619.7523 - val_mae: 619.7523\n",
      "Epoch 50/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 121.0994 - mae: 121.0994\n",
      "Epoch 50: val_loss did not improve from 612.78766\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 119.6992 - mae: 119.6992 - val_loss: 640.2747 - val_mae: 640.2747\n",
      "Epoch 51/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 112.8715 - mae: 112.8715\n",
      "Epoch 51: val_loss did not improve from 612.78766\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 114.8373 - mae: 114.8373 - val_loss: 613.8228 - val_mae: 613.8228\n",
      "Epoch 52/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 121.5226 - mae: 121.5226\n",
      "Epoch 52: val_loss improved from 612.78766 to 603.53430, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 114.2275 - mae: 114.2275 - val_loss: 603.5343 - val_mae: 603.5343\n",
      "Epoch 53/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 115.6363 - mae: 115.6363\n",
      "Epoch 53: val_loss improved from 603.53430 to 603.39722, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 19ms/step - loss: 114.9034 - mae: 114.9034 - val_loss: 603.3972 - val_mae: 603.3972\n",
      "Epoch 54/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 115.6296 - mae: 115.6296\n",
      "Epoch 54: val_loss did not improve from 603.39722\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 113.2380 - mae: 113.2380 - val_loss: 619.9258 - val_mae: 619.9258\n",
      "Epoch 55/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 115.3847 - mae: 115.3847\n",
      "Epoch 55: val_loss improved from 603.39722 to 599.71088, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 113.7643 - mae: 113.7643 - val_loss: 599.7109 - val_mae: 599.7109\n",
      "Epoch 56/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 114.8529 - mae: 114.8529\n",
      "Epoch 56: val_loss did not improve from 599.71088\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 117.6708 - mae: 117.6708 - val_loss: 657.2872 - val_mae: 657.2872\n",
      "Epoch 57/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 129.7017 - mae: 129.7017\n",
      "Epoch 57: val_loss did not improve from 599.71088\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 126.1676 - mae: 126.1676 - val_loss: 618.6085 - val_mae: 618.6085\n",
      "Epoch 58/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 116.0432 - mae: 116.0432\n",
      "Epoch 58: val_loss improved from 599.71088 to 597.85895, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 113.6348 - mae: 113.6348 - val_loss: 597.8589 - val_mae: 597.8589\n",
      "Epoch 59/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 107.4682 - mae: 107.4682\n",
      "Epoch 59: val_loss did not improve from 597.85895\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 111.7779 - mae: 111.7779 - val_loss: 599.0266 - val_mae: 599.0266\n",
      "Epoch 60/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 115.0315 - mae: 115.0315\n",
      "Epoch 60: val_loss did not improve from 597.85895\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 112.2515 - mae: 112.2515 - val_loss: 599.1823 - val_mae: 599.1823\n",
      "Epoch 61/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 113.0229 - mae: 113.0229\n",
      "Epoch 61: val_loss did not improve from 597.85895\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 112.0408 - mae: 112.0408 - val_loss: 613.0112 - val_mae: 613.0112\n",
      "Epoch 62/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 113.4949 - mae: 113.4949\n",
      "Epoch 62: val_loss did not improve from 597.85895\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 114.4345 - mae: 114.4345 - val_loss: 607.5443 - val_mae: 607.5443\n",
      "Epoch 63/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 117.8985 - mae: 117.8985\n",
      "Epoch 63: val_loss did not improve from 597.85895\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 114.6645 - mae: 114.6645 - val_loss: 601.3671 - val_mae: 601.3671\n",
      "Epoch 64/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 115.1396 - mae: 115.1396\n",
      "Epoch 64: val_loss did not improve from 597.85895\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 111.5975 - mae: 111.5975 - val_loss: 628.2851 - val_mae: 628.2851\n",
      "Epoch 65/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 114.5740 - mae: 114.5740\n",
      "Epoch 65: val_loss did not improve from 597.85895\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 113.6747 - mae: 113.6747 - val_loss: 599.7472 - val_mae: 599.7472\n",
      "Epoch 66/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 112.4351 - mae: 112.4351\n",
      "Epoch 66: val_loss did not improve from 597.85895\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 112.0976 - mae: 112.0976 - val_loss: 603.4747 - val_mae: 603.4747\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 113.8843 - mae: 113.8843\n",
      "Epoch 67: val_loss improved from 597.85895 to 592.99524, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 113.8843 - mae: 113.8843 - val_loss: 592.9952 - val_mae: 592.9952\n",
      "Epoch 68/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 111.3327 - mae: 111.3327\n",
      "Epoch 68: val_loss did not improve from 592.99524\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 114.5138 - mae: 114.5138 - val_loss: 623.6289 - val_mae: 623.6289\n",
      "Epoch 69/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 123.5563 - mae: 123.5563\n",
      "Epoch 69: val_loss did not improve from 592.99524\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 118.6160 - mae: 118.6160 - val_loss: 619.4093 - val_mae: 619.4093\n",
      "Epoch 70/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 114.3656 - mae: 114.3656\n",
      "Epoch 70: val_loss did not improve from 592.99524\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 116.5529 - mae: 116.5529 - val_loss: 624.3079 - val_mae: 624.3079\n",
      "Epoch 71/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 108.6741 - mae: 108.6741\n",
      "Epoch 71: val_loss did not improve from 592.99524\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 111.5487 - mae: 111.5487 - val_loss: 596.6046 - val_mae: 596.6046\n",
      "Epoch 72/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 109.6943 - mae: 109.6943\n",
      "Epoch 72: val_loss improved from 592.99524 to 585.25684, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 111.0878 - mae: 111.0878 - val_loss: 585.2568 - val_mae: 585.2568\n",
      "Epoch 73/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 115.1013 - mae: 115.1013\n",
      "Epoch 73: val_loss improved from 585.25684 to 585.10663, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 111.8596 - mae: 111.8596 - val_loss: 585.1066 - val_mae: 585.1066\n",
      "Epoch 74/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 113.6506 - mae: 113.6506\n",
      "Epoch 74: val_loss did not improve from 585.10663\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 112.0734 - mae: 112.0734 - val_loss: 591.9080 - val_mae: 591.9080\n",
      "Epoch 75/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 109.2585 - mae: 109.2585\n",
      "Epoch 75: val_loss did not improve from 585.10663\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 111.4241 - mae: 111.4241 - val_loss: 647.8035 - val_mae: 647.8035\n",
      "Epoch 76/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 111.9362 - mae: 111.9362\n",
      "Epoch 76: val_loss did not improve from 585.10663\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 111.9288 - mae: 111.9288 - val_loss: 587.5558 - val_mae: 587.5558\n",
      "Epoch 77/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 115.0192 - mae: 115.0192\n",
      "Epoch 77: val_loss did not improve from 585.10663\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 112.4445 - mae: 112.4445 - val_loss: 638.6985 - val_mae: 638.6985\n",
      "Epoch 78/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 108.8716 - mae: 108.8716\n",
      "Epoch 78: val_loss did not improve from 585.10663\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 112.3566 - mae: 112.3566 - val_loss: 610.2241 - val_mae: 610.2241\n",
      "Epoch 79/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 108.7155 - mae: 108.7155\n",
      "Epoch 79: val_loss improved from 585.10663 to 584.32056, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 110.8991 - mae: 110.8991 - val_loss: 584.3206 - val_mae: 584.3206\n",
      "Epoch 80/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 113.5236 - mae: 113.5236\n",
      "Epoch 80: val_loss did not improve from 584.32056\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 113.8929 - mae: 113.8929 - val_loss: 653.5350 - val_mae: 653.5350\n",
      "Epoch 81/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 110.7580 - mae: 110.7580\n",
      "Epoch 81: val_loss did not improve from 584.32056\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 111.0340 - mae: 111.0340 - val_loss: 599.8354 - val_mae: 599.8354\n",
      "Epoch 82/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 119.5168 - mae: 119.5168\n",
      "Epoch 82: val_loss did not improve from 584.32056\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 112.1213 - mae: 112.1213 - val_loss: 587.4441 - val_mae: 587.4441\n",
      "Epoch 83/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 109.3283 - mae: 109.3283\n",
      "Epoch 83: val_loss did not improve from 584.32056\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 110.4315 - mae: 110.4315 - val_loss: 619.7518 - val_mae: 619.7518\n",
      "Epoch 84/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 108.1186 - mae: 108.1186\n",
      "Epoch 84: val_loss did not improve from 584.32056\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 112.3092 - mae: 112.3092 - val_loss: 589.4591 - val_mae: 589.4591\n",
      "Epoch 85/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 106.2267 - mae: 106.2267\n",
      "Epoch 85: val_loss did not improve from 584.32056\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.8473 - mae: 109.8473 - val_loss: 618.4038 - val_mae: 618.4038\n",
      "Epoch 86/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 105.5577 - mae: 105.5577\n",
      "Epoch 86: val_loss did not improve from 584.32056\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 111.6019 - mae: 111.6019 - val_loss: 644.7286 - val_mae: 644.7286\n",
      "Epoch 87/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 118.3877 - mae: 118.3877\n",
      "Epoch 87: val_loss improved from 584.32056 to 580.75171, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 113.0085 - mae: 113.0085 - val_loss: 580.7517 - val_mae: 580.7517\n",
      "Epoch 88/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 107.0424 - mae: 107.0424\n",
      "Epoch 88: val_loss improved from 580.75171 to 576.76526, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 108.7026 - mae: 108.7026 - val_loss: 576.7653 - val_mae: 576.7653\n",
      "Epoch 89/100\n",
      " 8/18 [============>.................] - ETA: 0s - loss: 106.0133 - mae: 106.0133\n",
      "Epoch 89: val_loss did not improve from 576.76526\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 109.7535 - mae: 109.7535 - val_loss: 585.0951 - val_mae: 585.0951\n",
      "Epoch 90/100\n",
      "12/18 [===================>..........] - ETA: 0s - loss: 107.9767 - mae: 107.9767\n",
      "Epoch 90: val_loss did not improve from 576.76526\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 111.6269 - mae: 111.6269 - val_loss: 728.4570 - val_mae: 728.4570\n",
      "Epoch 91/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 120.7279 - mae: 120.7279\n",
      "Epoch 91: val_loss improved from 576.76526 to 575.91895, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 114.3648 - mae: 114.3648 - val_loss: 575.9189 - val_mae: 575.9189\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 110.7842 - mae: 110.7842\n",
      "Epoch 92: val_loss did not improve from 575.91895\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 110.7842 - mae: 110.7842 - val_loss: 577.3823 - val_mae: 577.3823\n",
      "Epoch 93/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 110.3652 - mae: 110.3652\n",
      "Epoch 93: val_loss did not improve from 575.91895\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 110.1119 - mae: 110.1119 - val_loss: 609.2866 - val_mae: 609.2866\n",
      "Epoch 94/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 107.6729 - mae: 107.6729\n",
      "Epoch 94: val_loss did not improve from 575.91895\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 110.5253 - mae: 110.5253 - val_loss: 612.4694 - val_mae: 612.4694\n",
      "Epoch 95/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 108.8012 - mae: 108.8012\n",
      "Epoch 95: val_loss did not improve from 575.91895\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 108.9362 - mae: 108.9362 - val_loss: 584.0804 - val_mae: 584.0804\n",
      "Epoch 96/100\n",
      " 7/18 [==========>...................] - ETA: 0s - loss: 113.7882 - mae: 113.7882\n",
      "Epoch 96: val_loss did not improve from 575.91895\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 110.5403 - mae: 110.5403 - val_loss: 583.4609 - val_mae: 583.4609\n",
      "Epoch 97/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 108.1421 - mae: 108.1421\n",
      "Epoch 97: val_loss did not improve from 575.91895\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 107.8182 - mae: 107.8182 - val_loss: 589.6803 - val_mae: 589.6803\n",
      "Epoch 98/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 106.3239 - mae: 106.3239\n",
      "Epoch 98: val_loss did not improve from 575.91895\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 107.9457 - mae: 107.9457 - val_loss: 584.4767 - val_mae: 584.4767\n",
      "Epoch 99/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 106.0380 - mae: 106.0380\n",
      "Epoch 99: val_loss improved from 575.91895 to 573.07788, saving model to ./saves/10//model_5_lstm/model_5_lstm\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 108.1369 - mae: 108.1369 - val_loss: 573.0779 - val_mae: 573.0779\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 112.8992 - mae: 112.8992\n",
      "Epoch 100: val_loss did not improve from 573.07788\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 112.8992 - mae: 112.8992 - val_loss: 576.9951 - val_mae: 576.9951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1328a2400>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "inputs = layers.Input(shape=(WINDOW_SIZE))\n",
    "x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs)\n",
    "x = layers.LSTM(128, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(HORIZON)(x)\n",
    "model_5 = tf.keras.Model(inputs, outputs=outputs, name=\"model_5_lstm\")\n",
    "\n",
    "model_5.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mae\"])\n",
    "\n",
    "model_5.fit(train_windows,\n",
    "            train_labels,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_5.name)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make multivariate time series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_reward_2 = 25\n",
    "block_reward_3 = 12.5\n",
    "block_reward_4 = 6.25\n",
    "\n",
    "block_reward_2_datetime = np.datetime64(\"2012-11-28\")\n",
    "block_reward_3_datetime = np.datetime64(\"2016-07-09\")\n",
    "block_reward_4_datetime = np.datetime64(\"2020-05-18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_reward_2_days = (block_reward_3_datetime - bitcoin_prices.index[0]).days\n",
    "block_reward_3_days = (block_reward_4_datetime - bitcoin_prices.index[0]).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>block_reward</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-10-01</th>\n",
       "      <td>123.65499</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-02</th>\n",
       "      <td>125.45500</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-03</th>\n",
       "      <td>108.58483</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-04</th>\n",
       "      <td>118.67466</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-05</th>\n",
       "      <td>121.33866</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Price block_reward\n",
       "Date                              \n",
       "2013-10-01  123.65499           25\n",
       "2013-10-02  125.45500           25\n",
       "2013-10-03  108.58483           25\n",
       "2013-10-04  118.67466           25\n",
       "2013-10-05  121.33866           25"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add block_reward column\n",
    "bitcoin_prices_block = bitcoin_prices.copy()\n",
    "bitcoin_prices_block[\"block_reward\"] = None\n",
    "\n",
    "# Set values of block_reward column (it's the last column hence -1 indexing on iloc)\n",
    "bitcoin_prices_block.iloc[:block_reward_2_days, -1] = block_reward_2\n",
    "bitcoin_prices_block.iloc[block_reward_2_days:block_reward_3_days,\n",
    "                          -1] = block_reward_3\n",
    "bitcoin_prices_block.iloc[block_reward_3_days:, -1] = block_reward_4\n",
    "bitcoin_prices_block.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='Date'>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the block reward/price over time\n",
    "# Note: Because of the different scales of our values we'll scale them to be between 0 and 1.\n",
    "scaled_price_block_df = pd.DataFrame(\n",
    "    minmax_scale(bitcoin_prices_block[[\"Price\", \"block_reward\"\n",
    "                                       ]]),  # we need to scale the data first\n",
    "    columns=bitcoin_prices_block.columns,\n",
    "    index=bitcoin_prices_block.index)\n",
    "scaled_price_block_df.plot(figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>block_reward</th>\n",
       "      <th>Price+1</th>\n",
       "      <th>Price+2</th>\n",
       "      <th>Price+3</th>\n",
       "      <th>Price+4</th>\n",
       "      <th>Price+5</th>\n",
       "      <th>Price+6</th>\n",
       "      <th>Price+7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-10-01</th>\n",
       "      <td>123.65499</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-02</th>\n",
       "      <td>125.45500</td>\n",
       "      <td>25</td>\n",
       "      <td>123.65499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-03</th>\n",
       "      <td>108.58483</td>\n",
       "      <td>25</td>\n",
       "      <td>125.45500</td>\n",
       "      <td>123.65499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-04</th>\n",
       "      <td>118.67466</td>\n",
       "      <td>25</td>\n",
       "      <td>108.58483</td>\n",
       "      <td>125.45500</td>\n",
       "      <td>123.65499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-05</th>\n",
       "      <td>121.33866</td>\n",
       "      <td>25</td>\n",
       "      <td>118.67466</td>\n",
       "      <td>108.58483</td>\n",
       "      <td>125.45500</td>\n",
       "      <td>123.65499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-06</th>\n",
       "      <td>120.65533</td>\n",
       "      <td>25</td>\n",
       "      <td>121.33866</td>\n",
       "      <td>118.67466</td>\n",
       "      <td>108.58483</td>\n",
       "      <td>125.45500</td>\n",
       "      <td>123.65499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-07</th>\n",
       "      <td>121.79500</td>\n",
       "      <td>25</td>\n",
       "      <td>120.65533</td>\n",
       "      <td>121.33866</td>\n",
       "      <td>118.67466</td>\n",
       "      <td>108.58483</td>\n",
       "      <td>125.45500</td>\n",
       "      <td>123.65499</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-08</th>\n",
       "      <td>123.03300</td>\n",
       "      <td>25</td>\n",
       "      <td>121.79500</td>\n",
       "      <td>120.65533</td>\n",
       "      <td>121.33866</td>\n",
       "      <td>118.67466</td>\n",
       "      <td>108.58483</td>\n",
       "      <td>125.45500</td>\n",
       "      <td>123.65499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-09</th>\n",
       "      <td>124.04900</td>\n",
       "      <td>25</td>\n",
       "      <td>123.03300</td>\n",
       "      <td>121.79500</td>\n",
       "      <td>120.65533</td>\n",
       "      <td>121.33866</td>\n",
       "      <td>118.67466</td>\n",
       "      <td>108.58483</td>\n",
       "      <td>125.45500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-10</th>\n",
       "      <td>125.96116</td>\n",
       "      <td>25</td>\n",
       "      <td>124.04900</td>\n",
       "      <td>123.03300</td>\n",
       "      <td>121.79500</td>\n",
       "      <td>120.65533</td>\n",
       "      <td>121.33866</td>\n",
       "      <td>118.67466</td>\n",
       "      <td>108.58483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Price block_reward    Price+1    Price+2    Price+3  \\\n",
       "Date                                                                  \n",
       "2013-10-01  123.65499           25        NaN        NaN        NaN   \n",
       "2013-10-02  125.45500           25  123.65499        NaN        NaN   \n",
       "2013-10-03  108.58483           25  125.45500  123.65499        NaN   \n",
       "2013-10-04  118.67466           25  108.58483  125.45500  123.65499   \n",
       "2013-10-05  121.33866           25  118.67466  108.58483  125.45500   \n",
       "2013-10-06  120.65533           25  121.33866  118.67466  108.58483   \n",
       "2013-10-07  121.79500           25  120.65533  121.33866  118.67466   \n",
       "2013-10-08  123.03300           25  121.79500  120.65533  121.33866   \n",
       "2013-10-09  124.04900           25  123.03300  121.79500  120.65533   \n",
       "2013-10-10  125.96116           25  124.04900  123.03300  121.79500   \n",
       "\n",
       "              Price+4    Price+5    Price+6    Price+7  \n",
       "Date                                                    \n",
       "2013-10-01        NaN        NaN        NaN        NaN  \n",
       "2013-10-02        NaN        NaN        NaN        NaN  \n",
       "2013-10-03        NaN        NaN        NaN        NaN  \n",
       "2013-10-04        NaN        NaN        NaN        NaN  \n",
       "2013-10-05  123.65499        NaN        NaN        NaN  \n",
       "2013-10-06  125.45500  123.65499        NaN        NaN  \n",
       "2013-10-07  108.58483  125.45500  123.65499        NaN  \n",
       "2013-10-08  118.67466  108.58483  125.45500  123.65499  \n",
       "2013-10-09  121.33866  118.67466  108.58483  125.45500  \n",
       "2013-10-10  120.65533  121.33866  118.67466  108.58483  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a copy of the Bitcoin historical data with block reward feature\n",
    "bitcoin_prices_windowed = bitcoin_prices_block.copy()\n",
    "\n",
    "# Add windowed columns\n",
    "for i in range(WINDOW_SIZE):  # Shift values for each step in WINDOW_SIZE\n",
    "    bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\n",
    "        \"Price\"].shift(periods=i + 1)\n",
    "bitcoin_prices_windowed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block_reward</th>\n",
       "      <th>Price+1</th>\n",
       "      <th>Price+2</th>\n",
       "      <th>Price+3</th>\n",
       "      <th>Price+4</th>\n",
       "      <th>Price+5</th>\n",
       "      <th>Price+6</th>\n",
       "      <th>Price+7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-10-08</th>\n",
       "      <td>25.0</td>\n",
       "      <td>121.794998</td>\n",
       "      <td>120.655327</td>\n",
       "      <td>121.338661</td>\n",
       "      <td>118.674660</td>\n",
       "      <td>108.584831</td>\n",
       "      <td>125.455002</td>\n",
       "      <td>123.654991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-09</th>\n",
       "      <td>25.0</td>\n",
       "      <td>123.032997</td>\n",
       "      <td>121.794998</td>\n",
       "      <td>120.655327</td>\n",
       "      <td>121.338661</td>\n",
       "      <td>118.674660</td>\n",
       "      <td>108.584831</td>\n",
       "      <td>125.455002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-10</th>\n",
       "      <td>25.0</td>\n",
       "      <td>124.049004</td>\n",
       "      <td>123.032997</td>\n",
       "      <td>121.794998</td>\n",
       "      <td>120.655327</td>\n",
       "      <td>121.338661</td>\n",
       "      <td>118.674660</td>\n",
       "      <td>108.584831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-11</th>\n",
       "      <td>25.0</td>\n",
       "      <td>125.961159</td>\n",
       "      <td>124.049004</td>\n",
       "      <td>123.032997</td>\n",
       "      <td>121.794998</td>\n",
       "      <td>120.655327</td>\n",
       "      <td>121.338661</td>\n",
       "      <td>118.674660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-12</th>\n",
       "      <td>25.0</td>\n",
       "      <td>125.279663</td>\n",
       "      <td>125.961159</td>\n",
       "      <td>124.049004</td>\n",
       "      <td>123.032997</td>\n",
       "      <td>121.794998</td>\n",
       "      <td>120.655327</td>\n",
       "      <td>121.338661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            block_reward     Price+1     Price+2     Price+3     Price+4  \\\n",
       "Date                                                                       \n",
       "2013-10-08          25.0  121.794998  120.655327  121.338661  118.674660   \n",
       "2013-10-09          25.0  123.032997  121.794998  120.655327  121.338661   \n",
       "2013-10-10          25.0  124.049004  123.032997  121.794998  120.655327   \n",
       "2013-10-11          25.0  125.961159  124.049004  123.032997  121.794998   \n",
       "2013-10-12          25.0  125.279663  125.961159  124.049004  123.032997   \n",
       "\n",
       "               Price+5     Price+6     Price+7  \n",
       "Date                                            \n",
       "2013-10-08  108.584831  125.455002  123.654991  \n",
       "2013-10-09  118.674660  108.584831  125.455002  \n",
       "2013-10-10  121.338661  118.674660  108.584831  \n",
       "2013-10-11  120.655327  121.338661  118.674660  \n",
       "2013-10-12  121.794998  120.655327  121.338661  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create X & y, remove the NaN's and convert to float32 to prevent TensorFlow errors\n",
    "X = bitcoin_prices_windowed.dropna().drop(\"Price\", axis=1).astype(np.float32)\n",
    "y = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32)\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2224, 2224, 556, 556)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make train and test sets\n",
    "split_size = int(len(X) * 0.8)\n",
    "X_train, y_train = X[:split_size], y[:split_size]\n",
    "X_test, y_test = X[split_size:], y[split_size:]\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_6 Dense Multivariate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 3639.9133 - mae: 3639.9133 \n",
      "Epoch 1: val_loss improved from inf to 7555.65039, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 2s 47ms/step - loss: 2984.6279 - mae: 2984.6279 - val_loss: 7555.6504 - val_mae: 7555.6504\n",
      "Epoch 2/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 509.1732 - mae: 509.1732\n",
      "Epoch 2: val_loss improved from 7555.65039 to 1734.52747, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 496.6994 - mae: 496.6994 - val_loss: 1734.5275 - val_mae: 1734.5275\n",
      "Epoch 3/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 234.8860 - mae: 234.8860\n",
      "Epoch 3: val_loss improved from 1734.52747 to 1118.17493, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 19ms/step - loss: 231.6964 - mae: 231.6964 - val_loss: 1118.1749 - val_mae: 1118.1749\n",
      "Epoch 4/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 273.0829 - mae: 273.0829\n",
      "Epoch 4: val_loss improved from 1118.17493 to 1036.27771, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 201.4386 - mae: 201.4386 - val_loss: 1036.2777 - val_mae: 1036.2777\n",
      "Epoch 5/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 188.0992 - mae: 188.0992\n",
      "Epoch 5: val_loss improved from 1036.27771 to 1012.04755, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 184.8192 - mae: 184.8192 - val_loss: 1012.0475 - val_mae: 1012.0477\n",
      "Epoch 6/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 182.2839 - mae: 182.2839\n",
      "Epoch 6: val_loss did not improve from 1012.04755\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 183.9679 - mae: 183.9679 - val_loss: 1043.5420 - val_mae: 1043.5420\n",
      "Epoch 7/100\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 183.1486 - mae: 183.1486\n",
      "Epoch 7: val_loss improved from 1012.04755 to 958.92651, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 178.5758 - mae: 178.5758 - val_loss: 958.9265 - val_mae: 958.9265\n",
      "Epoch 8/100\n",
      " 6/18 [=========>....................] - ETA: 0s - loss: 173.5969 - mae: 173.5969\n",
      "Epoch 8: val_loss improved from 958.92651 to 954.75885, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 170.3201 - mae: 170.3201 - val_loss: 954.7589 - val_mae: 954.7589\n",
      "Epoch 9/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 166.5548 - mae: 166.5548\n",
      "Epoch 9: val_loss did not improve from 954.75885\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 166.5451 - mae: 166.5451 - val_loss: 961.4470 - val_mae: 961.4470\n",
      "Epoch 10/100\n",
      " 9/18 [==============>...............] - ETA: 0s - loss: 158.9235 - mae: 158.9235\n",
      "Epoch 10: val_loss improved from 954.75885 to 899.30798, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 162.7794 - mae: 162.7794 - val_loss: 899.3080 - val_mae: 899.3080\n",
      "Epoch 11/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 177.7204 - mae: 177.7204\n",
      "Epoch 11: val_loss improved from 899.30798 to 870.43213, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 158.1496 - mae: 158.1496 - val_loss: 870.4321 - val_mae: 870.4321\n",
      "Epoch 12/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 152.8441 - mae: 152.8441\n",
      "Epoch 12: val_loss did not improve from 870.43213\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 154.9435 - mae: 154.9435 - val_loss: 964.6779 - val_mae: 964.6779\n",
      "Epoch 13/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 125.1103 - mae: 125.1103\n",
      "Epoch 13: val_loss improved from 870.43213 to 839.72284, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 153.2296 - mae: 153.2296 - val_loss: 839.7228 - val_mae: 839.7228\n",
      "Epoch 14/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 162.8354 - mae: 162.8354\n",
      "Epoch 14: val_loss did not improve from 839.72284\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 147.0731 - mae: 147.0731 - val_loss: 841.8123 - val_mae: 841.8123\n",
      "Epoch 15/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 142.8521 - mae: 142.8521\n",
      "Epoch 15: val_loss did not improve from 839.72284\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 144.8025 - mae: 144.8025 - val_loss: 848.7396 - val_mae: 848.7396\n",
      "Epoch 16/100\n",
      " 6/18 [=========>....................] - ETA: 0s - loss: 142.8880 - mae: 142.8880\n",
      "Epoch 16: val_loss improved from 839.72284 to 788.43018, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 141.9737 - mae: 141.9737 - val_loss: 788.4302 - val_mae: 788.4302\n",
      "Epoch 17/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 138.1999 - mae: 138.1999\n",
      "Epoch 17: val_loss improved from 788.43018 to 767.56427, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 138.3642 - mae: 138.3642 - val_loss: 767.5643 - val_mae: 767.5643\n",
      "Epoch 18/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 141.3380 - mae: 141.3380\n",
      "Epoch 18: val_loss improved from 767.56427 to 751.05249, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 135.7696 - mae: 135.7696 - val_loss: 751.0525 - val_mae: 751.0525\n",
      "Epoch 19/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 78.6244 - mae: 78.6244\n",
      "Epoch 19: val_loss improved from 751.05249 to 730.78149, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 131.8482 - mae: 131.8482 - val_loss: 730.7815 - val_mae: 730.7815\n",
      "Epoch 20/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 135.9948 - mae: 135.9948\n",
      "Epoch 20: val_loss improved from 730.78149 to 730.72089, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 132.3862 - mae: 132.3862 - val_loss: 730.7209 - val_mae: 730.7209\n",
      "Epoch 21/100\n",
      "11/18 [=================>............] - ETA: 0s - loss: 140.5184 - mae: 140.5184\n",
      "Epoch 21: val_loss improved from 730.72089 to 709.93988, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 133.7035 - mae: 133.7035 - val_loss: 709.9399 - val_mae: 709.9399\n",
      "Epoch 22/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 134.3879 - mae: 134.3879\n",
      "Epoch 22: val_loss improved from 709.93988 to 684.39679, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 129.0543 - mae: 129.0543 - val_loss: 684.3968 - val_mae: 684.3968\n",
      "Epoch 23/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 122.5433 - mae: 122.5433\n",
      "Epoch 23: val_loss did not improve from 684.39679\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 124.2773 - mae: 124.2773 - val_loss: 685.2132 - val_mae: 685.2132\n",
      "Epoch 24/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 136.6978 - mae: 136.6978\n",
      "Epoch 24: val_loss improved from 684.39679 to 665.06342, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 123.5391 - mae: 123.5391 - val_loss: 665.0634 - val_mae: 665.0634\n",
      "Epoch 25/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 128.1007 - mae: 128.1007\n",
      "Epoch 25: val_loss improved from 665.06342 to 658.52844, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 121.7094 - mae: 121.7094 - val_loss: 658.5284 - val_mae: 658.5284\n",
      "Epoch 26/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 118.7280 - mae: 118.7280\n",
      "Epoch 26: val_loss improved from 658.52844 to 651.13727, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 122.5089 - mae: 122.5089 - val_loss: 651.1373 - val_mae: 651.1373\n",
      "Epoch 27/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 122.1193 - mae: 122.1193\n",
      "Epoch 27: val_loss improved from 651.13727 to 634.06635, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 120.8525 - mae: 120.8525 - val_loss: 634.0663 - val_mae: 634.0663\n",
      "Epoch 28/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 109.7254 - mae: 109.7254\n",
      "Epoch 28: val_loss improved from 634.06635 to 622.11841, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 116.7464 - mae: 116.7464 - val_loss: 622.1184 - val_mae: 622.1184\n",
      "Epoch 29/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 114.0602 - mae: 114.0602\n",
      "Epoch 29: val_loss did not improve from 622.11841\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 116.4472 - mae: 116.4472 - val_loss: 624.9618 - val_mae: 624.9618\n",
      "Epoch 30/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 133.2208 - mae: 133.2208\n",
      "Epoch 30: val_loss improved from 622.11841 to 613.93164, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 118.0039 - mae: 118.0039 - val_loss: 613.9316 - val_mae: 613.9316\n",
      "Epoch 31/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 123.3362 - mae: 123.3362\n",
      "Epoch 31: val_loss did not improve from 613.93164\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 114.3833 - mae: 114.3833 - val_loss: 621.6876 - val_mae: 621.6876\n",
      "Epoch 32/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 110.7782 - mae: 110.7782\n",
      "Epoch 32: val_loss improved from 613.93164 to 605.71814, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 113.7587 - mae: 113.7587 - val_loss: 605.7181 - val_mae: 605.7181\n",
      "Epoch 33/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 144.3970 - mae: 144.3970\n",
      "Epoch 33: val_loss did not improve from 605.71814\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 114.3641 - mae: 114.3641 - val_loss: 649.4486 - val_mae: 649.4486\n",
      "Epoch 34/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 140.3535 - mae: 140.3535\n",
      "Epoch 34: val_loss did not improve from 605.71814\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 117.7349 - mae: 117.7349 - val_loss: 614.1536 - val_mae: 614.1536\n",
      "Epoch 35/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 97.2569 - mae: 97.2569\n",
      "Epoch 35: val_loss did not improve from 605.71814\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.3012 - mae: 113.3012 - val_loss: 605.7338 - val_mae: 605.7338\n",
      "Epoch 36/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 160.3133 - mae: 160.3133\n",
      "Epoch 36: val_loss improved from 605.71814 to 604.14514, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 117.3088 - mae: 117.3088 - val_loss: 604.1451 - val_mae: 604.1451\n",
      "Epoch 37/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 136.0036 - mae: 136.0036\n",
      "Epoch 37: val_loss did not improve from 604.14514\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 114.4625 - mae: 114.4625 - val_loss: 614.0172 - val_mae: 614.0172\n",
      "Epoch 38/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 106.3153 - mae: 106.3153\n",
      "Epoch 38: val_loss improved from 604.14514 to 597.34332, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 112.4603 - mae: 112.4603 - val_loss: 597.3433 - val_mae: 597.3433\n",
      "Epoch 39/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 131.7793 - mae: 131.7793\n",
      "Epoch 39: val_loss improved from 597.34332 to 588.96069, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 111.9422 - mae: 111.9422 - val_loss: 588.9607 - val_mae: 588.9607\n",
      "Epoch 40/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 104.1687 - mae: 104.1687\n",
      "Epoch 40: val_loss improved from 588.96069 to 586.89453, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 110.7299 - mae: 110.7299 - val_loss: 586.8945 - val_mae: 586.8945\n",
      "Epoch 41/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 101.5477 - mae: 101.5477\n",
      "Epoch 41: val_loss did not improve from 586.89453\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.0091 - mae: 113.0091 - val_loss: 700.5454 - val_mae: 700.5454\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 120.1191 - mae: 120.1191\n",
      "Epoch 42: val_loss did not improve from 586.89453\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 120.1191 - mae: 120.1191 - val_loss: 587.1578 - val_mae: 587.1578\n",
      "Epoch 43/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 90.9376 - mae: 90.9376\n",
      "Epoch 43: val_loss did not improve from 586.89453\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.6549 - mae: 110.6549 - val_loss: 596.8481 - val_mae: 596.8481\n",
      "Epoch 44/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 65.6462 - mae: 65.6462\n",
      "Epoch 44: val_loss did not improve from 586.89453\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.8062 - mae: 110.8062 - val_loss: 590.2009 - val_mae: 590.2009\n",
      "Epoch 45/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 84.0019 - mae: 84.0019\n",
      "Epoch 45: val_loss did not improve from 586.89453\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 109.8608 - mae: 109.8608 - val_loss: 592.4762 - val_mae: 592.4762\n",
      "Epoch 46/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 86.5520 - mae: 86.5520\n",
      "Epoch 46: val_loss did not improve from 586.89453\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.5050 - mae: 110.5050 - val_loss: 596.8453 - val_mae: 596.8453\n",
      "Epoch 47/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 101.8952 - mae: 101.8952\n",
      "Epoch 47: val_loss did not improve from 586.89453\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 111.0152 - mae: 111.0152 - val_loss: 608.3344 - val_mae: 608.3344\n",
      "Epoch 48/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 102.0672 - mae: 102.0672\n",
      "Epoch 48: val_loss did not improve from 586.89453\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9242 - mae: 109.9242 - val_loss: 629.1143 - val_mae: 629.1143\n",
      "Epoch 49/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 168.0371 - mae: 168.0371\n",
      "Epoch 49: val_loss improved from 586.89453 to 580.29254, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 113.8644 - mae: 113.8644 - val_loss: 580.2925 - val_mae: 580.2925\n",
      "Epoch 50/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 109.8301 - mae: 109.8301\n",
      "Epoch 50: val_loss did not improve from 580.29254\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 112.1458 - mae: 112.1458 - val_loss: 628.0585 - val_mae: 628.0585\n",
      "Epoch 51/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 94.3104 - mae: 94.3104\n",
      "Epoch 51: val_loss did not improve from 580.29254\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.3650 - mae: 111.3650 - val_loss: 603.0842 - val_mae: 603.0842\n",
      "Epoch 52/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 120.4341 - mae: 120.4341\n",
      "Epoch 52: val_loss did not improve from 580.29254\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.2422 - mae: 113.2422 - val_loss: 623.8638 - val_mae: 623.8638\n",
      "Epoch 53/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 63.5962 - mae: 63.5962\n",
      "Epoch 53: val_loss did not improve from 580.29254\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 115.0536 - mae: 115.0536 - val_loss: 585.7347 - val_mae: 585.7347\n",
      "Epoch 54/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 99.3270 - mae: 99.3270\n",
      "Epoch 54: val_loss improved from 580.29254 to 572.30475, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 111.3179 - mae: 111.3179 - val_loss: 572.3047 - val_mae: 572.3047\n",
      "Epoch 55/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 127.8577 - mae: 127.8577\n",
      "Epoch 55: val_loss did not improve from 572.30475\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.0572 - mae: 113.0572 - val_loss: 579.8574 - val_mae: 579.8574\n",
      "Epoch 56/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 108.3512 - mae: 108.3512\n",
      "Epoch 56: val_loss did not improve from 572.30475\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.7298 - mae: 111.7298 - val_loss: 849.0670 - val_mae: 849.0670\n",
      "Epoch 57/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 123.2276 - mae: 123.2276\n",
      "Epoch 57: val_loss did not improve from 572.30475\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 138.2020 - mae: 138.2020 - val_loss: 674.4948 - val_mae: 674.4948\n",
      "Epoch 58/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 96.8931 - mae: 96.8931\n",
      "Epoch 58: val_loss did not improve from 572.30475\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 120.3098 - mae: 120.3098 - val_loss: 667.7099 - val_mae: 667.7099\n",
      "Epoch 59/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 81.2355 - mae: 81.2355\n",
      "Epoch 59: val_loss improved from 572.30475 to 571.74573, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.5195 - mae: 109.5195 - val_loss: 571.7457 - val_mae: 571.7457\n",
      "Epoch 60/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 91.7074 - mae: 91.7074\n",
      "Epoch 60: val_loss did not improve from 571.74573\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 108.3940 - mae: 108.3940 - val_loss: 587.1152 - val_mae: 587.1152\n",
      "Epoch 61/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 109.3696 - mae: 109.3696\n",
      "Epoch 61: val_loss did not improve from 571.74573\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 108.2091 - mae: 108.2091 - val_loss: 574.5104 - val_mae: 574.5104\n",
      "Epoch 62/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 73.7984 - mae: 73.7984\n",
      "Epoch 62: val_loss did not improve from 571.74573\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 108.4418 - mae: 108.4418 - val_loss: 577.2139 - val_mae: 577.2139\n",
      "Epoch 63/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 90.5972 - mae: 90.5972\n",
      "Epoch 63: val_loss did not improve from 571.74573\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 108.7375 - mae: 108.7375 - val_loss: 583.7997 - val_mae: 583.7997\n",
      "Epoch 64/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 89.4624 - mae: 89.4624\n",
      "Epoch 64: val_loss did not improve from 571.74573\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 108.2681 - mae: 108.2681 - val_loss: 590.5693 - val_mae: 590.5693\n",
      "Epoch 65/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 129.5800 - mae: 129.5800\n",
      "Epoch 65: val_loss improved from 571.74573 to 568.69684, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 110.3559 - mae: 110.3559 - val_loss: 568.6968 - val_mae: 568.6968\n",
      "Epoch 66/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 116.1437 - mae: 116.1437\n",
      "Epoch 66: val_loss did not improve from 568.69684\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8268 - mae: 109.8268 - val_loss: 575.0633 - val_mae: 575.0633\n",
      "Epoch 67/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 89.5627 - mae: 89.5627\n",
      "Epoch 67: val_loss did not improve from 568.69684\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 108.4015 - mae: 108.4015 - val_loss: 633.2881 - val_mae: 633.2881\n",
      "Epoch 68/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 69.8196 - mae: 69.8196\n",
      "Epoch 68: val_loss did not improve from 568.69684\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 114.6708 - mae: 114.6708 - val_loss: 692.1640 - val_mae: 692.1640\n",
      "Epoch 69/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 106.0564 - mae: 106.0564\n",
      "Epoch 69: val_loss did not improve from 568.69684\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 122.6599 - mae: 122.6599 - val_loss: 610.4360 - val_mae: 610.4360\n",
      "Epoch 70/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 115.5980 - mae: 115.5980\n",
      "Epoch 70: val_loss did not improve from 568.69684\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 127.3708 - mae: 127.3708 - val_loss: 596.0018 - val_mae: 596.0018\n",
      "Epoch 71/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 89.7417 - mae: 89.7417\n",
      "Epoch 71: val_loss did not improve from 568.69684\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.8792 - mae: 110.8792 - val_loss: 592.1594 - val_mae: 592.1594\n",
      "Epoch 72/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 102.1976 - mae: 102.1976\n",
      "Epoch 72: val_loss improved from 568.69684 to 567.66809, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.3028 - mae: 109.3028 - val_loss: 567.6681 - val_mae: 567.6681\n",
      "Epoch 73/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 121.2244 - mae: 121.2244\n",
      "Epoch 73: val_loss did not improve from 567.66809\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.6874 - mae: 109.6874 - val_loss: 568.6454 - val_mae: 568.6454\n",
      "Epoch 74/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 159.9374 - mae: 159.9374\n",
      "Epoch 74: val_loss improved from 567.66809 to 567.20807, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 108.8021 - mae: 108.8021 - val_loss: 567.2081 - val_mae: 567.2081\n",
      "Epoch 75/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 126.1887 - mae: 126.1887\n",
      "Epoch 75: val_loss did not improve from 567.20807\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 108.3613 - mae: 108.3613 - val_loss: 598.9367 - val_mae: 598.9367\n",
      "Epoch 76/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 79.4655 - mae: 79.4655\n",
      "Epoch 76: val_loss did not improve from 567.20807\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.8492 - mae: 110.8492 - val_loss: 588.2048 - val_mae: 588.2048\n",
      "Epoch 77/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 85.3652 - mae: 85.3652\n",
      "Epoch 77: val_loss did not improve from 567.20807\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.4213 - mae: 109.4213 - val_loss: 572.5679 - val_mae: 572.5679\n",
      "Epoch 78/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 106.2156 - mae: 106.2156\n",
      "Epoch 78: val_loss did not improve from 567.20807\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 108.6840 - mae: 108.6840 - val_loss: 567.2518 - val_mae: 567.2518\n",
      "Epoch 79/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 104.3164 - mae: 104.3164\n",
      "Epoch 79: val_loss did not improve from 567.20807\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 108.1142 - mae: 108.1142 - val_loss: 610.8257 - val_mae: 610.8257\n",
      "Epoch 80/100\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 114.3192 - mae: 114.3192\n",
      "Epoch 80: val_loss did not improve from 567.20807\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 108.8245 - mae: 108.8245 - val_loss: 576.9479 - val_mae: 576.9479\n",
      "Epoch 81/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 96.4908 - mae: 96.4908\n",
      "Epoch 81: val_loss did not improve from 567.20807\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 106.2261 - mae: 106.2261 - val_loss: 567.9827 - val_mae: 567.9827\n",
      "Epoch 82/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 190.7938 - mae: 190.7938\n",
      "Epoch 82: val_loss improved from 567.20807 to 563.74933, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 113.9352 - mae: 113.9352 - val_loss: 563.7493 - val_mae: 563.7493\n",
      "Epoch 83/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 106.7932 - mae: 106.7932\n",
      "Epoch 83: val_loss did not improve from 563.74933\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.1858 - mae: 112.1858 - val_loss: 638.3842 - val_mae: 638.3842\n",
      "Epoch 84/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 116.4161 - mae: 116.4161\n",
      "Epoch 84: val_loss did not improve from 563.74933\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.5028 - mae: 112.5028 - val_loss: 571.9734 - val_mae: 571.9734\n",
      "Epoch 85/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 87.0243 - mae: 87.0243\n",
      "Epoch 85: val_loss did not improve from 563.74933\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 107.0039 - mae: 107.0039 - val_loss: 596.7181 - val_mae: 596.7181\n",
      "Epoch 86/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 111.7446 - mae: 111.7446\n",
      "Epoch 86: val_loss did not improve from 563.74933\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 108.9309 - mae: 108.9309 - val_loss: 598.9283 - val_mae: 598.9283\n",
      "Epoch 87/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 123.4251 - mae: 123.4251\n",
      "Epoch 87: val_loss did not improve from 563.74933\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 106.5058 - mae: 106.5058 - val_loss: 583.8217 - val_mae: 583.8217\n",
      "Epoch 88/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 133.4811 - mae: 133.4811\n",
      "Epoch 88: val_loss did not improve from 563.74933\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 107.3328 - mae: 107.3328 - val_loss: 572.2121 - val_mae: 572.2121\n",
      "Epoch 89/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 100.4934 - mae: 100.4934\n",
      "Epoch 89: val_loss did not improve from 563.74933\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 107.6039 - mae: 107.6039 - val_loss: 566.9580 - val_mae: 566.9580\n",
      "Epoch 90/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 121.8069 - mae: 121.8069\n",
      "Epoch 90: val_loss did not improve from 563.74933\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 106.4755 - mae: 106.4755 - val_loss: 639.0736 - val_mae: 639.0736\n",
      "Epoch 91/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 99.4115 - mae: 99.4115\n",
      "Epoch 91: val_loss did not improve from 563.74933\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.2636 - mae: 113.2636 - val_loss: 638.6801 - val_mae: 638.6801\n",
      "Epoch 92/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 98.4511 - mae: 98.4511\n",
      "Epoch 92: val_loss did not improve from 563.74933\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 108.7271 - mae: 108.7271 - val_loss: 576.3959 - val_mae: 576.3959\n",
      "Epoch 93/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 110.4380 - mae: 110.4380\n",
      "Epoch 93: val_loss did not improve from 563.74933\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.8951 - mae: 113.8951 - val_loss: 611.6424 - val_mae: 611.6424\n",
      "Epoch 94/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 142.9921 - mae: 142.9921\n",
      "Epoch 94: val_loss did not improve from 563.74933\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9383 - mae: 109.9383 - val_loss: 591.8934 - val_mae: 591.8934\n",
      "Epoch 95/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 139.4608 - mae: 139.4608\n",
      "Epoch 95: val_loss improved from 563.74933 to 563.33252, saving model to ./saves/10//model_6_dense_multivar/model_6_dense_multivar\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 106.3982 - mae: 106.3982 - val_loss: 563.3325 - val_mae: 563.3325\n",
      "Epoch 96/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 110.5417 - mae: 110.5417\n",
      "Epoch 96: val_loss did not improve from 563.33252\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.7469 - mae: 110.7469 - val_loss: 664.2544 - val_mae: 664.2544\n",
      "Epoch 97/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 111.5259 - mae: 111.5259\n",
      "Epoch 97: val_loss did not improve from 563.33252\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.3715 - mae: 111.3715 - val_loss: 583.1803 - val_mae: 583.1803\n",
      "Epoch 98/100\n",
      "14/18 [======================>.......] - ETA: 0s - loss: 111.2732 - mae: 111.2732\n",
      "Epoch 98: val_loss did not improve from 563.33252\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1286 - mae: 109.1286 - val_loss: 565.5475 - val_mae: 565.5475\n",
      "Epoch 99/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 150.6285 - mae: 150.6285\n",
      "Epoch 99: val_loss did not improve from 563.33252\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 108.1663 - mae: 108.1663 - val_loss: 563.8272 - val_mae: 563.8272\n",
      "Epoch 100/100\n",
      " 1/18 [>.............................] - ETA: 0s - loss: 114.9594 - mae: 114.9594\n",
      "Epoch 100: val_loss did not improve from 563.33252\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9848 - mae: 109.9848 - val_loss: 604.5006 - val_mae: 604.5006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132efda90>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_6 = tf.keras.Sequential([\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(HORIZON, activation=\"linear\"),\n",
    "],\n",
    "                              name=\"model_6_dense_multivar\")\n",
    "\n",
    "model_6.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mae\"])\n",
    "\n",
    "model_6.fit(X_train,\n",
    "            y_train,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=[create_model_checkpoint(model_6.name)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model_7 N-BEATS algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NBeatsBlock custom layer\n",
    "class NBeatsBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(\n",
    "        self,  # the constructor takes all the hyperparameters for the layer\n",
    "        input_size: int,\n",
    "        theta_size: int,\n",
    "        horizon: int,\n",
    "        n_neurons: int,\n",
    "        n_layers: int,\n",
    "        **kwargs\n",
    "    ):  # the **kwargs argument takes care of all of the arguments for the parent class (input_shape, trainable, name)\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_size = input_size\n",
    "        self.theta_size = theta_size\n",
    "        self.horizon = horizon\n",
    "        self.n_neurons = n_neurons\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Block contains stack of 4 fully connected layers each has ReLU activation\n",
    "        self.hidden = [\n",
    "            tf.keras.layers.Dense(n_neurons, activation=\"relu\")\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        # Output of block is a theta layer with linear activation\n",
    "        self.theta_layer = tf.keras.layers.Dense(theta_size,\n",
    "                                                 activation=\"linear\",\n",
    "                                                 name=\"theta\")\n",
    "\n",
    "    def call(self,\n",
    "             inputs):  # the call method is what runs when the layer is called\n",
    "        x = inputs\n",
    "        for layer in self.hidden:  # pass inputs through each hidden layer\n",
    "            x = layer(x)\n",
    "        theta = self.theta_layer(x)\n",
    "        # Output the backcast and forecast from theta\n",
    "        backcast, forecast = theta[:, :self.input_size], theta[:,\n",
    "                                                               -self.horizon:]\n",
    "        return backcast, forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_nbeats_block_layer = NBeatsBlock(\n",
    "    input_size=WINDOW_SIZE,\n",
    "    theta_size=WINDOW_SIZE + HORIZON,\n",
    "    horizon=HORIZON,\n",
    "    n_neurons=128,\n",
    "    n_layers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 2, 3, 4, 5, 6, 7]], dtype=int32)>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating some dummy inputs\n",
    "dummy_inputs = tf.expand_dims(tf.range(WINDOW_SIZE) + 1, axis=0)\n",
    "dummy_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7), dtype=float32, numpy=\n",
       "array([[-0.23995999,  1.0135219 , -0.23360425,  0.15807968, -0.06819643,\n",
       "         0.9365648 , -0.03097062]], dtype=float32)>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backcast, forecast = dummy_nbeats_block_layer(dummy_inputs)\n",
    "backcast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for N-Beats Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Price+1</th>\n",
       "      <th>Price+2</th>\n",
       "      <th>Price+3</th>\n",
       "      <th>Price+4</th>\n",
       "      <th>Price+5</th>\n",
       "      <th>Price+6</th>\n",
       "      <th>Price+7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-10-01</th>\n",
       "      <td>123.65499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-02</th>\n",
       "      <td>125.45500</td>\n",
       "      <td>123.65499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-03</th>\n",
       "      <td>108.58483</td>\n",
       "      <td>125.45500</td>\n",
       "      <td>123.65499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-04</th>\n",
       "      <td>118.67466</td>\n",
       "      <td>108.58483</td>\n",
       "      <td>125.45500</td>\n",
       "      <td>123.65499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-05</th>\n",
       "      <td>121.33866</td>\n",
       "      <td>118.67466</td>\n",
       "      <td>108.58483</td>\n",
       "      <td>125.45500</td>\n",
       "      <td>123.65499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Price    Price+1    Price+2    Price+3    Price+4  Price+5  \\\n",
       "Date                                                                         \n",
       "2013-10-01  123.65499        NaN        NaN        NaN        NaN      NaN   \n",
       "2013-10-02  125.45500  123.65499        NaN        NaN        NaN      NaN   \n",
       "2013-10-03  108.58483  125.45500  123.65499        NaN        NaN      NaN   \n",
       "2013-10-04  118.67466  108.58483  125.45500  123.65499        NaN      NaN   \n",
       "2013-10-05  121.33866  118.67466  108.58483  125.45500  123.65499      NaN   \n",
       "\n",
       "            Price+6  Price+7  \n",
       "Date                          \n",
       "2013-10-01      NaN      NaN  \n",
       "2013-10-02      NaN      NaN  \n",
       "2013-10-03      NaN      NaN  \n",
       "2013-10-04      NaN      NaN  \n",
       "2013-10-05      NaN      NaN  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HORIZON = 1\n",
    "WINDOW_SIZE = 7\n",
    "\n",
    "bitcoin_prices_nbeats = bitcoin_prices.copy()\n",
    "for i in range(WINDOW_SIZE):\n",
    "    bitcoin_prices_nbeats[f\"Price+{i+1}\"] = bitcoin_prices_nbeats.Price.shift(\n",
    "        periods=i + 1)\n",
    "\n",
    "bitcoin_prices_nbeats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2224, 556)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make features and labels\n",
    "X = bitcoin_prices_nbeats.dropna().drop(\"Price\", axis=1)\n",
    "y = bitcoin_prices_nbeats.dropna()[\"Price\"]\n",
    "\n",
    "# make train and test splits\n",
    "split_size = int(len(X) * 0.8)\n",
    "X_train, y_train = X[:split_size], y[:split_size]\n",
    "X_test, y_test = X[split_size:], y[split_size:]\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset element_spec=(TensorSpec(shape=(None, 7), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>,\n",
       " <PrefetchDataset element_spec=(TensorSpec(shape=(None, 7), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we havnt made our datasets performate so far cuz datasets have been fairly small but now we should by using tf.data api\n",
    "train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "train_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "\n",
    "test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)\n",
    "test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip(\n",
    "    (train_features_dataset, train_labels_dataset))\n",
    "test_dataset = tf.data.Dataset.zip(\n",
    "    (test_features_dataset, test_labels_dataset))\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5000\n",
    "N_NEURONS = 512\n",
    "N_LAYERS = 4\n",
    "N_STACKS = 30\n",
    "\n",
    "INPUT_SIZE = WINDOW_SIZE * HORIZON\n",
    "THETA_SIZE = INPUT_SIZE * HORIZON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "3/3 [==============================] - 29s 2s/step - loss: 12119.3447 - val_loss: 15044.7129 - lr: 0.0010\n",
      "Epoch 2/5000\n",
      "3/3 [==============================] - 3s 996ms/step - loss: 1849.2184 - val_loss: 18843.1504 - lr: 0.0010\n",
      "Epoch 3/5000\n",
      "3/3 [==============================] - 3s 950ms/step - loss: 2816.4304 - val_loss: 11720.5635 - lr: 0.0010\n",
      "Epoch 4/5000\n",
      "3/3 [==============================] - 3s 991ms/step - loss: 2032.3759 - val_loss: 2831.1826 - lr: 0.0010\n",
      "Epoch 5/5000\n",
      "3/3 [==============================] - 3s 987ms/step - loss: 288.1413 - val_loss: 1632.8208 - lr: 0.0010\n",
      "Epoch 6/5000\n",
      "3/3 [==============================] - 3s 986ms/step - loss: 621.7042 - val_loss: 1405.4771 - lr: 0.0010\n",
      "Epoch 7/5000\n",
      "3/3 [==============================] - 3s 956ms/step - loss: 295.6120 - val_loss: 2088.9395 - lr: 0.0010\n",
      "Epoch 8/5000\n",
      "3/3 [==============================] - 3s 946ms/step - loss: 576.9362 - val_loss: 3496.1917 - lr: 0.0010\n",
      "Epoch 9/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1486.5590 - val_loss: 1899.5024 - lr: 0.0010\n",
      "Epoch 10/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 341.8595 - val_loss: 3947.4043 - lr: 0.0010\n",
      "Epoch 11/5000\n",
      "3/3 [==============================] - 3s 935ms/step - loss: 1451.0217 - val_loss: 1794.9103 - lr: 0.0010\n",
      "Epoch 12/5000\n",
      "3/3 [==============================] - 3s 943ms/step - loss: 571.6560 - val_loss: 2750.0310 - lr: 0.0010\n",
      "Epoch 13/5000\n",
      "3/3 [==============================] - 3s 999ms/step - loss: 260.3802 - val_loss: 1188.2217 - lr: 0.0010\n",
      "Epoch 14/5000\n",
      "3/3 [==============================] - 3s 932ms/step - loss: 224.0705 - val_loss: 1839.8336 - lr: 0.0010\n",
      "Epoch 15/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 459.1935 - val_loss: 973.1207 - lr: 0.0010\n",
      "Epoch 16/5000\n",
      "3/3 [==============================] - 3s 933ms/step - loss: 247.0708 - val_loss: 7237.8755 - lr: 0.0010\n",
      "Epoch 17/5000\n",
      "3/3 [==============================] - 3s 954ms/step - loss: 2120.4731 - val_loss: 1260.2612 - lr: 0.0010\n",
      "Epoch 18/5000\n",
      "3/3 [==============================] - 3s 950ms/step - loss: 303.7949 - val_loss: 1849.7644 - lr: 0.0010\n",
      "Epoch 19/5000\n",
      "3/3 [==============================] - 3s 989ms/step - loss: 700.0635 - val_loss: 969.0016 - lr: 0.0010\n",
      "Epoch 20/5000\n",
      "3/3 [==============================] - 3s 989ms/step - loss: 237.7227 - val_loss: 911.7267 - lr: 0.0010\n",
      "Epoch 21/5000\n",
      "3/3 [==============================] - 3s 975ms/step - loss: 226.6047 - val_loss: 895.6873 - lr: 0.0010\n",
      "Epoch 22/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 260.1519 - val_loss: 945.1318 - lr: 0.0010\n",
      "Epoch 23/5000\n",
      "3/3 [==============================] - 3s 957ms/step - loss: 265.9439 - val_loss: 1035.5342 - lr: 0.0010\n",
      "Epoch 24/5000\n",
      "3/3 [==============================] - 3s 944ms/step - loss: 225.1149 - val_loss: 954.5883 - lr: 0.0010\n",
      "Epoch 25/5000\n",
      "3/3 [==============================] - 3s 971ms/step - loss: 245.9645 - val_loss: 961.1198 - lr: 0.0010\n",
      "Epoch 26/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 215.5110 - val_loss: 886.7980 - lr: 0.0010\n",
      "Epoch 27/5000\n",
      "3/3 [==============================] - 3s 939ms/step - loss: 184.2671 - val_loss: 894.2502 - lr: 0.0010\n",
      "Epoch 28/5000\n",
      "3/3 [==============================] - 3s 988ms/step - loss: 187.5924 - val_loss: 861.2111 - lr: 0.0010\n",
      "Epoch 29/5000\n",
      "3/3 [==============================] - 3s 980ms/step - loss: 207.5216 - val_loss: 839.4738 - lr: 0.0010\n",
      "Epoch 30/5000\n",
      "3/3 [==============================] - 3s 967ms/step - loss: 224.8745 - val_loss: 818.9457 - lr: 0.0010\n",
      "Epoch 31/5000\n",
      "3/3 [==============================] - 3s 944ms/step - loss: 224.6284 - val_loss: 1015.9791 - lr: 0.0010\n",
      "Epoch 32/5000\n",
      "3/3 [==============================] - 3s 989ms/step - loss: 177.0232 - val_loss: 817.2429 - lr: 0.0010\n",
      "Epoch 33/5000\n",
      "3/3 [==============================] - 3s 923ms/step - loss: 167.6629 - val_loss: 817.6456 - lr: 0.0010\n",
      "Epoch 34/5000\n",
      "3/3 [==============================] - 3s 943ms/step - loss: 178.7912 - val_loss: 868.0155 - lr: 0.0010\n",
      "Epoch 35/5000\n",
      "3/3 [==============================] - 3s 939ms/step - loss: 224.7527 - val_loss: 833.1672 - lr: 0.0010\n",
      "Epoch 36/5000\n",
      "3/3 [==============================] - 3s 984ms/step - loss: 213.3183 - val_loss: 776.6922 - lr: 0.0010\n",
      "Epoch 37/5000\n",
      "3/3 [==============================] - 3s 965ms/step - loss: 210.8535 - val_loss: 922.3840 - lr: 0.0010\n",
      "Epoch 38/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 174.4247 - val_loss: 769.0781 - lr: 0.0010\n",
      "Epoch 39/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 174.7663 - val_loss: 774.7213 - lr: 0.0010\n",
      "Epoch 40/5000\n",
      "3/3 [==============================] - 3s 927ms/step - loss: 193.8904 - val_loss: 780.2650 - lr: 0.0010\n",
      "Epoch 41/5000\n",
      "3/3 [==============================] - 3s 959ms/step - loss: 214.7971 - val_loss: 849.6367 - lr: 0.0010\n",
      "Epoch 42/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 186.4595 - val_loss: 754.6349 - lr: 0.0010\n",
      "Epoch 43/5000\n",
      "3/3 [==============================] - 3s 942ms/step - loss: 180.6272 - val_loss: 762.7509 - lr: 0.0010\n",
      "Epoch 44/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 198.8153 - val_loss: 752.0616 - lr: 0.0010\n",
      "Epoch 45/5000\n",
      "3/3 [==============================] - 3s 929ms/step - loss: 212.7071 - val_loss: 864.1154 - lr: 0.0010\n",
      "Epoch 46/5000\n",
      "3/3 [==============================] - 3s 992ms/step - loss: 176.5141 - val_loss: 739.8360 - lr: 0.0010\n",
      "Epoch 47/5000\n",
      "3/3 [==============================] - 3s 958ms/step - loss: 176.8431 - val_loss: 748.2964 - lr: 0.0010\n",
      "Epoch 48/5000\n",
      "3/3 [==============================] - 3s 941ms/step - loss: 193.4728 - val_loss: 746.9540 - lr: 0.0010\n",
      "Epoch 49/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 215.5921 - val_loss: 842.0571 - lr: 0.0010\n",
      "Epoch 50/5000\n",
      "3/3 [==============================] - 3s 992ms/step - loss: 179.6260 - val_loss: 729.2608 - lr: 0.0010\n",
      "Epoch 51/5000\n",
      "3/3 [==============================] - 3s 933ms/step - loss: 195.9726 - val_loss: 763.3397 - lr: 0.0010\n",
      "Epoch 52/5000\n",
      "3/3 [==============================] - 3s 998ms/step - loss: 193.6342 - val_loss: 723.9155 - lr: 0.0010\n",
      "Epoch 53/5000\n",
      "3/3 [==============================] - 3s 942ms/step - loss: 203.8189 - val_loss: 761.4917 - lr: 0.0010\n",
      "Epoch 54/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 198.7010 - val_loss: 735.2284 - lr: 0.0010\n",
      "Epoch 55/5000\n",
      "3/3 [==============================] - 3s 951ms/step - loss: 225.8077 - val_loss: 900.1271 - lr: 0.0010\n",
      "Epoch 56/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 168.3799 - val_loss: 715.5984 - lr: 0.0010\n",
      "Epoch 57/5000\n",
      "3/3 [==============================] - 3s 936ms/step - loss: 171.0932 - val_loss: 718.7560 - lr: 0.0010\n",
      "Epoch 58/5000\n",
      "3/3 [==============================] - 3s 954ms/step - loss: 192.1563 - val_loss: 727.3344 - lr: 0.0010\n",
      "Epoch 59/5000\n",
      "3/3 [==============================] - 3s 936ms/step - loss: 216.1196 - val_loss: 877.9532 - lr: 0.0010\n",
      "Epoch 60/5000\n",
      "3/3 [==============================] - 3s 995ms/step - loss: 161.8318 - val_loss: 709.5629 - lr: 0.0010\n",
      "Epoch 61/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 164.6663 - val_loss: 711.6365 - lr: 0.0010\n",
      "Epoch 62/5000\n",
      "3/3 [==============================] - 3s 944ms/step - loss: 179.4159 - val_loss: 734.7877 - lr: 0.0010\n",
      "Epoch 63/5000\n",
      "3/3 [==============================] - 3s 968ms/step - loss: 213.8525 - val_loss: 814.7443 - lr: 0.0010\n",
      "Epoch 64/5000\n",
      "3/3 [==============================] - 3s 990ms/step - loss: 171.7741 - val_loss: 698.2072 - lr: 0.0010\n",
      "Epoch 65/5000\n",
      "3/3 [==============================] - 3s 945ms/step - loss: 181.5015 - val_loss: 728.7819 - lr: 0.0010\n",
      "Epoch 66/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 180.1961 - val_loss: 701.8568 - lr: 0.0010\n",
      "Epoch 67/5000\n",
      "3/3 [==============================] - 3s 937ms/step - loss: 199.7703 - val_loss: 778.5266 - lr: 0.0010\n",
      "Epoch 68/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 171.9666 - val_loss: 692.0890 - lr: 0.0010\n",
      "Epoch 69/5000\n",
      "3/3 [==============================] - 3s 918ms/step - loss: 184.2660 - val_loss: 728.4576 - lr: 0.0010\n",
      "Epoch 70/5000\n",
      "3/3 [==============================] - 3s 929ms/step - loss: 178.4484 - val_loss: 692.6216 - lr: 0.0010\n",
      "Epoch 71/5000\n",
      "3/3 [==============================] - 3s 926ms/step - loss: 194.4360 - val_loss: 769.1923 - lr: 0.0010\n",
      "Epoch 72/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 168.3969 - val_loss: 688.7597 - lr: 0.0010\n",
      "Epoch 73/5000\n",
      "3/3 [==============================] - 3s 929ms/step - loss: 183.4327 - val_loss: 719.5941 - lr: 0.0010\n",
      "Epoch 74/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 179.4548 - val_loss: 685.3798 - lr: 0.0010\n",
      "Epoch 75/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 190.8283 - val_loss: 759.1966 - lr: 0.0010\n",
      "Epoch 76/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 167.8408 - val_loss: 684.8633 - lr: 0.0010\n",
      "Epoch 77/5000\n",
      "3/3 [==============================] - 3s 930ms/step - loss: 183.7498 - val_loss: 718.6324 - lr: 0.0010\n",
      "Epoch 78/5000\n",
      "3/3 [==============================] - 3s 996ms/step - loss: 177.0846 - val_loss: 681.2726 - lr: 0.0010\n",
      "Epoch 79/5000\n",
      "3/3 [==============================] - 3s 916ms/step - loss: 189.0776 - val_loss: 751.5578 - lr: 0.0010\n",
      "Epoch 80/5000\n",
      "3/3 [==============================] - 3s 996ms/step - loss: 166.5533 - val_loss: 680.9877 - lr: 0.0010\n",
      "Epoch 81/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 185.7371 - val_loss: 724.9415 - lr: 0.0010\n",
      "Epoch 82/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 172.4020 - val_loss: 677.6303 - lr: 0.0010\n",
      "Epoch 83/5000\n",
      "3/3 [==============================] - 3s 969ms/step - loss: 188.4701 - val_loss: 740.4759 - lr: 0.0010\n",
      "Epoch 84/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 167.6724 - val_loss: 675.5502 - lr: 0.0010\n",
      "Epoch 85/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 184.0974 - val_loss: 722.9186 - lr: 0.0010\n",
      "Epoch 86/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 170.2328 - val_loss: 673.8598 - lr: 0.0010\n",
      "Epoch 87/5000\n",
      "3/3 [==============================] - 3s 962ms/step - loss: 186.8959 - val_loss: 739.4072 - lr: 0.0010\n",
      "Epoch 88/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 164.3706 - val_loss: 673.8014 - lr: 0.0010\n",
      "Epoch 89/5000\n",
      "3/3 [==============================] - 3s 939ms/step - loss: 184.6606 - val_loss: 717.0455 - lr: 0.0010\n",
      "Epoch 90/5000\n",
      "3/3 [==============================] - 4s 997ms/step - loss: 170.2927 - val_loss: 669.2675 - lr: 0.0010\n",
      "Epoch 91/5000\n",
      "3/3 [==============================] - 3s 953ms/step - loss: 186.0665 - val_loss: 741.6757 - lr: 0.0010\n",
      "Epoch 92/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 161.4316 - val_loss: 668.5203 - lr: 0.0010\n",
      "Epoch 93/5000\n",
      "3/3 [==============================] - 3s 994ms/step - loss: 180.1582 - val_loss: 705.2832 - lr: 0.0010\n",
      "Epoch 94/5000\n",
      "3/3 [==============================] - 3s 967ms/step - loss: 169.4747 - val_loss: 668.6917 - lr: 0.0010\n",
      "Epoch 95/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 188.3625 - val_loss: 747.5622 - lr: 0.0010\n",
      "Epoch 96/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 159.4790 - val_loss: 663.6874 - lr: 0.0010\n",
      "Epoch 97/5000\n",
      "3/3 [==============================] - 3s 943ms/step - loss: 173.0567 - val_loss: 676.2639 - lr: 0.0010\n",
      "Epoch 98/5000\n",
      "3/3 [==============================] - 3s 984ms/step - loss: 179.7636 - val_loss: 666.6633 - lr: 0.0010\n",
      "Epoch 99/5000\n",
      "3/3 [==============================] - 3s 975ms/step - loss: 192.6850 - val_loss: 767.7139 - lr: 0.0010\n",
      "Epoch 100/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 155.6962 - val_loss: 659.2764 - lr: 0.0010\n",
      "Epoch 101/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 168.6998 - val_loss: 675.4839 - lr: 0.0010\n",
      "Epoch 102/5000\n",
      "3/3 [==============================] - 3s 971ms/step - loss: 168.2936 - val_loss: 696.4091 - lr: 0.0010\n",
      "Epoch 103/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 205.7937 - val_loss: 702.0602 - lr: 0.0010\n",
      "Epoch 104/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 185.4043 - val_loss: 658.1971 - lr: 0.0010\n",
      "Epoch 105/5000\n",
      "3/3 [==============================] - 3s 963ms/step - loss: 199.0037 - val_loss: 832.1514 - lr: 0.0010\n",
      "Epoch 106/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 145.9641 - val_loss: 657.9572 - lr: 0.0010\n",
      "Epoch 107/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 155.3549 - val_loss: 655.8865 - lr: 0.0010\n",
      "Epoch 108/5000\n",
      "3/3 [==============================] - 3s 963ms/step - loss: 187.2744 - val_loss: 739.8563 - lr: 0.0010\n",
      "Epoch 109/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 248.4811 - val_loss: 894.2545 - lr: 0.0010\n",
      "Epoch 110/5000\n",
      "3/3 [==============================] - 3s 950ms/step - loss: 155.8515 - val_loss: 661.7885 - lr: 0.0010\n",
      "Epoch 111/5000\n",
      "3/3 [==============================] - 3s 963ms/step - loss: 156.5897 - val_loss: 659.9065 - lr: 0.0010\n",
      "Epoch 112/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 166.5985 - val_loss: 686.0616 - lr: 0.0010\n",
      "Epoch 113/5000\n",
      "3/3 [==============================] - 3s 975ms/step - loss: 205.4806 - val_loss: 787.5154 - lr: 0.0010\n",
      "Epoch 114/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 153.4395 - val_loss: 650.9664 - lr: 0.0010\n",
      "Epoch 115/5000\n",
      "3/3 [==============================] - 3s 969ms/step - loss: 163.0102 - val_loss: 659.7792 - lr: 0.0010\n",
      "Epoch 116/5000\n",
      "3/3 [==============================] - 3s 974ms/step - loss: 172.3503 - val_loss: 659.3388 - lr: 0.0010\n",
      "Epoch 117/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 195.0205 - val_loss: 784.4726 - lr: 0.0010\n",
      "Epoch 118/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 148.7000 - val_loss: 650.3917 - lr: 0.0010\n",
      "Epoch 119/5000\n",
      "3/3 [==============================] - 3s 973ms/step - loss: 153.8328 - val_loss: 650.9322 - lr: 0.0010\n",
      "Epoch 120/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 165.5304 - val_loss: 678.8975 - lr: 0.0010\n",
      "Epoch 121/5000\n",
      "3/3 [==============================] - 3s 990ms/step - loss: 203.9371 - val_loss: 792.4599 - lr: 0.0010\n",
      "Epoch 122/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 148.8900 - val_loss: 643.9407 - lr: 0.0010\n",
      "Epoch 123/5000\n",
      "3/3 [==============================] - 3s 966ms/step - loss: 160.7741 - val_loss: 650.6058 - lr: 0.0010\n",
      "Epoch 124/5000\n",
      "3/3 [==============================] - 3s 993ms/step - loss: 170.8172 - val_loss: 652.1922 - lr: 0.0010\n",
      "Epoch 125/5000\n",
      "3/3 [==============================] - 3s 971ms/step - loss: 193.2541 - val_loss: 778.2501 - lr: 0.0010\n",
      "Epoch 126/5000\n",
      "3/3 [==============================] - 3s 971ms/step - loss: 146.8176 - val_loss: 644.5828 - lr: 0.0010\n",
      "Epoch 127/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 152.1253 - val_loss: 644.5391 - lr: 0.0010\n",
      "Epoch 128/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 163.7565 - val_loss: 662.2463 - lr: 0.0010\n",
      "Epoch 129/5000\n",
      "3/3 [==============================] - 4s 976ms/step - loss: 194.3238 - val_loss: 740.5706 - lr: 0.0010\n",
      "Epoch 130/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 154.3106 - val_loss: 638.6614 - lr: 0.0010\n",
      "Epoch 131/5000\n",
      "3/3 [==============================] - 3s 979ms/step - loss: 164.6801 - val_loss: 661.2592 - lr: 0.0010\n",
      "Epoch 132/5000\n",
      "3/3 [==============================] - 4s 991ms/step - loss: 162.2910 - val_loss: 644.6871 - lr: 0.0010\n",
      "Epoch 133/5000\n",
      "3/3 [==============================] - 3s 972ms/step - loss: 184.3906 - val_loss: 734.1212 - lr: 0.0010\n",
      "Epoch 134/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 149.1867 - val_loss: 638.0009 - lr: 0.0010\n",
      "Epoch 135/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 162.3934 - val_loss: 645.9765 - lr: 0.0010\n",
      "Epoch 136/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 169.6544 - val_loss: 640.0067 - lr: 0.0010\n",
      "Epoch 137/5000\n",
      "3/3 [==============================] - 3s 996ms/step - loss: 185.4583 - val_loss: 751.1425 - lr: 0.0010\n",
      "Epoch 138/5000\n",
      "3/3 [==============================] - 3s 954ms/step - loss: 145.8661 - val_loss: 637.5737 - lr: 0.0010\n",
      "Epoch 139/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 150.2035 - val_loss: 636.3384 - lr: 0.0010\n",
      "Epoch 140/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 163.8265 - val_loss: 656.3328 - lr: 0.0010\n",
      "Epoch 141/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 196.0613 - val_loss: 772.1539 - lr: 0.0010\n",
      "Epoch 142/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 145.6435 - val_loss: 634.6213 - lr: 0.0010\n",
      "Epoch 143/5000\n",
      "3/3 [==============================] - 3s 999ms/step - loss: 154.5502 - val_loss: 642.0436 - lr: 0.0010\n",
      "Epoch 144/5000\n",
      "3/3 [==============================] - 3s 995ms/step - loss: 162.8418 - val_loss: 657.0104 - lr: 0.0010\n",
      "Epoch 145/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 194.9443 - val_loss: 764.0330 - lr: 0.0010\n",
      "Epoch 146/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 146.0739 - val_loss: 633.6851 - lr: 0.0010\n",
      "Epoch 147/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 152.4449 - val_loss: 637.1375 - lr: 0.0010\n",
      "Epoch 148/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 161.4579 - val_loss: 645.8734 - lr: 0.0010\n",
      "Epoch 149/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 189.1823 - val_loss: 744.7756 - lr: 0.0010\n",
      "Epoch 150/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 146.2440 - val_loss: 630.0626 - lr: 0.0010\n",
      "Epoch 151/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 156.5266 - val_loss: 635.7534 - lr: 0.0010\n",
      "Epoch 152/5000\n",
      "3/3 [==============================] - 3s 993ms/step - loss: 166.2180 - val_loss: 633.1267 - lr: 0.0010\n",
      "Epoch 153/5000\n",
      "3/3 [==============================] - 3s 994ms/step - loss: 182.2535 - val_loss: 742.4464 - lr: 0.0010\n",
      "Epoch 154/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.7932 - val_loss: 629.6394 - lr: 0.0010\n",
      "Epoch 155/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 148.8029 - val_loss: 629.9650 - lr: 0.0010\n",
      "Epoch 156/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 159.3056 - val_loss: 641.1819 - lr: 0.0010\n",
      "Epoch 157/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 186.0419 - val_loss: 731.6468 - lr: 0.0010\n",
      "Epoch 158/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 145.3834 - val_loss: 625.8401 - lr: 0.0010\n",
      "Epoch 159/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 154.8107 - val_loss: 630.1448 - lr: 0.0010\n",
      "Epoch 160/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 166.6846 - val_loss: 630.9368 - lr: 0.0010\n",
      "Epoch 161/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 184.5709 - val_loss: 756.8574 - lr: 0.0010\n",
      "Epoch 162/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 140.8573 - val_loss: 631.4282 - lr: 0.0010\n",
      "Epoch 163/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.6922 - val_loss: 624.8345 - lr: 0.0010\n",
      "Epoch 164/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 157.9696 - val_loss: 657.5347 - lr: 0.0010\n",
      "Epoch 165/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 193.2983 - val_loss: 720.5425 - lr: 0.0010\n",
      "Epoch 166/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 147.0210 - val_loss: 622.0696 - lr: 0.0010\n",
      "Epoch 167/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 155.6777 - val_loss: 632.2752 - lr: 0.0010\n",
      "Epoch 168/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 160.4265 - val_loss: 630.3445 - lr: 0.0010\n",
      "Epoch 169/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 183.9127 - val_loss: 750.2703 - lr: 0.0010\n",
      "Epoch 170/5000\n",
      "3/3 [==============================] - 3s 998ms/step - loss: 140.2020 - val_loss: 628.5497 - lr: 0.0010\n",
      "Epoch 171/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 142.2616 - val_loss: 622.5645 - lr: 0.0010\n",
      "Epoch 172/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 157.1434 - val_loss: 646.5278 - lr: 0.0010\n",
      "Epoch 173/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 189.8428 - val_loss: 745.4119 - lr: 0.0010\n",
      "Epoch 174/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.3855 - val_loss: 621.7766 - lr: 0.0010\n",
      "Epoch 175/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 148.3412 - val_loss: 622.0137 - lr: 0.0010\n",
      "Epoch 176/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 159.4460 - val_loss: 629.0109 - lr: 0.0010\n",
      "Epoch 177/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 183.3628 - val_loss: 738.9710 - lr: 0.0010\n",
      "Epoch 178/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.5797 - val_loss: 623.1490 - lr: 0.0010\n",
      "Epoch 179/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.9015 - val_loss: 620.3405 - lr: 0.0010\n",
      "Epoch 180/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 157.9594 - val_loss: 630.5094 - lr: 0.0010\n",
      "Epoch 181/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 183.0970 - val_loss: 732.4503 - lr: 0.0010\n",
      "Epoch 182/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.7847 - val_loss: 619.8323 - lr: 0.0010\n",
      "Epoch 183/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 147.0353 - val_loss: 620.8796 - lr: 0.0010\n",
      "Epoch 184/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 155.3083 - val_loss: 629.2731 - lr: 0.0010\n",
      "Epoch 185/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 180.3917 - val_loss: 709.1430 - lr: 0.0010\n",
      "Epoch 186/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.1111 - val_loss: 617.9439 - lr: 0.0010\n",
      "Epoch 187/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 152.6036 - val_loss: 622.3959 - lr: 0.0010\n",
      "Epoch 188/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 161.1679 - val_loss: 621.5942 - lr: 0.0010\n",
      "Epoch 189/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 179.8210 - val_loss: 705.1115 - lr: 0.0010\n",
      "Epoch 190/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 149.3242 - val_loss: 631.4072 - lr: 0.0010\n",
      "Epoch 191/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 172.9644 - val_loss: 638.4883 - lr: 0.0010\n",
      "Epoch 192/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 164.9455 - val_loss: 619.3845 - lr: 0.0010\n",
      "Epoch 193/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 164.2897 - val_loss: 694.7691 - lr: 0.0010\n",
      "Epoch 194/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 137.2993 - val_loss: 630.4646 - lr: 0.0010\n",
      "Epoch 195/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 168.2192 - val_loss: 653.2904 - lr: 0.0010\n",
      "Epoch 196/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 224.4966 - val_loss: 621.7452 - lr: 0.0010\n",
      "Epoch 197/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 203.7250 - val_loss: 1038.9191 - lr: 0.0010\n",
      "Epoch 198/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 132.7995 - val_loss: 628.9478 - lr: 0.0010\n",
      "Epoch 199/5000\n",
      "3/3 [==============================] - 3s 959ms/step - loss: 136.4947 - val_loss: 642.5388 - lr: 0.0010\n",
      "Epoch 200/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 173.6804 - val_loss: 658.1406 - lr: 0.0010\n",
      "Epoch 201/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 209.2704 - val_loss: 922.9229 - lr: 0.0010\n",
      "Epoch 202/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 127.8774 - val_loss: 641.5676 - lr: 0.0010\n",
      "Epoch 203/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 123.3218 - val_loss: 614.4167 - lr: 0.0010\n",
      "Epoch 204/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.2986 - val_loss: 717.5189 - lr: 0.0010\n",
      "Epoch 205/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 196.2510 - val_loss: 689.4520 - lr: 0.0010\n",
      "Epoch 206/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 154.6458 - val_loss: 615.8396 - lr: 0.0010\n",
      "Epoch 207/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 157.0684 - val_loss: 655.5825 - lr: 0.0010\n",
      "Epoch 208/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.9104 - val_loss: 616.9788 - lr: 0.0010\n",
      "Epoch 209/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 160.9759 - val_loss: 632.9832 - lr: 0.0010\n",
      "Epoch 210/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 157.8125 - val_loss: 615.3123 - lr: 0.0010\n",
      "Epoch 211/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 170.5139 - val_loss: 698.0420 - lr: 0.0010\n",
      "Epoch 212/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.6436 - val_loss: 613.9135 - lr: 0.0010\n",
      "Epoch 213/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 151.2210 - val_loss: 617.4365 - lr: 0.0010\n",
      "Epoch 214/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 158.3618 - val_loss: 613.3926 - lr: 0.0010\n",
      "Epoch 215/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 174.1511 - val_loss: 718.1030 - lr: 0.0010\n",
      "Epoch 216/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.8366 - val_loss: 611.1080 - lr: 0.0010\n",
      "Epoch 217/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 146.5208 - val_loss: 612.3108 - lr: 0.0010\n",
      "Epoch 218/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 156.0389 - val_loss: 617.0263 - lr: 0.0010\n",
      "Epoch 219/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 177.4456 - val_loss: 724.5795 - lr: 0.0010\n",
      "Epoch 220/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.0898 - val_loss: 614.3165 - lr: 0.0010\n",
      "Epoch 221/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.3303 - val_loss: 613.6254 - lr: 0.0010\n",
      "Epoch 222/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 150.8258 - val_loss: 621.9594 - lr: 0.0010\n",
      "Epoch 223/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 174.7602 - val_loss: 689.7473 - lr: 0.0010\n",
      "Epoch 224/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.3212 - val_loss: 609.4254 - lr: 0.0010\n",
      "Epoch 225/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 151.5774 - val_loss: 620.8642 - lr: 0.0010\n",
      "Epoch 226/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 152.6214 - val_loss: 611.1032 - lr: 0.0010\n",
      "Epoch 227/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 170.1519 - val_loss: 694.2156 - lr: 0.0010\n",
      "Epoch 228/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.2313 - val_loss: 611.6506 - lr: 0.0010\n",
      "Epoch 229/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 145.5596 - val_loss: 614.9254 - lr: 0.0010\n",
      "Epoch 230/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 150.9287 - val_loss: 614.4645 - lr: 0.0010\n",
      "Epoch 231/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 170.1348 - val_loss: 685.3938 - lr: 0.0010\n",
      "Epoch 232/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.6012 - val_loss: 608.7321 - lr: 0.0010\n",
      "Epoch 233/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 148.4123 - val_loss: 618.2064 - lr: 0.0010\n",
      "Epoch 234/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 149.4237 - val_loss: 612.9464 - lr: 0.0010\n",
      "Epoch 235/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 170.2536 - val_loss: 688.8945 - lr: 0.0010\n",
      "Epoch 236/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.7447 - val_loss: 610.8884 - lr: 0.0010\n",
      "Epoch 237/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.9997 - val_loss: 616.3015 - lr: 0.0010\n",
      "Epoch 238/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 146.0339 - val_loss: 614.3432 - lr: 0.0010\n",
      "Epoch 239/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 167.3975 - val_loss: 667.8296 - lr: 0.0010\n",
      "Epoch 240/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.2478 - val_loss: 606.7048 - lr: 0.0010\n",
      "Epoch 241/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 152.8863 - val_loss: 629.4503 - lr: 0.0010\n",
      "Epoch 242/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 147.2962 - val_loss: 608.0374 - lr: 0.0010\n",
      "Epoch 243/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 160.7171 - val_loss: 652.8399 - lr: 0.0010\n",
      "Epoch 244/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.3240 - val_loss: 609.2380 - lr: 0.0010\n",
      "Epoch 245/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 156.7049 - val_loss: 653.0895 - lr: 0.0010\n",
      "Epoch 246/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.3104 - val_loss: 608.9958 - lr: 0.0010\n",
      "Epoch 247/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 152.9154 - val_loss: 625.3781 - lr: 0.0010\n",
      "Epoch 248/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 149.3728 - val_loss: 607.5350 - lr: 0.0010\n",
      "Epoch 249/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 162.3591 - val_loss: 667.3849 - lr: 0.0010\n",
      "Epoch 250/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.3693 - val_loss: 606.3930 - lr: 0.0010\n",
      "Epoch 251/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 147.2039 - val_loss: 616.3132 - lr: 0.0010\n",
      "Epoch 252/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 148.0578 - val_loss: 608.6628 - lr: 0.0010\n",
      "Epoch 253/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 165.4230 - val_loss: 680.1454 - lr: 0.0010\n",
      "Epoch 254/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.8468 - val_loss: 608.7257 - lr: 0.0010\n",
      "Epoch 255/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.3663 - val_loss: 618.1353 - lr: 0.0010\n",
      "Epoch 256/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.1906 - val_loss: 611.2237 - lr: 0.0010\n",
      "Epoch 257/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 163.3377 - val_loss: 652.2986 - lr: 0.0010\n",
      "Epoch 258/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.5854 - val_loss: 604.3378 - lr: 0.0010\n",
      "Epoch 259/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 153.2399 - val_loss: 637.5720 - lr: 0.0010\n",
      "Epoch 260/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.3522 - val_loss: 605.5381 - lr: 0.0010\n",
      "Epoch 261/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 154.9957 - val_loss: 637.0962 - lr: 0.0010\n",
      "Epoch 262/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.9617 - val_loss: 605.8911 - lr: 0.0010\n",
      "Epoch 263/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 155.1231 - val_loss: 645.5852 - lr: 0.0010\n",
      "Epoch 264/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.5461 - val_loss: 605.7300 - lr: 0.0010\n",
      "Epoch 265/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 150.9848 - val_loss: 623.1868 - lr: 0.0010\n",
      "Epoch 266/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 147.1286 - val_loss: 604.9644 - lr: 0.0010\n",
      "Epoch 267/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 160.0100 - val_loss: 661.2456 - lr: 0.0010\n",
      "Epoch 268/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 138.5518 - val_loss: 604.7941 - lr: 0.0010\n",
      "Epoch 269/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.7277 - val_loss: 615.5047 - lr: 0.0010\n",
      "Epoch 270/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.6477 - val_loss: 605.6005 - lr: 0.0010\n",
      "Epoch 271/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 160.1172 - val_loss: 650.6418 - lr: 0.0010\n",
      "Epoch 272/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.4240 - val_loss: 604.2232 - lr: 0.0010\n",
      "Epoch 273/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 150.3433 - val_loss: 628.8825 - lr: 0.0010\n",
      "Epoch 274/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.1237 - val_loss: 605.0757 - lr: 0.0010\n",
      "Epoch 275/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 154.6987 - val_loss: 646.8002 - lr: 0.0010\n",
      "Epoch 276/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.4782 - val_loss: 604.5174 - lr: 0.0010\n",
      "Epoch 277/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 149.0422 - val_loss: 610.2394 - lr: 0.0010\n",
      "Epoch 278/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 157.0308 - val_loss: 614.4639 - lr: 0.0010\n",
      "Epoch 279/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 177.4874 - val_loss: 749.7408 - lr: 0.0010\n",
      "Epoch 280/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 129.0740 - val_loss: 610.2545 - lr: 0.0010\n",
      "Epoch 281/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 130.7208 - val_loss: 602.6715 - lr: 0.0010\n",
      "Epoch 282/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.6210 - val_loss: 622.7281 - lr: 0.0010\n",
      "Epoch 283/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 168.7474 - val_loss: 644.6105 - lr: 0.0010\n",
      "Epoch 284/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 147.5919 - val_loss: 604.5463 - lr: 0.0010\n",
      "Epoch 285/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 164.4249 - val_loss: 654.8072 - lr: 0.0010\n",
      "Epoch 286/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.1021 - val_loss: 604.9375 - lr: 0.0010\n",
      "Epoch 287/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 148.5122 - val_loss: 627.1509 - lr: 0.0010\n",
      "Epoch 288/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.1546 - val_loss: 603.9496 - lr: 0.0010\n",
      "Epoch 289/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 150.3257 - val_loss: 628.8900 - lr: 0.0010\n",
      "Epoch 290/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.7876 - val_loss: 602.8995 - lr: 0.0010\n",
      "Epoch 291/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 151.4428 - val_loss: 632.8896 - lr: 0.0010\n",
      "Epoch 292/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.0838 - val_loss: 601.8444 - lr: 0.0010\n",
      "Epoch 293/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 150.9075 - val_loss: 626.4728 - lr: 0.0010\n",
      "Epoch 294/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.0852 - val_loss: 601.1583 - lr: 0.0010\n",
      "Epoch 295/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 151.6416 - val_loss: 635.5799 - lr: 0.0010\n",
      "Epoch 296/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.4834 - val_loss: 601.2914 - lr: 0.0010\n",
      "Epoch 297/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 149.9931 - val_loss: 625.5100 - lr: 0.0010\n",
      "Epoch 298/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.1698 - val_loss: 601.9448 - lr: 0.0010\n",
      "Epoch 299/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 153.8458 - val_loss: 649.7623 - lr: 0.0010\n",
      "Epoch 300/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.7244 - val_loss: 601.6891 - lr: 0.0010\n",
      "Epoch 301/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 145.9204 - val_loss: 615.7227 - lr: 0.0010\n",
      "Epoch 302/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.8317 - val_loss: 600.4059 - lr: 0.0010\n",
      "Epoch 303/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 152.7215 - val_loss: 638.1211 - lr: 0.0010\n",
      "Epoch 304/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 138.2674 - val_loss: 599.6174 - lr: 0.0010\n",
      "Epoch 305/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 147.9423 - val_loss: 620.6636 - lr: 0.0010\n",
      "Epoch 306/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.0874 - val_loss: 600.3956 - lr: 0.0010\n",
      "Epoch 307/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 150.9665 - val_loss: 638.4753 - lr: 0.0010\n",
      "Epoch 308/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.6420 - val_loss: 600.8809 - lr: 0.0010\n",
      "Epoch 309/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 147.3634 - val_loss: 622.4182 - lr: 0.0010\n",
      "Epoch 310/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.2879 - val_loss: 600.0318 - lr: 0.0010\n",
      "Epoch 311/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 149.6969 - val_loss: 628.2361 - lr: 0.0010\n",
      "Epoch 312/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.9402 - val_loss: 598.7925 - lr: 0.0010\n",
      "Epoch 313/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 148.0833 - val_loss: 619.5458 - lr: 0.0010\n",
      "Epoch 314/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.4120 - val_loss: 598.6537 - lr: 0.0010\n",
      "Epoch 315/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 151.4810 - val_loss: 639.1326 - lr: 0.0010\n",
      "Epoch 316/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.8952 - val_loss: 599.4811 - lr: 0.0010\n",
      "Epoch 317/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 145.5921 - val_loss: 618.5397 - lr: 0.0010\n",
      "Epoch 318/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.9600 - val_loss: 598.9041 - lr: 0.0010\n",
      "Epoch 319/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 149.9640 - val_loss: 629.4758 - lr: 0.0010\n",
      "Epoch 320/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.1250 - val_loss: 598.6196 - lr: 0.0010\n",
      "Epoch 321/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 146.1446 - val_loss: 617.7518 - lr: 0.0010\n",
      "Epoch 322/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.7280 - val_loss: 598.3947 - lr: 0.0010\n",
      "Epoch 323/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 150.6806 - val_loss: 638.2603 - lr: 0.0010\n",
      "Epoch 324/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.3590 - val_loss: 598.8342 - lr: 0.0010\n",
      "Epoch 325/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.7741 - val_loss: 616.0063 - lr: 0.0010\n",
      "Epoch 326/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.0523 - val_loss: 597.9199 - lr: 0.0010\n",
      "Epoch 327/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 149.0348 - val_loss: 631.5848 - lr: 0.0010\n",
      "Epoch 328/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.1810 - val_loss: 597.8368 - lr: 0.0010\n",
      "Epoch 329/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 145.5415 - val_loss: 618.6260 - lr: 0.0010\n",
      "Epoch 330/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.8554 - val_loss: 597.7855 - lr: 0.0010\n",
      "Epoch 331/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 148.1723 - val_loss: 627.5777 - lr: 0.0010\n",
      "Epoch 332/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 138.0182 - val_loss: 597.8469 - lr: 0.0010\n",
      "Epoch 333/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 145.5682 - val_loss: 621.3873 - lr: 0.0010\n",
      "Epoch 334/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 138.4695 - val_loss: 597.7649 - lr: 0.0010\n",
      "Epoch 335/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 147.9567 - val_loss: 625.6429 - lr: 0.0010\n",
      "Epoch 336/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 138.5709 - val_loss: 597.7026 - lr: 0.0010\n",
      "Epoch 337/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.6531 - val_loss: 622.1985 - lr: 0.0010\n",
      "Epoch 338/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 131.3716 - val_loss: 605.8352 - lr: 0.0010\n",
      "Epoch 339/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 153.0962 - val_loss: 596.2513 - lr: 0.0010\n",
      "Epoch 340/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 166.2263 - val_loss: 597.0287 - lr: 0.0010\n",
      "Epoch 341/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 168.5568 - val_loss: 771.7386 - lr: 0.0010\n",
      "Epoch 342/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 122.0036 - val_loss: 607.9621 - lr: 0.0010\n",
      "Epoch 343/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 123.5751 - val_loss: 600.8514 - lr: 0.0010\n",
      "Epoch 344/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 129.6378 - val_loss: 640.9913 - lr: 0.0010\n",
      "Epoch 345/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 164.7392 - val_loss: 623.8157 - lr: 0.0010\n",
      "Epoch 346/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 149.6871 - val_loss: 609.3748 - lr: 0.0010\n",
      "Epoch 347/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.2125 - val_loss: 631.0255 - lr: 0.0010\n",
      "Epoch 348/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.0827 - val_loss: 598.3349 - lr: 0.0010\n",
      "Epoch 349/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.9921 - val_loss: 606.0480 - lr: 0.0010\n",
      "Epoch 350/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.3687 - val_loss: 597.9454 - lr: 0.0010\n",
      "Epoch 351/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 156.8839 - val_loss: 661.0422 - lr: 0.0010\n",
      "Epoch 352/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 132.8333 - val_loss: 598.9320 - lr: 0.0010\n",
      "Epoch 353/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.0886 - val_loss: 615.8335 - lr: 0.0010\n",
      "Epoch 354/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 130.7823 - val_loss: 601.6157 - lr: 0.0010\n",
      "Epoch 355/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 148.7924 - val_loss: 604.2117 - lr: 0.0010\n",
      "Epoch 356/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 148.6384 - val_loss: 596.1417 - lr: 0.0010\n",
      "Epoch 357/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 151.6194 - val_loss: 658.5055 - lr: 0.0010\n",
      "Epoch 358/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 130.0707 - val_loss: 597.6237 - lr: 0.0010\n",
      "Epoch 359/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 134.6705 - val_loss: 612.7025 - lr: 0.0010\n",
      "Epoch 360/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 128.6513 - val_loss: 607.4319 - lr: 0.0010\n",
      "Epoch 361/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 152.5705 - val_loss: 606.3272 - lr: 0.0010\n",
      "Epoch 362/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 148.7630 - val_loss: 597.9244 - lr: 0.0010\n",
      "Epoch 363/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 146.7153 - val_loss: 640.3095 - lr: 0.0010\n",
      "Epoch 364/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 131.8469 - val_loss: 595.5251 - lr: 0.0010\n",
      "Epoch 365/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.8721 - val_loss: 609.7326 - lr: 0.0010\n",
      "Epoch 366/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 133.3822 - val_loss: 597.8690 - lr: 0.0010\n",
      "Epoch 367/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 150.0284 - val_loss: 613.5154 - lr: 0.0010\n",
      "Epoch 368/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.5899 - val_loss: 593.4465 - lr: 0.0010\n",
      "Epoch 369/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 148.6839 - val_loss: 639.7144 - lr: 0.0010\n",
      "Epoch 370/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 132.8769 - val_loss: 594.9600 - lr: 0.0010\n",
      "Epoch 371/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.9326 - val_loss: 614.3197 - lr: 0.0010\n",
      "Epoch 372/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.0357 - val_loss: 595.5107 - lr: 0.0010\n",
      "Epoch 373/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 148.9011 - val_loss: 623.1205 - lr: 0.0010\n",
      "Epoch 374/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 138.2425 - val_loss: 594.9052 - lr: 0.0010\n",
      "Epoch 375/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.8449 - val_loss: 620.5891 - lr: 0.0010\n",
      "Epoch 376/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.5999 - val_loss: 594.1780 - lr: 0.0010\n",
      "Epoch 377/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.1759 - val_loss: 614.4371 - lr: 0.0010\n",
      "Epoch 378/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.9047 - val_loss: 593.4769 - lr: 0.0010\n",
      "Epoch 379/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 146.8554 - val_loss: 624.1799 - lr: 0.0010\n",
      "Epoch 380/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.3733 - val_loss: 594.8593 - lr: 0.0010\n",
      "Epoch 381/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.2155 - val_loss: 616.0139 - lr: 0.0010\n",
      "Epoch 382/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 134.8139 - val_loss: 594.3979 - lr: 0.0010\n",
      "Epoch 383/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 146.7330 - val_loss: 616.7993 - lr: 0.0010\n",
      "Epoch 384/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 138.5848 - val_loss: 593.2219 - lr: 0.0010\n",
      "Epoch 385/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 145.7935 - val_loss: 625.2828 - lr: 0.0010\n",
      "Epoch 386/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 134.6958 - val_loss: 593.7035 - lr: 0.0010\n",
      "Epoch 387/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.6736 - val_loss: 617.3853 - lr: 0.0010\n",
      "Epoch 388/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 132.1974 - val_loss: 593.9537 - lr: 0.0010\n",
      "Epoch 389/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.2867 - val_loss: 612.4451 - lr: 0.0010\n",
      "Epoch 390/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.8859 - val_loss: 593.3533 - lr: 0.0010\n",
      "Epoch 391/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.7982 - val_loss: 616.5417 - lr: 0.0010\n",
      "Epoch 392/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.5123 - val_loss: 592.9428 - lr: 0.0010\n",
      "Epoch 393/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.9212 - val_loss: 617.7314 - lr: 0.0010\n",
      "Epoch 394/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 134.5801 - val_loss: 592.7769 - lr: 0.0010\n",
      "Epoch 395/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.1072 - val_loss: 616.2312 - lr: 0.0010\n",
      "Epoch 396/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.1479 - val_loss: 592.6678 - lr: 0.0010\n",
      "Epoch 397/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.5484 - val_loss: 617.8676 - lr: 0.0010\n",
      "Epoch 398/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.6431 - val_loss: 593.0485 - lr: 0.0010\n",
      "Epoch 399/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.6937 - val_loss: 619.2383 - lr: 0.0010\n",
      "Epoch 400/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 131.3715 - val_loss: 592.5327 - lr: 0.0010\n",
      "Epoch 401/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.3069 - val_loss: 610.4247 - lr: 0.0010\n",
      "Epoch 402/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.4637 - val_loss: 592.1400 - lr: 0.0010\n",
      "Epoch 403/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.0740 - val_loss: 617.9944 - lr: 0.0010\n",
      "Epoch 404/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 134.9603 - val_loss: 592.8568 - lr: 0.0010\n",
      "Epoch 405/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.1402 - val_loss: 621.0433 - lr: 0.0010\n",
      "Epoch 406/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 130.7580 - val_loss: 592.3608 - lr: 0.0010\n",
      "Epoch 407/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.9632 - val_loss: 611.9592 - lr: 0.0010\n",
      "Epoch 408/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 134.7744 - val_loss: 591.4073 - lr: 0.0010\n",
      "Epoch 409/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.5046 - val_loss: 617.8176 - lr: 0.0010\n",
      "Epoch 410/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 134.6237 - val_loss: 592.6537 - lr: 0.0010\n",
      "Epoch 411/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.6778 - val_loss: 601.5646 - lr: 0.0010\n",
      "Epoch 412/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 138.7799 - val_loss: 653.7319 - lr: 0.0010\n",
      "Epoch 413/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 178.8599 - val_loss: 676.6276 - lr: 0.0010\n",
      "Epoch 414/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.0807 - val_loss: 623.0635 - lr: 0.0010\n",
      "Epoch 415/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 128.6423 - val_loss: 619.4386 - lr: 0.0010\n",
      "Epoch 416/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 120.1647 - val_loss: 601.5809 - lr: 0.0010\n",
      "Epoch 417/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.2330 - val_loss: 596.2009 - lr: 0.0010\n",
      "Epoch 418/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.3506 - val_loss: 594.2654 - lr: 0.0010\n",
      "Epoch 419/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 153.9447 - val_loss: 639.9147 - lr: 0.0010\n",
      "Epoch 420/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 134.1364 - val_loss: 596.7220 - lr: 0.0010\n",
      "Epoch 421/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.2220 - val_loss: 614.3380 - lr: 0.0010\n",
      "Epoch 422/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 127.7883 - val_loss: 596.4705 - lr: 0.0010\n",
      "Epoch 423/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.0712 - val_loss: 600.4733 - lr: 0.0010\n",
      "Epoch 424/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.2630 - val_loss: 590.7653 - lr: 0.0010\n",
      "Epoch 425/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 150.0910 - val_loss: 660.7050 - lr: 0.0010\n",
      "Epoch 426/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 126.1016 - val_loss: 593.0555 - lr: 0.0010\n",
      "Epoch 427/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 129.9248 - val_loss: 601.9037 - lr: 0.0010\n",
      "Epoch 428/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 125.8873 - val_loss: 604.0938 - lr: 0.0010\n",
      "Epoch 429/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 150.5265 - val_loss: 604.3250 - lr: 0.0010\n",
      "Epoch 430/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.6682 - val_loss: 591.0031 - lr: 0.0010\n",
      "Epoch 431/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 145.8393 - val_loss: 643.8861 - lr: 0.0010\n",
      "Epoch 432/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 127.9765 - val_loss: 592.0848 - lr: 0.0010\n",
      "Epoch 433/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 133.6187 - val_loss: 615.3412 - lr: 0.0010\n",
      "Epoch 434/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 123.4520 - val_loss: 597.4262 - lr: 0.0010\n",
      "Epoch 435/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.1767 - val_loss: 602.2991 - lr: 0.0010\n",
      "Epoch 436/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.9282 - val_loss: 589.3773 - lr: 0.0010\n",
      "Epoch 437/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.7984 - val_loss: 623.1406 - lr: 0.0010\n",
      "Epoch 438/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 131.6973 - val_loss: 590.4232 - lr: 0.0010\n",
      "Epoch 439/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 137.7596 - val_loss: 612.7569 - lr: 0.0010\n",
      "Epoch 440/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 128.7818 - val_loss: 590.6879 - lr: 0.0010\n",
      "Epoch 441/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.4267 - val_loss: 603.4064 - lr: 0.0010\n",
      "Epoch 442/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 138.1244 - val_loss: 588.6703 - lr: 0.0010\n",
      "Epoch 443/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 146.4808 - val_loss: 638.1485 - lr: 0.0010\n",
      "Epoch 444/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 129.0354 - val_loss: 590.5856 - lr: 0.0010\n",
      "Epoch 445/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 134.6428 - val_loss: 621.1723 - lr: 0.0010\n",
      "Epoch 446/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 121.9159 - val_loss: 595.4361 - lr: 0.0010\n",
      "Epoch 447/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.7055 - val_loss: 599.4153 - lr: 0.0010\n",
      "Epoch 448/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.0473 - val_loss: 588.5342 - lr: 0.0010\n",
      "Epoch 449/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 145.0674 - val_loss: 623.1021 - lr: 0.0010\n",
      "Epoch 450/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 132.3534 - val_loss: 590.1948 - lr: 0.0010\n",
      "Epoch 451/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.7001 - val_loss: 615.2698 - lr: 0.0010\n",
      "Epoch 452/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 126.6669 - val_loss: 590.0298 - lr: 0.0010\n",
      "Epoch 453/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.8305 - val_loss: 607.5037 - lr: 0.0010\n",
      "Epoch 454/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 131.7510 - val_loss: 587.8376 - lr: 0.0010\n",
      "Epoch 455/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.2524 - val_loss: 611.0057 - lr: 0.0010\n",
      "Epoch 456/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 131.6861 - val_loss: 588.1895 - lr: 0.0010\n",
      "Epoch 457/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.2951 - val_loss: 610.6201 - lr: 0.0010\n",
      "Epoch 458/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 132.5150 - val_loss: 588.3308 - lr: 0.0010\n",
      "Epoch 459/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.8948 - val_loss: 611.0732 - lr: 0.0010\n",
      "Epoch 460/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 132.9832 - val_loss: 588.1315 - lr: 0.0010\n",
      "Epoch 461/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.5255 - val_loss: 618.1196 - lr: 0.0010\n",
      "Epoch 462/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 127.9558 - val_loss: 587.7974 - lr: 0.0010\n",
      "Epoch 463/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.0028 - val_loss: 611.9435 - lr: 0.0010\n",
      "Epoch 464/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 126.5647 - val_loss: 592.0408 - lr: 0.0010\n",
      "Epoch 465/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.6448 - val_loss: 605.8794 - lr: 0.0010\n",
      "Epoch 466/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.5901 - val_loss: 589.0723 - lr: 0.0010\n",
      "Epoch 467/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.3062 - val_loss: 624.3828 - lr: 0.0010\n",
      "Epoch 468/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 128.4172 - val_loss: 588.0402 - lr: 0.0010\n",
      "Epoch 469/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.9991 - val_loss: 617.4352 - lr: 0.0010\n",
      "Epoch 470/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 124.6317 - val_loss: 589.2816 - lr: 0.0010\n",
      "Epoch 471/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.0211 - val_loss: 608.0522 - lr: 0.0010\n",
      "Epoch 472/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 126.9560 - val_loss: 590.8963 - lr: 0.0010\n",
      "Epoch 473/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.9996 - val_loss: 605.4152 - lr: 0.0010\n",
      "Epoch 474/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.9958 - val_loss: 588.1666 - lr: 0.0010\n",
      "Epoch 475/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.4911 - val_loss: 625.6147 - lr: 0.0010\n",
      "Epoch 476/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 122.7343 - val_loss: 669.7268 - lr: 0.0010\n",
      "Epoch 477/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 170.2879 - val_loss: 717.6822 - lr: 0.0010\n",
      "Epoch 478/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 259.6138 - val_loss: 796.4272 - lr: 0.0010\n",
      "Epoch 479/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 147.9312 - val_loss: 891.1944 - lr: 0.0010\n",
      "Epoch 480/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 119.9131 - val_loss: 655.2245 - lr: 0.0010\n",
      "Epoch 481/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 114.5777 - val_loss: 704.1292 - lr: 0.0010\n",
      "Epoch 482/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.7163 - val_loss: 627.1285 - lr: 0.0010\n",
      "Epoch 483/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 181.4031 - val_loss: 597.0624 - lr: 0.0010\n",
      "Epoch 484/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 183.9344 - val_loss: 889.1195 - lr: 0.0010\n",
      "Epoch 485/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 119.3233 - val_loss: 677.9932 - lr: 0.0010\n",
      "Epoch 486/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 110.8120 - val_loss: 611.6831 - lr: 0.0010\n",
      "Epoch 487/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 108.2242 - val_loss: 588.8219 - lr: 0.0010\n",
      "Epoch 488/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 108.9629 - val_loss: 582.1531 - lr: 0.0010\n",
      "Epoch 489/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 109.0650 - val_loss: 628.4733 - lr: 0.0010\n",
      "Epoch 490/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 127.5583 - val_loss: 705.1462 - lr: 0.0010\n",
      "Epoch 491/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 197.4335 - val_loss: 730.9395 - lr: 0.0010\n",
      "Epoch 492/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 256.1898 - val_loss: 936.5588 - lr: 0.0010\n",
      "Epoch 493/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 127.5633 - val_loss: 596.1166 - lr: 0.0010\n",
      "Epoch 494/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 132.6556 - val_loss: 589.3550 - lr: 0.0010\n",
      "Epoch 495/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.3903 - val_loss: 621.1002 - lr: 0.0010\n",
      "Epoch 496/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 172.1430 - val_loss: 640.3352 - lr: 0.0010\n",
      "Epoch 497/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.2596 - val_loss: 592.4991 - lr: 0.0010\n",
      "Epoch 498/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 145.3913 - val_loss: 611.7867 - lr: 0.0010\n",
      "Epoch 499/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.9501 - val_loss: 587.1580 - lr: 0.0010\n",
      "Epoch 500/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 146.2262 - val_loss: 612.8639 - lr: 0.0010\n",
      "Epoch 501/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 136.7918 - val_loss: 587.4947 - lr: 0.0010\n",
      "Epoch 502/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.5179 - val_loss: 609.7057 - lr: 0.0010\n",
      "Epoch 503/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.0188 - val_loss: 586.5604 - lr: 0.0010\n",
      "Epoch 504/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.4487 - val_loss: 612.6887 - lr: 0.0010\n",
      "Epoch 505/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 133.6967 - val_loss: 586.0750 - lr: 0.0010\n",
      "Epoch 506/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.3813 - val_loss: 614.6672 - lr: 0.0010\n",
      "Epoch 507/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 128.8450 - val_loss: 585.1911 - lr: 0.0010\n",
      "Epoch 508/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.5943 - val_loss: 605.8694 - lr: 0.0010\n",
      "Epoch 509/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 128.3303 - val_loss: 587.0757 - lr: 0.0010\n",
      "Epoch 510/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.5254 - val_loss: 602.1684 - lr: 0.0010\n",
      "Epoch 511/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.5172 - val_loss: 586.6021 - lr: 0.0010\n",
      "Epoch 512/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.6814 - val_loss: 616.3309 - lr: 0.0010\n",
      "Epoch 513/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 129.6828 - val_loss: 585.8570 - lr: 0.0010\n",
      "Epoch 514/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.1680 - val_loss: 610.8272 - lr: 0.0010\n",
      "Epoch 515/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 128.8362 - val_loss: 585.0186 - lr: 0.0010\n",
      "Epoch 516/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.8098 - val_loss: 609.3536 - lr: 0.0010\n",
      "Epoch 517/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 126.2571 - val_loss: 587.6261 - lr: 0.0010\n",
      "Epoch 518/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.4216 - val_loss: 600.4244 - lr: 0.0010\n",
      "Epoch 519/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.8148 - val_loss: 585.7263 - lr: 0.0010\n",
      "Epoch 520/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.6167 - val_loss: 614.0925 - lr: 0.0010\n",
      "Epoch 521/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 130.4908 - val_loss: 585.5056 - lr: 0.0010\n",
      "Epoch 522/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.4698 - val_loss: 618.5839 - lr: 0.0010\n",
      "Epoch 523/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 123.1608 - val_loss: 587.6859 - lr: 0.0010\n",
      "Epoch 524/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.1296 - val_loss: 605.8629 - lr: 0.0010\n",
      "Epoch 525/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 126.5594 - val_loss: 587.9075 - lr: 0.0010\n",
      "Epoch 526/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.6289 - val_loss: 601.3825 - lr: 0.0010\n",
      "Epoch 527/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.4976 - val_loss: 586.1953 - lr: 0.0010\n",
      "Epoch 528/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 138.8657 - val_loss: 616.2773 - lr: 0.0010\n",
      "Epoch 529/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 127.4971 - val_loss: 585.1774 - lr: 0.0010\n",
      "Epoch 530/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 133.0698 - val_loss: 614.9277 - lr: 0.0010\n",
      "Epoch 531/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 120.4910 - val_loss: 590.0135 - lr: 0.0010\n",
      "Epoch 532/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 137.5309 - val_loss: 600.6879 - lr: 0.0010\n",
      "Epoch 533/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 128.3092 - val_loss: 585.9976 - lr: 0.0010\n",
      "Epoch 534/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 142.5039 - val_loss: 610.6810 - lr: 0.0010\n",
      "Epoch 535/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 131.5711 - val_loss: 585.8193 - lr: 0.0010\n",
      "Epoch 536/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 135.1776 - val_loss: 595.7930 - lr: 0.0010\n",
      "Epoch 537/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 134.8407 - val_loss: 625.7322 - lr: 0.0010\n",
      "Epoch 538/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 169.3923 - val_loss: 686.2930 - lr: 0.0010\n",
      "Epoch 539/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 128.6953 - val_loss: 611.0060 - lr: 0.0010\n",
      "Epoch 540/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 121.7128 - val_loss: 601.9819 - lr: 0.0010\n",
      "Epoch 541/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 116.2841 - val_loss: 593.8089 - lr: 0.0010\n",
      "Epoch 542/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 132.8020 - val_loss: 585.6445 - lr: 0.0010\n",
      "Epoch 543/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 134.7782 - val_loss: 589.2996 - lr: 0.0010\n",
      "Epoch 544/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 150.7460 - val_loss: 647.8771 - lr: 0.0010\n",
      "Epoch 545/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 126.5599 - val_loss: 589.7991 - lr: 0.0010\n",
      "Epoch 546/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 125.7751 - val_loss: 586.5262 - lr: 0.0010\n",
      "Epoch 547/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.8132 - val_loss: 765.0129 - lr: 0.0010\n",
      "Epoch 548/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 214.0828 - val_loss: 702.5428 - lr: 0.0010\n",
      "Epoch 549/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 143.0645 - val_loss: 630.4701 - lr: 0.0010\n",
      "Epoch 550/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 127.6215 - val_loss: 628.5781 - lr: 0.0010\n",
      "Epoch 551/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 114.9085 - val_loss: 593.7731 - lr: 0.0010\n",
      "Epoch 552/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 129.9347 - val_loss: 586.9664 - lr: 0.0010\n",
      "Epoch 553/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.9667 - val_loss: 594.1852 - lr: 0.0010\n",
      "Epoch 554/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 153.7316 - val_loss: 658.9249 - lr: 0.0010\n",
      "Epoch 555/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 125.8198 - val_loss: 590.8434 - lr: 0.0010\n",
      "Epoch 556/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 127.8958 - val_loss: 601.2919 - lr: 0.0010\n",
      "Epoch 557/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 122.1347 - val_loss: 599.5783 - lr: 0.0010\n",
      "Epoch 558/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 145.3602 - val_loss: 599.8718 - lr: 0.0010\n",
      "Epoch 559/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 138.5276 - val_loss: 590.1523 - lr: 0.0010\n",
      "Epoch 560/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 136.5010 - val_loss: 628.3824 - lr: 0.0010\n",
      "Epoch 561/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 121.7868 - val_loss: 585.6805 - lr: 0.0010\n",
      "Epoch 562/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 132.3176 - val_loss: 606.2870 - lr: 0.0010\n",
      "Epoch 563/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 122.1979 - val_loss: 592.3091 - lr: 0.0010\n",
      "Epoch 564/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 141.0741 - val_loss: 602.9216 - lr: 0.0010\n",
      "Epoch 565/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 132.5960 - val_loss: 584.1616 - lr: 0.0010\n",
      "Epoch 566/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 147.7458 - val_loss: 589.7266 - lr: 0.0010\n",
      "Epoch 567/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 145.3311 - val_loss: 584.2565 - lr: 0.0010\n",
      "Epoch 568/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 144.8980 - val_loss: 646.7229 - lr: 0.0010\n",
      "Epoch 569/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 123.7091 - val_loss: 589.9464 - lr: 0.0010\n",
      "Epoch 570/5000\n",
      "3/3 [==============================] - 4s 994ms/step - loss: 125.3558 - val_loss: 598.2180 - lr: 0.0010\n",
      "Epoch 571/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 121.0898 - val_loss: 589.2581 - lr: 0.0010\n",
      "Epoch 572/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 133.5087 - val_loss: 599.4891 - lr: 0.0010\n",
      "Epoch 573/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 124.9437 - val_loss: 587.8826 - lr: 0.0010\n",
      "Epoch 574/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 140.7775 - val_loss: 603.1830 - lr: 0.0010\n",
      "Epoch 575/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 132.7810 - val_loss: 585.7583 - lr: 0.0010\n",
      "Epoch 576/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 135.7630 - val_loss: 630.4591 - lr: 0.0010\n",
      "Epoch 577/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 119.3122 - val_loss: 585.6771 - lr: 0.0010\n",
      "Epoch 578/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 131.5379 - val_loss: 600.4502 - lr: 0.0010\n",
      "Epoch 579/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 123.8756 - val_loss: 589.0551 - lr: 0.0010\n",
      "Epoch 580/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 139.5627 - val_loss: 608.4324 - lr: 0.0010\n",
      "Epoch 581/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 127.8422 - val_loss: 583.4137 - lr: 0.0010\n",
      "Epoch 582/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 133.2789 - val_loss: 617.9733 - lr: 0.0010\n",
      "Epoch 583/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 119.4311 - val_loss: 586.5461 - lr: 0.0010\n",
      "Epoch 584/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 133.2049 - val_loss: 602.8259 - lr: 0.0010\n",
      "Epoch 585/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 123.1916 - val_loss: 587.6514 - lr: 0.0010\n",
      "Epoch 586/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 138.8572 - val_loss: 606.4464 - lr: 0.0010\n",
      "Epoch 587/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 128.4163 - val_loss: 583.5337 - lr: 0.0010\n",
      "Epoch 588/5000\n",
      "3/3 [==============================] - ETA: 0s - loss: 133.4312\n",
      "Epoch 588: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "3/3 [==============================] - 4s 1s/step - loss: 133.4312 - val_loss: 616.1050 - lr: 0.0010\n",
      "Epoch 589/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 107.5584 - val_loss: 585.3176 - lr: 1.0000e-04\n",
      "Epoch 590/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.9364 - val_loss: 588.5041 - lr: 1.0000e-04\n",
      "Epoch 591/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 107.5170 - val_loss: 588.9784 - lr: 1.0000e-04\n",
      "Epoch 592/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 106.5337 - val_loss: 583.0210 - lr: 1.0000e-04\n",
      "Epoch 593/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.7209 - val_loss: 588.9052 - lr: 1.0000e-04\n",
      "Epoch 594/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 106.5130 - val_loss: 593.1371 - lr: 1.0000e-04\n",
      "Epoch 595/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 106.5727 - val_loss: 588.2213 - lr: 1.0000e-04\n",
      "Epoch 596/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.8130 - val_loss: 582.8203 - lr: 1.0000e-04\n",
      "Epoch 597/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.5157 - val_loss: 581.7589 - lr: 1.0000e-04\n",
      "Epoch 598/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.6215 - val_loss: 581.5712 - lr: 1.0000e-04\n",
      "Epoch 599/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.5217 - val_loss: 581.6859 - lr: 1.0000e-04\n",
      "Epoch 600/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.4536 - val_loss: 583.0215 - lr: 1.0000e-04\n",
      "Epoch 601/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.5967 - val_loss: 584.2262 - lr: 1.0000e-04\n",
      "Epoch 602/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.6377 - val_loss: 583.4586 - lr: 1.0000e-04\n",
      "Epoch 603/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.4835 - val_loss: 581.8239 - lr: 1.0000e-04\n",
      "Epoch 604/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.3414 - val_loss: 581.0898 - lr: 1.0000e-04\n",
      "Epoch 605/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.2980 - val_loss: 580.9604 - lr: 1.0000e-04\n",
      "Epoch 606/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.2803 - val_loss: 581.1287 - lr: 1.0000e-04\n",
      "Epoch 607/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.2879 - val_loss: 581.5180 - lr: 1.0000e-04\n",
      "Epoch 608/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.3223 - val_loss: 582.0096 - lr: 1.0000e-04\n",
      "Epoch 609/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.3483 - val_loss: 581.8345 - lr: 1.0000e-04\n",
      "Epoch 610/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.2941 - val_loss: 581.3262 - lr: 1.0000e-04\n",
      "Epoch 611/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 105.2502 - val_loss: 581.1270 - lr: 1.0000e-04\n",
      "Epoch 612/5000\n",
      "3/3 [==============================] - 6s 1s/step - loss: 105.2339 - val_loss: 581.0621 - lr: 1.0000e-04\n",
      "Epoch 613/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 105.2276 - val_loss: 581.0582 - lr: 1.0000e-04\n",
      "Epoch 614/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 105.2280 - val_loss: 581.1179 - lr: 1.0000e-04\n",
      "Epoch 615/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.2386 - val_loss: 581.1197 - lr: 1.0000e-04\n",
      "Epoch 616/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 105.2293 - val_loss: 580.9455 - lr: 1.0000e-04\n",
      "Epoch 617/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.2184 - val_loss: 580.8267 - lr: 1.0000e-04\n",
      "Epoch 618/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.2092 - val_loss: 580.7259 - lr: 1.0000e-04\n",
      "Epoch 619/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.1946 - val_loss: 580.6448 - lr: 1.0000e-04\n",
      "Epoch 620/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.1748 - val_loss: 580.5753 - lr: 1.0000e-04\n",
      "Epoch 621/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.1539 - val_loss: 580.4177 - lr: 1.0000e-04\n",
      "Epoch 622/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.1141 - val_loss: 580.1365 - lr: 1.0000e-04\n",
      "Epoch 623/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.0811 - val_loss: 579.9909 - lr: 1.0000e-04\n",
      "Epoch 624/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.0694 - val_loss: 579.9765 - lr: 1.0000e-04\n",
      "Epoch 625/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.0726 - val_loss: 580.0171 - lr: 1.0000e-04\n",
      "Epoch 626/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.0789 - val_loss: 579.8986 - lr: 1.0000e-04\n",
      "Epoch 627/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.0473 - val_loss: 579.6278 - lr: 1.0000e-04\n",
      "Epoch 628/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.0162 - val_loss: 579.5005 - lr: 1.0000e-04\n",
      "Epoch 629/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.0016 - val_loss: 579.5040 - lr: 1.0000e-04\n",
      "Epoch 630/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 105.0008 - val_loss: 579.5173 - lr: 1.0000e-04\n",
      "Epoch 631/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.9958 - val_loss: 579.3669 - lr: 1.0000e-04\n",
      "Epoch 632/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.9571 - val_loss: 579.1541 - lr: 1.0000e-04\n",
      "Epoch 633/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.9380 - val_loss: 579.1174 - lr: 1.0000e-04\n",
      "Epoch 634/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.9336 - val_loss: 579.1612 - lr: 1.0000e-04\n",
      "Epoch 635/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.9375 - val_loss: 579.0542 - lr: 1.0000e-04\n",
      "Epoch 636/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.9041 - val_loss: 578.8193 - lr: 1.0000e-04\n",
      "Epoch 637/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.8791 - val_loss: 578.7358 - lr: 1.0000e-04\n",
      "Epoch 638/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.8688 - val_loss: 578.7723 - lr: 1.0000e-04\n",
      "Epoch 639/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.8728 - val_loss: 578.6990 - lr: 1.0000e-04\n",
      "Epoch 640/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.8450 - val_loss: 578.5109 - lr: 1.0000e-04\n",
      "Epoch 641/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.8229 - val_loss: 578.4570 - lr: 1.0000e-04\n",
      "Epoch 642/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.8157 - val_loss: 578.4720 - lr: 1.0000e-04\n",
      "Epoch 643/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.8186 - val_loss: 578.3948 - lr: 1.0000e-04\n",
      "Epoch 644/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.7943 - val_loss: 578.1966 - lr: 1.0000e-04\n",
      "Epoch 645/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.7716 - val_loss: 578.1051 - lr: 1.0000e-04\n",
      "Epoch 646/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.7608 - val_loss: 578.1076 - lr: 1.0000e-04\n",
      "Epoch 647/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.7582 - val_loss: 578.1303 - lr: 1.0000e-04\n",
      "Epoch 648/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.7584 - val_loss: 578.0166 - lr: 1.0000e-04\n",
      "Epoch 649/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.7246 - val_loss: 577.8203 - lr: 1.0000e-04\n",
      "Epoch 650/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.7033 - val_loss: 577.7764 - lr: 1.0000e-04\n",
      "Epoch 651/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.6989 - val_loss: 577.8906 - lr: 1.0000e-04\n",
      "Epoch 652/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.7061 - val_loss: 577.8730 - lr: 1.0000e-04\n",
      "Epoch 653/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.6806 - val_loss: 577.6312 - lr: 1.0000e-04\n",
      "Epoch 654/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.6570 - val_loss: 577.5938 - lr: 1.0000e-04\n",
      "Epoch 655/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.6520 - val_loss: 577.5748 - lr: 1.0000e-04\n",
      "Epoch 656/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.6340 - val_loss: 577.4705 - lr: 1.0000e-04\n",
      "Epoch 657/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.6222 - val_loss: 577.5351 - lr: 1.0000e-04\n",
      "Epoch 658/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.6235 - val_loss: 577.5111 - lr: 1.0000e-04\n",
      "Epoch 659/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.6003 - val_loss: 577.3502 - lr: 1.0000e-04\n",
      "Epoch 660/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.5861 - val_loss: 577.2283 - lr: 1.0000e-04\n",
      "Epoch 661/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.5641 - val_loss: 577.0853 - lr: 1.0000e-04\n",
      "Epoch 662/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.5496 - val_loss: 577.1641 - lr: 1.0000e-04\n",
      "Epoch 663/5000\n",
      "3/3 [==============================] - 7s 2s/step - loss: 104.5515 - val_loss: 577.2199 - lr: 1.0000e-04\n",
      "Epoch 664/5000\n",
      "3/3 [==============================] - 8s 2s/step - loss: 104.5392 - val_loss: 577.0139 - lr: 1.0000e-04\n",
      "Epoch 665/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.5093 - val_loss: 576.8179 - lr: 1.0000e-04\n",
      "Epoch 666/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.4943 - val_loss: 576.8746 - lr: 1.0000e-04\n",
      "Epoch 667/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.4956 - val_loss: 576.9528 - lr: 1.0000e-04\n",
      "Epoch 668/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.4837 - val_loss: 576.9073 - lr: 1.0000e-04\n",
      "Epoch 669/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.4763 - val_loss: 576.8591 - lr: 1.0000e-04\n",
      "Epoch 670/5000\n",
      "3/3 [==============================] - 8s 3s/step - loss: 104.4602 - val_loss: 576.7173 - lr: 1.0000e-04\n",
      "Epoch 671/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.4466 - val_loss: 576.7816 - lr: 1.0000e-04\n",
      "Epoch 672/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.4442 - val_loss: 576.7881 - lr: 1.0000e-04\n",
      "Epoch 673/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.4235 - val_loss: 576.6504 - lr: 1.0000e-04\n",
      "Epoch 674/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 104.4137 - val_loss: 576.5971 - lr: 1.0000e-04\n",
      "Epoch 675/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.3979 - val_loss: 576.5157 - lr: 1.0000e-04\n",
      "Epoch 676/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.3879 - val_loss: 576.5200 - lr: 1.0000e-04\n",
      "Epoch 677/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.3724 - val_loss: 576.4595 - lr: 1.0000e-04\n",
      "Epoch 678/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.3667 - val_loss: 576.4653 - lr: 1.0000e-04\n",
      "Epoch 679/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.3540 - val_loss: 576.4294 - lr: 1.0000e-04\n",
      "Epoch 680/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.3486 - val_loss: 576.4177 - lr: 1.0000e-04\n",
      "Epoch 681/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 104.3314 - val_loss: 576.2870 - lr: 1.0000e-04\n",
      "Epoch 682/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.3229 - val_loss: 576.3667 - lr: 1.0000e-04\n",
      "Epoch 683/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.3247 - val_loss: 576.3525 - lr: 1.0000e-04\n",
      "Epoch 684/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.3015 - val_loss: 576.1725 - lr: 1.0000e-04\n",
      "Epoch 685/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 104.2876 - val_loss: 576.1929 - lr: 1.0000e-04\n",
      "Epoch 686/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.2867 - val_loss: 576.1631 - lr: 1.0000e-04\n",
      "Epoch 687/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.2660 - val_loss: 576.0028 - lr: 1.0000e-04\n",
      "Epoch 688/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.2519 - val_loss: 576.0657 - lr: 1.0000e-04\n",
      "Epoch 689/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.2529 - val_loss: 576.0302 - lr: 1.0000e-04\n",
      "Epoch 690/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.2320 - val_loss: 575.8463 - lr: 1.0000e-04\n",
      "Epoch 691/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.2178 - val_loss: 575.8871 - lr: 1.0000e-04\n",
      "Epoch 692/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 104.2175 - val_loss: 576.0228 - lr: 1.0000e-04\n",
      "Epoch 693/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.2208 - val_loss: 575.9333 - lr: 1.0000e-04\n",
      "Epoch 694/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.1906 - val_loss: 575.6455 - lr: 1.0000e-04\n",
      "Epoch 695/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.1691 - val_loss: 575.6218 - lr: 1.0000e-04\n",
      "Epoch 696/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.1644 - val_loss: 575.7720 - lr: 1.0000e-04\n",
      "Epoch 697/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.1692 - val_loss: 575.9011 - lr: 1.0000e-04\n",
      "Epoch 698/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.1717 - val_loss: 575.6997 - lr: 1.0000e-04\n",
      "Epoch 699/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.1283 - val_loss: 575.3080 - lr: 1.0000e-04\n",
      "Epoch 700/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.1007 - val_loss: 575.2686 - lr: 1.0000e-04\n",
      "Epoch 701/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.0956 - val_loss: 575.5062 - lr: 1.0000e-04\n",
      "Epoch 702/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.1101 - val_loss: 575.7291 - lr: 1.0000e-04\n",
      "Epoch 703/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.1192 - val_loss: 575.5797 - lr: 1.0000e-04\n",
      "Epoch 704/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 104.0786 - val_loss: 575.1884 - lr: 1.0000e-04\n",
      "Epoch 705/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.0472 - val_loss: 575.1058 - lr: 1.0000e-04\n",
      "Epoch 706/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.0398 - val_loss: 575.2799 - lr: 1.0000e-04\n",
      "Epoch 707/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.0500 - val_loss: 575.4683 - lr: 1.0000e-04\n",
      "Epoch 708/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.0553 - val_loss: 575.5288 - lr: 1.0000e-04\n",
      "Epoch 709/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.0478 - val_loss: 575.4359 - lr: 1.0000e-04\n",
      "Epoch 710/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.0272 - val_loss: 575.2357 - lr: 1.0000e-04\n",
      "Epoch 711/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 104.0017 - val_loss: 575.1152 - lr: 1.0000e-04\n",
      "Epoch 712/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.9873 - val_loss: 575.1267 - lr: 1.0000e-04\n",
      "Epoch 713/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.9814 - val_loss: 575.1858 - lr: 1.0000e-04\n",
      "Epoch 714/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.9807 - val_loss: 575.1466 - lr: 1.0000e-04\n",
      "Epoch 715/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 103.9690 - val_loss: 575.0421 - lr: 1.0000e-04\n",
      "Epoch 716/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 103.9500 - val_loss: 574.9843 - lr: 1.0000e-04\n",
      "Epoch 717/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.9392 - val_loss: 574.9931 - lr: 1.0000e-04\n",
      "Epoch 718/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.9342 - val_loss: 574.9790 - lr: 1.0000e-04\n",
      "Epoch 719/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.9257 - val_loss: 574.9001 - lr: 1.0000e-04\n",
      "Epoch 720/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.9118 - val_loss: 574.8332 - lr: 1.0000e-04\n",
      "Epoch 721/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.9026 - val_loss: 574.8018 - lr: 1.0000e-04\n",
      "Epoch 722/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.8944 - val_loss: 574.8455 - lr: 1.0000e-04\n",
      "Epoch 723/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.8965 - val_loss: 574.7623 - lr: 1.0000e-04\n",
      "Epoch 724/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.8784 - val_loss: 574.6835 - lr: 1.0000e-04\n",
      "Epoch 725/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.8707 - val_loss: 574.7420 - lr: 1.0000e-04\n",
      "Epoch 726/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.8780 - val_loss: 574.5748 - lr: 1.0000e-04\n",
      "Epoch 727/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.8462 - val_loss: 574.4167 - lr: 1.0000e-04\n",
      "Epoch 728/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 103.8365 - val_loss: 574.5101 - lr: 1.0000e-04\n",
      "Epoch 729/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.8459 - val_loss: 574.5710 - lr: 1.0000e-04\n",
      "Epoch 730/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 103.8389 - val_loss: 574.4453 - lr: 1.0000e-04\n",
      "Epoch 731/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.8268 - val_loss: 574.4075 - lr: 1.0000e-04\n",
      "Epoch 732/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.8199 - val_loss: 574.4245 - lr: 1.0000e-04\n",
      "Epoch 733/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.8187 - val_loss: 574.3201 - lr: 1.0000e-04\n",
      "Epoch 734/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7972 - val_loss: 574.2227 - lr: 1.0000e-04\n",
      "Epoch 735/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7913 - val_loss: 574.3235 - lr: 1.0000e-04\n",
      "Epoch 736/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.8019 - val_loss: 574.2994 - lr: 1.0000e-04\n",
      "Epoch 737/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7838 - val_loss: 574.1591 - lr: 1.0000e-04\n",
      "Epoch 738/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7680 - val_loss: 574.1931 - lr: 1.0000e-04\n",
      "Epoch 739/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7715 - val_loss: 574.1346 - lr: 1.0000e-04\n",
      "Epoch 740/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7517 - val_loss: 574.0361 - lr: 1.0000e-04\n",
      "Epoch 741/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7429 - val_loss: 574.0209 - lr: 1.0000e-04\n",
      "Epoch 742/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7328 - val_loss: 573.9876 - lr: 1.0000e-04\n",
      "Epoch 743/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7314 - val_loss: 573.9830 - lr: 1.0000e-04\n",
      "Epoch 744/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7217 - val_loss: 573.8715 - lr: 1.0000e-04\n",
      "Epoch 745/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7050 - val_loss: 573.9522 - lr: 1.0000e-04\n",
      "Epoch 746/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7273 - val_loss: 573.7778 - lr: 1.0000e-04\n",
      "Epoch 747/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6780 - val_loss: 573.3841 - lr: 1.0000e-04\n",
      "Epoch 748/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6636 - val_loss: 573.6639 - lr: 1.0000e-04\n",
      "Epoch 749/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6963 - val_loss: 573.8761 - lr: 1.0000e-04\n",
      "Epoch 750/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6765 - val_loss: 573.6077 - lr: 1.0000e-04\n",
      "Epoch 751/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6669 - val_loss: 573.8348 - lr: 1.0000e-04\n",
      "Epoch 752/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7013 - val_loss: 573.6611 - lr: 1.0000e-04\n",
      "Epoch 753/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6350 - val_loss: 573.1768 - lr: 1.0000e-04\n",
      "Epoch 754/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6148 - val_loss: 573.3746 - lr: 1.0000e-04\n",
      "Epoch 755/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6394 - val_loss: 573.9563 - lr: 1.0000e-04\n",
      "Epoch 756/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.7150 - val_loss: 573.6995 - lr: 1.0000e-04\n",
      "Epoch 757/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6164 - val_loss: 573.0421 - lr: 1.0000e-04\n",
      "Epoch 758/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5821 - val_loss: 573.1278 - lr: 1.0000e-04\n",
      "Epoch 759/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5963 - val_loss: 573.7833 - lr: 1.0000e-04\n",
      "Epoch 760/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6996 - val_loss: 573.8397 - lr: 1.0000e-04\n",
      "Epoch 761/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6123 - val_loss: 573.1725 - lr: 1.0000e-04\n",
      "Epoch 762/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5691 - val_loss: 573.1530 - lr: 1.0000e-04\n",
      "Epoch 763/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5765 - val_loss: 573.6135 - lr: 1.0000e-04\n",
      "Epoch 764/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6497 - val_loss: 573.5519 - lr: 1.0000e-04\n",
      "Epoch 765/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5687 - val_loss: 572.9443 - lr: 1.0000e-04\n",
      "Epoch 766/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5387 - val_loss: 573.0048 - lr: 1.0000e-04\n",
      "Epoch 767/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5513 - val_loss: 573.5670 - lr: 1.0000e-04\n",
      "Epoch 768/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6422 - val_loss: 573.5087 - lr: 1.0000e-04\n",
      "Epoch 769/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5466 - val_loss: 572.9010 - lr: 1.0000e-04\n",
      "Epoch 770/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5120 - val_loss: 572.9515 - lr: 1.0000e-04\n",
      "Epoch 771/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5262 - val_loss: 573.4807 - lr: 1.0000e-04\n",
      "Epoch 772/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.6182 - val_loss: 573.4383 - lr: 1.0000e-04\n",
      "Epoch 773/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5253 - val_loss: 572.8307 - lr: 1.0000e-04\n",
      "Epoch 774/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4886 - val_loss: 572.8688 - lr: 1.0000e-04\n",
      "Epoch 775/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5002 - val_loss: 573.3427 - lr: 1.0000e-04\n",
      "Epoch 776/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5646 - val_loss: 573.1906 - lr: 1.0000e-04\n",
      "Epoch 777/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4856 - val_loss: 572.6685 - lr: 1.0000e-04\n",
      "Epoch 778/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4588 - val_loss: 572.7631 - lr: 1.0000e-04\n",
      "Epoch 779/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4759 - val_loss: 573.3953 - lr: 1.0000e-04\n",
      "Epoch 780/5000\n",
      "3/3 [==============================] - 4s 999ms/step - loss: 103.5854 - val_loss: 573.4471 - lr: 1.0000e-04\n",
      "Epoch 781/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 103.4944 - val_loss: 572.7593 - lr: 1.0000e-04\n",
      "Epoch 782/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4442 - val_loss: 572.7390 - lr: 1.0000e-04\n",
      "Epoch 783/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4496 - val_loss: 573.2110 - lr: 1.0000e-04\n",
      "Epoch 784/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.5385 - val_loss: 573.1883 - lr: 1.0000e-04\n",
      "Epoch 785/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4530 - val_loss: 572.6238 - lr: 1.0000e-04\n",
      "Epoch 786/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4136 - val_loss: 572.6779 - lr: 1.0000e-04\n",
      "Epoch 787/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4295 - val_loss: 573.0914 - lr: 1.0000e-04\n",
      "Epoch 788/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4908 - val_loss: 572.9683 - lr: 1.0000e-04\n",
      "Epoch 789/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4212 - val_loss: 572.5935 - lr: 1.0000e-04\n",
      "Epoch 790/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3940 - val_loss: 572.7089 - lr: 1.0000e-04\n",
      "Epoch 791/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4202 - val_loss: 573.0056 - lr: 1.0000e-04\n",
      "Epoch 792/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4471 - val_loss: 572.8296 - lr: 1.0000e-04\n",
      "Epoch 793/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3980 - val_loss: 572.6247 - lr: 1.0000e-04\n",
      "Epoch 794/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3837 - val_loss: 572.7759 - lr: 1.0000e-04\n",
      "Epoch 795/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.4255 - val_loss: 572.7868 - lr: 1.0000e-04\n",
      "Epoch 796/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3821 - val_loss: 572.5281 - lr: 1.0000e-04\n",
      "Epoch 797/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3576 - val_loss: 572.7094 - lr: 1.0000e-04\n",
      "Epoch 798/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3985 - val_loss: 572.9211 - lr: 1.0000e-04\n",
      "Epoch 799/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3972 - val_loss: 572.7527 - lr: 1.0000e-04\n",
      "Epoch 800/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3721 - val_loss: 572.7533 - lr: 1.0000e-04\n",
      "Epoch 801/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3804 - val_loss: 572.7999 - lr: 1.0000e-04\n",
      "Epoch 802/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3729 - val_loss: 572.7791 - lr: 1.0000e-04\n",
      "Epoch 803/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3782 - val_loss: 572.7524 - lr: 1.0000e-04\n",
      "Epoch 804/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3473 - val_loss: 572.6375 - lr: 1.0000e-04\n",
      "Epoch 805/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3381 - val_loss: 572.7794 - lr: 1.0000e-04\n",
      "Epoch 806/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3776 - val_loss: 572.7504 - lr: 1.0000e-04\n",
      "Epoch 807/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3293 - val_loss: 572.4963 - lr: 1.0000e-04\n",
      "Epoch 808/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3010 - val_loss: 572.5909 - lr: 1.0000e-04\n",
      "Epoch 809/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3337 - val_loss: 572.7201 - lr: 1.0000e-04\n",
      "Epoch 810/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3263 - val_loss: 572.6576 - lr: 1.0000e-04\n",
      "Epoch 811/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3196 - val_loss: 572.6810 - lr: 1.0000e-04\n",
      "Epoch 812/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3233 - val_loss: 572.6631 - lr: 1.0000e-04\n",
      "Epoch 813/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3151 - val_loss: 572.6439 - lr: 1.0000e-04\n",
      "Epoch 814/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3005 - val_loss: 572.6680 - lr: 1.0000e-04\n",
      "Epoch 815/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3123 - val_loss: 572.5256 - lr: 1.0000e-04\n",
      "Epoch 816/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2596 - val_loss: 572.3434 - lr: 1.0000e-04\n",
      "Epoch 817/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2454 - val_loss: 572.6057 - lr: 1.0000e-04\n",
      "Epoch 818/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.3210 - val_loss: 572.6715 - lr: 1.0000e-04\n",
      "Epoch 819/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2678 - val_loss: 572.4166 - lr: 1.0000e-04\n",
      "Epoch 820/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2470 - val_loss: 572.5160 - lr: 1.0000e-04\n",
      "Epoch 821/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2890 - val_loss: 572.4468 - lr: 1.0000e-04\n",
      "Epoch 822/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2311 - val_loss: 572.2213 - lr: 1.0000e-04\n",
      "Epoch 823/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2116 - val_loss: 572.4136 - lr: 1.0000e-04\n",
      "Epoch 824/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2729 - val_loss: 572.4351 - lr: 1.0000e-04\n",
      "Epoch 825/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2184 - val_loss: 572.1647 - lr: 1.0000e-04\n",
      "Epoch 826/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1884 - val_loss: 572.3020 - lr: 1.0000e-04\n",
      "Epoch 827/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2420 - val_loss: 572.4911 - lr: 1.0000e-04\n",
      "Epoch 828/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2362 - val_loss: 572.3125 - lr: 1.0000e-04\n",
      "Epoch 829/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1979 - val_loss: 572.2993 - lr: 1.0000e-04\n",
      "Epoch 830/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2064 - val_loss: 572.4572 - lr: 1.0000e-04\n",
      "Epoch 831/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2411 - val_loss: 572.3654 - lr: 1.0000e-04\n",
      "Epoch 832/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1846 - val_loss: 572.2003 - lr: 1.0000e-04\n",
      "Epoch 833/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1721 - val_loss: 572.4393 - lr: 1.0000e-04\n",
      "Epoch 834/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2529 - val_loss: 572.3676 - lr: 1.0000e-04\n",
      "Epoch 835/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1633 - val_loss: 572.0240 - lr: 1.0000e-04\n",
      "Epoch 836/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1385 - val_loss: 572.2172 - lr: 1.0000e-04\n",
      "Epoch 837/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.2257 - val_loss: 572.3441 - lr: 1.0000e-04\n",
      "Epoch 838/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1700 - val_loss: 572.0222 - lr: 1.0000e-04\n",
      "Epoch 839/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1351 - val_loss: 572.0825 - lr: 1.0000e-04\n",
      "Epoch 840/5000\n",
      "3/3 [==============================] - 3s 997ms/step - loss: 103.1543 - val_loss: 572.2734 - lr: 1.0000e-04\n",
      "Epoch 841/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1756 - val_loss: 572.2512 - lr: 1.0000e-04\n",
      "Epoch 842/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1482 - val_loss: 572.2492 - lr: 1.0000e-04\n",
      "Epoch 843/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1546 - val_loss: 572.3392 - lr: 1.0000e-04\n",
      "Epoch 844/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1550 - val_loss: 572.2664 - lr: 1.0000e-04\n",
      "Epoch 845/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1372 - val_loss: 572.2916 - lr: 1.0000e-04\n",
      "Epoch 846/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1724 - val_loss: 572.2659 - lr: 1.0000e-04\n",
      "Epoch 847/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1152 - val_loss: 572.0320 - lr: 1.0000e-04\n",
      "Epoch 848/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0957 - val_loss: 572.2042 - lr: 1.0000e-04\n",
      "Epoch 849/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1674 - val_loss: 572.1433 - lr: 1.0000e-04\n",
      "Epoch 850/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0990 - val_loss: 571.8503 - lr: 1.0000e-04\n",
      "Epoch 851/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0798 - val_loss: 572.0690 - lr: 1.0000e-04\n",
      "Epoch 852/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1374 - val_loss: 572.2025 - lr: 1.0000e-04\n",
      "Epoch 853/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1012 - val_loss: 571.9971 - lr: 1.0000e-04\n",
      "Epoch 854/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0753 - val_loss: 572.0604 - lr: 1.0000e-04\n",
      "Epoch 855/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0845 - val_loss: 572.1401 - lr: 1.0000e-04\n",
      "Epoch 856/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0844 - val_loss: 572.1124 - lr: 1.0000e-04\n",
      "Epoch 857/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0670 - val_loss: 572.1928 - lr: 1.0000e-04\n",
      "Epoch 858/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 103.1177 - val_loss: 572.1442 - lr: 1.0000e-04\n",
      "Epoch 859/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0621 - val_loss: 571.8403 - lr: 1.0000e-04\n",
      "Epoch 860/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0475 - val_loss: 572.0707 - lr: 1.0000e-04\n",
      "Epoch 861/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1199 - val_loss: 572.1438 - lr: 1.0000e-04\n",
      "Epoch 862/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 103.0462 - val_loss: 571.8401 - lr: 1.0000e-04\n",
      "Epoch 863/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0167 - val_loss: 571.9402 - lr: 1.0000e-04\n",
      "Epoch 864/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0475 - val_loss: 572.0430 - lr: 1.0000e-04\n",
      "Epoch 865/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0566 - val_loss: 571.9875 - lr: 1.0000e-04\n",
      "Epoch 866/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0298 - val_loss: 572.0273 - lr: 1.0000e-04\n",
      "Epoch 867/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0666 - val_loss: 572.1652 - lr: 1.0000e-04\n",
      "Epoch 868/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0586 - val_loss: 571.9808 - lr: 1.0000e-04\n",
      "Epoch 869/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0072 - val_loss: 571.8979 - lr: 1.0000e-04\n",
      "Epoch 870/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0122 - val_loss: 571.9805 - lr: 1.0000e-04\n",
      "Epoch 871/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 103.0261 - val_loss: 571.9663 - lr: 1.0000e-04\n",
      "Epoch 872/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0112 - val_loss: 572.0121 - lr: 1.0000e-04\n",
      "Epoch 873/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0384 - val_loss: 572.0418 - lr: 1.0000e-04\n",
      "Epoch 874/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 103.0137 - val_loss: 571.9115 - lr: 1.0000e-04\n",
      "Epoch 875/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9912 - val_loss: 571.8660 - lr: 1.0000e-04\n",
      "Epoch 876/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9817 - val_loss: 571.9329 - lr: 1.0000e-04\n",
      "Epoch 877/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0225 - val_loss: 571.9890 - lr: 1.0000e-04\n",
      "Epoch 878/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0106 - val_loss: 571.8210 - lr: 1.0000e-04\n",
      "Epoch 879/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9803 - val_loss: 571.8445 - lr: 1.0000e-04\n",
      "Epoch 880/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0079 - val_loss: 571.9420 - lr: 1.0000e-04\n",
      "Epoch 881/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9927 - val_loss: 571.7281 - lr: 1.0000e-04\n",
      "Epoch 882/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9663 - val_loss: 571.8111 - lr: 1.0000e-04\n",
      "Epoch 883/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0197 - val_loss: 571.9105 - lr: 1.0000e-04\n",
      "Epoch 884/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9753 - val_loss: 571.7509 - lr: 1.0000e-04\n",
      "Epoch 885/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9827 - val_loss: 571.8328 - lr: 1.0000e-04\n",
      "Epoch 886/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9601 - val_loss: 571.6358 - lr: 1.0000e-04\n",
      "Epoch 887/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9351 - val_loss: 571.8094 - lr: 1.0000e-04\n",
      "Epoch 888/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0440 - val_loss: 572.1385 - lr: 1.0000e-04\n",
      "Epoch 889/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0047 - val_loss: 571.6688 - lr: 1.0000e-04\n",
      "Epoch 890/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9435 - val_loss: 571.6322 - lr: 1.0000e-04\n",
      "Epoch 891/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9715 - val_loss: 571.8448 - lr: 1.0000e-04\n",
      "Epoch 892/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9303 - val_loss: 571.5472 - lr: 1.0000e-04\n",
      "Epoch 893/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.8993 - val_loss: 571.6047 - lr: 1.0000e-04\n",
      "Epoch 894/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9447 - val_loss: 571.7376 - lr: 1.0000e-04\n",
      "Epoch 895/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9212 - val_loss: 571.4829 - lr: 1.0000e-04\n",
      "Epoch 896/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.8781 - val_loss: 571.6955 - lr: 1.0000e-04\n",
      "Epoch 897/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0283 - val_loss: 572.3089 - lr: 1.0000e-04\n",
      "Epoch 898/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0078 - val_loss: 571.5028 - lr: 1.0000e-04\n",
      "Epoch 899/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9128 - val_loss: 571.4346 - lr: 1.0000e-04\n",
      "Epoch 900/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.8775 - val_loss: 571.8325 - lr: 1.0000e-04\n",
      "Epoch 901/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.0035 - val_loss: 571.8749 - lr: 1.0000e-04\n",
      "Epoch 902/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.8891 - val_loss: 571.4674 - lr: 1.0000e-04\n",
      "Epoch 903/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.8612 - val_loss: 571.6432 - lr: 1.0000e-04\n",
      "Epoch 904/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9426 - val_loss: 571.7745 - lr: 1.0000e-04\n",
      "Epoch 905/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9018 - val_loss: 571.5925 - lr: 1.0000e-04\n",
      "Epoch 906/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.8917 - val_loss: 571.7213 - lr: 1.0000e-04\n",
      "Epoch 907/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9262 - val_loss: 571.6525 - lr: 1.0000e-04\n",
      "Epoch 908/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.8722 - val_loss: 571.6921 - lr: 1.0000e-04\n",
      "Epoch 909/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9519 - val_loss: 572.0188 - lr: 1.0000e-04\n",
      "Epoch 910/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9256 - val_loss: 571.6368 - lr: 1.0000e-04\n",
      "Epoch 911/5000\n",
      "3/3 [==============================] - 6s 2s/step - loss: 102.8484 - val_loss: 571.6295 - lr: 1.0000e-04\n",
      "Epoch 912/5000\n",
      "3/3 [==============================] - 6s 2s/step - loss: 102.9317 - val_loss: 571.8418 - lr: 1.0000e-04\n",
      "Epoch 913/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 102.8396 - val_loss: 571.3386 - lr: 1.0000e-04\n",
      "Epoch 914/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 102.8064 - val_loss: 571.5834 - lr: 1.0000e-04\n",
      "Epoch 915/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 102.9064 - val_loss: 572.3828 - lr: 1.0000e-04\n",
      "Epoch 916/5000\n",
      "3/3 [==============================] - 8s 3s/step - loss: 102.9576 - val_loss: 571.8024 - lr: 1.0000e-04\n",
      "Epoch 917/5000\n",
      "3/3 [==============================] - 7s 2s/step - loss: 102.8524 - val_loss: 571.6483 - lr: 1.0000e-04\n",
      "Epoch 918/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 102.8548 - val_loss: 571.7566 - lr: 1.0000e-04\n",
      "Epoch 919/5000\n",
      "3/3 [==============================] - 7s 2s/step - loss: 102.8012 - val_loss: 571.5724 - lr: 1.0000e-04\n",
      "Epoch 920/5000\n",
      "3/3 [==============================] - 6s 2s/step - loss: 102.7889 - val_loss: 571.7391 - lr: 1.0000e-04\n",
      "Epoch 921/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 102.8087 - val_loss: 571.7711 - lr: 1.0000e-04\n",
      "Epoch 922/5000\n",
      "3/3 [==============================] - 6s 2s/step - loss: 102.7994 - val_loss: 571.8138 - lr: 1.0000e-04\n",
      "Epoch 923/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 102.8371 - val_loss: 571.7396 - lr: 1.0000e-04\n",
      "Epoch 924/5000\n",
      "3/3 [==============================] - 6s 2s/step - loss: 102.7833 - val_loss: 571.6446 - lr: 1.0000e-04\n",
      "Epoch 925/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 102.8018 - val_loss: 571.7846 - lr: 1.0000e-04\n",
      "Epoch 926/5000\n",
      "3/3 [==============================] - 6s 2s/step - loss: 102.7779 - val_loss: 571.6044 - lr: 1.0000e-04\n",
      "Epoch 927/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 102.7580 - val_loss: 571.7645 - lr: 1.0000e-04\n",
      "Epoch 928/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.8314 - val_loss: 572.0841 - lr: 1.0000e-04\n",
      "Epoch 929/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.7740 - val_loss: 571.6026 - lr: 1.0000e-04\n",
      "Epoch 930/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.7095 - val_loss: 571.7409 - lr: 1.0000e-04\n",
      "Epoch 931/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.7465 - val_loss: 571.8611 - lr: 1.0000e-04\n",
      "Epoch 932/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.7303 - val_loss: 571.7724 - lr: 1.0000e-04\n",
      "Epoch 933/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.7359 - val_loss: 572.0725 - lr: 1.0000e-04\n",
      "Epoch 934/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6889 - val_loss: 571.8764 - lr: 1.0000e-04\n",
      "Epoch 935/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6604 - val_loss: 572.0615 - lr: 1.0000e-04\n",
      "Epoch 936/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6390 - val_loss: 572.1780 - lr: 1.0000e-04\n",
      "Epoch 937/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6506 - val_loss: 572.0915 - lr: 1.0000e-04\n",
      "Epoch 938/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5665 - val_loss: 572.4377 - lr: 1.0000e-04\n",
      "Epoch 939/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.8019 - val_loss: 572.7270 - lr: 1.0000e-04\n",
      "Epoch 940/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4994 - val_loss: 572.8079 - lr: 1.0000e-04\n",
      "Epoch 941/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9147 - val_loss: 572.1346 - lr: 1.0000e-04\n",
      "Epoch 942/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.7201 - val_loss: 572.2205 - lr: 1.0000e-04\n",
      "Epoch 943/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.7224 - val_loss: 571.6500 - lr: 1.0000e-04\n",
      "Epoch 944/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.7108 - val_loss: 571.7432 - lr: 1.0000e-04\n",
      "Epoch 945/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.7172 - val_loss: 571.9180 - lr: 1.0000e-04\n",
      "Epoch 946/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.7488 - val_loss: 571.7834 - lr: 1.0000e-04\n",
      "Epoch 947/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6583 - val_loss: 571.7083 - lr: 1.0000e-04\n",
      "Epoch 948/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6535 - val_loss: 572.2710 - lr: 1.0000e-04\n",
      "Epoch 949/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5478 - val_loss: 572.5225 - lr: 1.0000e-04\n",
      "Epoch 950/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.9583 - val_loss: 572.7960 - lr: 1.0000e-04\n",
      "Epoch 951/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3925 - val_loss: 572.3926 - lr: 1.0000e-04\n",
      "Epoch 952/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6896 - val_loss: 572.8881 - lr: 1.0000e-04\n",
      "Epoch 953/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4912 - val_loss: 573.5188 - lr: 1.0000e-04\n",
      "Epoch 954/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.7981 - val_loss: 572.4254 - lr: 1.0000e-04\n",
      "Epoch 955/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 102.5199 - val_loss: 572.3293 - lr: 1.0000e-04\n",
      "Epoch 956/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4485 - val_loss: 572.6454 - lr: 1.0000e-04\n",
      "Epoch 957/5000\n",
      "3/3 [==============================] - 6s 2s/step - loss: 102.6150 - val_loss: 573.3403 - lr: 1.0000e-04\n",
      "Epoch 958/5000\n",
      "3/3 [==============================] - 7s 2s/step - loss: 102.8394 - val_loss: 572.3412 - lr: 1.0000e-04\n",
      "Epoch 959/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5005 - val_loss: 571.9075 - lr: 1.0000e-04\n",
      "Epoch 960/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6015 - val_loss: 572.1638 - lr: 1.0000e-04\n",
      "Epoch 961/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6584 - val_loss: 572.0103 - lr: 1.0000e-04\n",
      "Epoch 962/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5390 - val_loss: 572.1118 - lr: 1.0000e-04\n",
      "Epoch 963/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4539 - val_loss: 572.8987 - lr: 1.0000e-04\n",
      "Epoch 964/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6158 - val_loss: 574.6194 - lr: 1.0000e-04\n",
      "Epoch 965/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 103.1495 - val_loss: 572.5374 - lr: 1.0000e-04\n",
      "Epoch 966/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6760 - val_loss: 571.7556 - lr: 1.0000e-04\n",
      "Epoch 967/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6632 - val_loss: 571.2226 - lr: 1.0000e-04\n",
      "Epoch 968/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6787 - val_loss: 571.4589 - lr: 1.0000e-04\n",
      "Epoch 969/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6409 - val_loss: 571.1808 - lr: 1.0000e-04\n",
      "Epoch 970/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6326 - val_loss: 571.6489 - lr: 1.0000e-04\n",
      "Epoch 971/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6999 - val_loss: 571.4697 - lr: 1.0000e-04\n",
      "Epoch 972/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6445 - val_loss: 571.1788 - lr: 1.0000e-04\n",
      "Epoch 973/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6467 - val_loss: 571.6875 - lr: 1.0000e-04\n",
      "Epoch 974/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6256 - val_loss: 571.3484 - lr: 1.0000e-04\n",
      "Epoch 975/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6063 - val_loss: 571.7479 - lr: 1.0000e-04\n",
      "Epoch 976/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6907 - val_loss: 571.7226 - lr: 1.0000e-04\n",
      "Epoch 977/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5909 - val_loss: 571.3242 - lr: 1.0000e-04\n",
      "Epoch 978/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6080 - val_loss: 571.7280 - lr: 1.0000e-04\n",
      "Epoch 979/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5854 - val_loss: 571.5987 - lr: 1.0000e-04\n",
      "Epoch 980/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6092 - val_loss: 571.5609 - lr: 1.0000e-04\n",
      "Epoch 981/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5653 - val_loss: 571.5278 - lr: 1.0000e-04\n",
      "Epoch 982/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 102.6234 - val_loss: 571.9800 - lr: 1.0000e-04\n",
      "Epoch 983/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5685 - val_loss: 571.5249 - lr: 1.0000e-04\n",
      "Epoch 984/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5541 - val_loss: 571.9862 - lr: 1.0000e-04\n",
      "Epoch 985/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6260 - val_loss: 571.6418 - lr: 1.0000e-04\n",
      "Epoch 986/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5592 - val_loss: 571.4053 - lr: 1.0000e-04\n",
      "Epoch 987/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5651 - val_loss: 571.9722 - lr: 1.0000e-04\n",
      "Epoch 988/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5428 - val_loss: 571.5403 - lr: 1.0000e-04\n",
      "Epoch 989/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5435 - val_loss: 571.9432 - lr: 1.0000e-04\n",
      "Epoch 990/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.6425 - val_loss: 572.0255 - lr: 1.0000e-04\n",
      "Epoch 991/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5194 - val_loss: 571.4925 - lr: 1.0000e-04\n",
      "Epoch 992/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5089 - val_loss: 571.9999 - lr: 1.0000e-04\n",
      "Epoch 993/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5479 - val_loss: 571.8404 - lr: 1.0000e-04\n",
      "Epoch 994/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5198 - val_loss: 571.8812 - lr: 1.0000e-04\n",
      "Epoch 995/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 102.5193 - val_loss: 571.6381 - lr: 1.0000e-04\n",
      "Epoch 996/5000\n",
      "3/3 [==============================] - 6s 1s/step - loss: 102.4899 - val_loss: 571.7847 - lr: 1.0000e-04\n",
      "Epoch 997/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 102.5693 - val_loss: 572.0876 - lr: 1.0000e-04\n",
      "Epoch 998/5000\n",
      "3/3 [==============================] - 8s 2s/step - loss: 102.4718 - val_loss: 571.6613 - lr: 1.0000e-04\n",
      "Epoch 999/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4874 - val_loss: 572.2454 - lr: 1.0000e-04\n",
      "Epoch 1000/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4872 - val_loss: 571.7615 - lr: 1.0000e-04\n",
      "Epoch 1001/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4778 - val_loss: 572.2069 - lr: 1.0000e-04\n",
      "Epoch 1002/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5928 - val_loss: 572.0785 - lr: 1.0000e-04\n",
      "Epoch 1003/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4452 - val_loss: 571.4760 - lr: 1.0000e-04\n",
      "Epoch 1004/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4304 - val_loss: 572.4805 - lr: 1.0000e-04\n",
      "Epoch 1005/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4915 - val_loss: 571.8200 - lr: 1.0000e-04\n",
      "Epoch 1006/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4493 - val_loss: 571.7392 - lr: 1.0000e-04\n",
      "Epoch 1007/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5111 - val_loss: 572.0478 - lr: 1.0000e-04\n",
      "Epoch 1008/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4025 - val_loss: 571.6105 - lr: 1.0000e-04\n",
      "Epoch 1009/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3942 - val_loss: 572.2778 - lr: 1.0000e-04\n",
      "Epoch 1010/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4547 - val_loss: 571.8833 - lr: 1.0000e-04\n",
      "Epoch 1011/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4285 - val_loss: 572.1851 - lr: 1.0000e-04\n",
      "Epoch 1012/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5127 - val_loss: 572.1909 - lr: 1.0000e-04\n",
      "Epoch 1013/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3692 - val_loss: 571.4177 - lr: 1.0000e-04\n",
      "Epoch 1014/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3698 - val_loss: 572.5142 - lr: 1.0000e-04\n",
      "Epoch 1015/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.5409 - val_loss: 572.1731 - lr: 1.0000e-04\n",
      "Epoch 1016/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4085 - val_loss: 571.5927 - lr: 1.0000e-04\n",
      "Epoch 1017/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3632 - val_loss: 572.8808 - lr: 1.0000e-04\n",
      "Epoch 1018/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4424 - val_loss: 571.8015 - lr: 1.0000e-04\n",
      "Epoch 1019/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3871 - val_loss: 571.5786 - lr: 1.0000e-04\n",
      "Epoch 1020/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3682 - val_loss: 572.9165 - lr: 1.0000e-04\n",
      "Epoch 1021/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4074 - val_loss: 571.9675 - lr: 1.0000e-04\n",
      "Epoch 1022/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3442 - val_loss: 572.0377 - lr: 1.0000e-04\n",
      "Epoch 1023/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3831 - val_loss: 572.4167 - lr: 1.0000e-04\n",
      "Epoch 1024/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3090 - val_loss: 571.6976 - lr: 1.0000e-04\n",
      "Epoch 1025/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3066 - val_loss: 572.9763 - lr: 1.0000e-04\n",
      "Epoch 1026/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4933 - val_loss: 572.9101 - lr: 1.0000e-04\n",
      "Epoch 1027/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3459 - val_loss: 571.9072 - lr: 1.0000e-04\n",
      "Epoch 1028/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2958 - val_loss: 572.8242 - lr: 1.0000e-04\n",
      "Epoch 1029/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3226 - val_loss: 572.2588 - lr: 1.0000e-04\n",
      "Epoch 1030/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3088 - val_loss: 572.6194 - lr: 1.0000e-04\n",
      "Epoch 1031/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.4248 - val_loss: 573.1107 - lr: 1.0000e-04\n",
      "Epoch 1032/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2809 - val_loss: 571.9214 - lr: 1.0000e-04\n",
      "Epoch 1033/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2477 - val_loss: 572.8654 - lr: 1.0000e-04\n",
      "Epoch 1034/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3621 - val_loss: 572.3146 - lr: 1.0000e-04\n",
      "Epoch 1035/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3106 - val_loss: 572.0714 - lr: 1.0000e-04\n",
      "Epoch 1036/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2698 - val_loss: 573.9904 - lr: 1.0000e-04\n",
      "Epoch 1037/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.3465 - val_loss: 572.8264 - lr: 1.0000e-04\n",
      "Epoch 1038/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2812 - val_loss: 572.4705 - lr: 1.0000e-04\n",
      "Epoch 1039/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2837 - val_loss: 573.4150 - lr: 1.0000e-04\n",
      "Epoch 1040/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2387 - val_loss: 573.2074 - lr: 1.0000e-04\n",
      "Epoch 1041/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2870 - val_loss: 572.9427 - lr: 1.0000e-04\n",
      "Epoch 1042/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2388 - val_loss: 572.5466 - lr: 1.0000e-04\n",
      "Epoch 1043/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2449 - val_loss: 573.2835 - lr: 1.0000e-04\n",
      "Epoch 1044/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2224 - val_loss: 572.9973 - lr: 1.0000e-04\n",
      "Epoch 1045/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2161 - val_loss: 572.9905 - lr: 1.0000e-04\n",
      "Epoch 1046/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2068 - val_loss: 573.3498 - lr: 1.0000e-04\n",
      "Epoch 1047/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2984 - val_loss: 573.4776 - lr: 1.0000e-04\n",
      "Epoch 1048/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1977 - val_loss: 572.8034 - lr: 1.0000e-04\n",
      "Epoch 1049/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1871 - val_loss: 573.2785 - lr: 1.0000e-04\n",
      "Epoch 1050/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1869 - val_loss: 573.5411 - lr: 1.0000e-04\n",
      "Epoch 1051/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2686 - val_loss: 573.5139 - lr: 1.0000e-04\n",
      "Epoch 1052/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1731 - val_loss: 572.7800 - lr: 1.0000e-04\n",
      "Epoch 1053/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1501 - val_loss: 573.3947 - lr: 1.0000e-04\n",
      "Epoch 1054/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1301 - val_loss: 573.2371 - lr: 1.0000e-04\n",
      "Epoch 1055/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1719 - val_loss: 573.4105 - lr: 1.0000e-04\n",
      "Epoch 1056/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1174 - val_loss: 573.7975 - lr: 1.0000e-04\n",
      "Epoch 1057/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.2055 - val_loss: 573.3723 - lr: 1.0000e-04\n",
      "Epoch 1058/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1477 - val_loss: 572.5547 - lr: 1.0000e-04\n",
      "Epoch 1059/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.0969 - val_loss: 574.4792 - lr: 1.0000e-04\n",
      "Epoch 1060/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1233 - val_loss: 573.5215 - lr: 1.0000e-04\n",
      "Epoch 1061/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.0831 - val_loss: 573.6917 - lr: 1.0000e-04\n",
      "Epoch 1062/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1512 - val_loss: 573.3541 - lr: 1.0000e-04\n",
      "Epoch 1063/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1235 - val_loss: 572.7633 - lr: 1.0000e-04\n",
      "Epoch 1064/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.0630 - val_loss: 575.4562 - lr: 1.0000e-04\n",
      "Epoch 1065/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.1608 - val_loss: 573.9357 - lr: 1.0000e-04\n",
      "Epoch 1066/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.0667 - val_loss: 573.0552 - lr: 1.0000e-04\n",
      "Epoch 1067/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.0217 - val_loss: 574.7819 - lr: 1.0000e-04\n",
      "Epoch 1068/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.0578 - val_loss: 573.6332 - lr: 1.0000e-04\n",
      "Epoch 1069/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.0473 - val_loss: 573.9290 - lr: 1.0000e-04\n",
      "Epoch 1070/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.0870 - val_loss: 574.5577 - lr: 1.0000e-04\n",
      "Epoch 1071/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 102.0136 - val_loss: 573.3264 - lr: 1.0000e-04\n",
      "Epoch 1072/5000\n",
      "3/3 [==============================] - ETA: 0s - loss: 101.9753\n",
      "Epoch 1072: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "3/3 [==============================] - 5s 2s/step - loss: 101.9753 - val_loss: 574.6866 - lr: 1.0000e-04\n",
      "Epoch 1073/5000\n",
      "3/3 [==============================] - 7s 2s/step - loss: 101.9722 - val_loss: 574.6756 - lr: 1.0000e-05\n",
      "Epoch 1074/5000\n",
      "3/3 [==============================] - 6s 2s/step - loss: 101.9713 - val_loss: 574.4884 - lr: 1.0000e-05\n",
      "Epoch 1075/5000\n",
      "3/3 [==============================] - 6s 1s/step - loss: 101.9652 - val_loss: 574.2932 - lr: 1.0000e-05\n",
      "Epoch 1076/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9585 - val_loss: 574.2154 - lr: 1.0000e-05\n",
      "Epoch 1077/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 101.9531 - val_loss: 574.2331 - lr: 1.0000e-05\n",
      "Epoch 1078/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9497 - val_loss: 574.2892 - lr: 1.0000e-05\n",
      "Epoch 1079/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9481 - val_loss: 574.3245 - lr: 1.0000e-05\n",
      "Epoch 1080/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9466 - val_loss: 574.3416 - lr: 1.0000e-05\n",
      "Epoch 1081/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9450 - val_loss: 574.3463 - lr: 1.0000e-05\n",
      "Epoch 1082/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9435 - val_loss: 574.3693 - lr: 1.0000e-05\n",
      "Epoch 1083/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9428 - val_loss: 574.4156 - lr: 1.0000e-05\n",
      "Epoch 1084/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9421 - val_loss: 574.4307 - lr: 1.0000e-05\n",
      "Epoch 1085/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9410 - val_loss: 574.4523 - lr: 1.0000e-05\n",
      "Epoch 1086/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9412 - val_loss: 574.4896 - lr: 1.0000e-05\n",
      "Epoch 1087/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9408 - val_loss: 574.4879 - lr: 1.0000e-05\n",
      "Epoch 1088/5000\n",
      "3/3 [==============================] - 3s 979ms/step - loss: 101.9380 - val_loss: 574.4901 - lr: 1.0000e-05\n",
      "Epoch 1089/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9356 - val_loss: 574.5074 - lr: 1.0000e-05\n",
      "Epoch 1090/5000\n",
      "3/3 [==============================] - 3s 965ms/step - loss: 101.9336 - val_loss: 574.5248 - lr: 1.0000e-05\n",
      "Epoch 1091/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 101.9336 - val_loss: 574.5250 - lr: 1.0000e-05\n",
      "Epoch 1092/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9342 - val_loss: 574.4421 - lr: 1.0000e-05\n",
      "Epoch 1093/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9341 - val_loss: 574.4075 - lr: 1.0000e-05\n",
      "Epoch 1094/5000\n",
      "3/3 [==============================] - 8s 3s/step - loss: 101.9321 - val_loss: 574.4645 - lr: 1.0000e-05\n",
      "Epoch 1095/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 101.9295 - val_loss: 574.5480 - lr: 1.0000e-05\n",
      "Epoch 1096/5000\n",
      "3/3 [==============================] - 3s 1s/step - loss: 101.9273 - val_loss: 574.5434 - lr: 1.0000e-05\n",
      "Epoch 1097/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9253 - val_loss: 574.4892 - lr: 1.0000e-05\n",
      "Epoch 1098/5000\n",
      "3/3 [==============================] - 7s 2s/step - loss: 101.9260 - val_loss: 574.4894 - lr: 1.0000e-05\n",
      "Epoch 1099/5000\n",
      "3/3 [==============================] - 7s 2s/step - loss: 101.9248 - val_loss: 574.4875 - lr: 1.0000e-05\n",
      "Epoch 1100/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9217 - val_loss: 574.5096 - lr: 1.0000e-05\n",
      "Epoch 1101/5000\n",
      "3/3 [==============================] - 6s 2s/step - loss: 101.9197 - val_loss: 574.5579 - lr: 1.0000e-05\n",
      "Epoch 1102/5000\n",
      "3/3 [==============================] - 6s 2s/step - loss: 101.9185 - val_loss: 574.5253 - lr: 1.0000e-05\n",
      "Epoch 1103/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9183 - val_loss: 574.4603 - lr: 1.0000e-05\n",
      "Epoch 1104/5000\n",
      "3/3 [==============================] - 6s 2s/step - loss: 101.9192 - val_loss: 574.4966 - lr: 1.0000e-05\n",
      "Epoch 1105/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9190 - val_loss: 574.5728 - lr: 1.0000e-05\n",
      "Epoch 1106/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9171 - val_loss: 574.5649 - lr: 1.0000e-05\n",
      "Epoch 1107/5000\n",
      "3/3 [==============================] - 5s 1s/step - loss: 101.9151 - val_loss: 574.5099 - lr: 1.0000e-05\n",
      "Epoch 1108/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9155 - val_loss: 574.5278 - lr: 1.0000e-05\n",
      "Epoch 1109/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9141 - val_loss: 574.5454 - lr: 1.0000e-05\n",
      "Epoch 1110/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9105 - val_loss: 574.5865 - lr: 1.0000e-05\n",
      "Epoch 1111/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9094 - val_loss: 574.5842 - lr: 1.0000e-05\n",
      "Epoch 1112/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9092 - val_loss: 574.5299 - lr: 1.0000e-05\n",
      "Epoch 1113/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9104 - val_loss: 574.5737 - lr: 1.0000e-05\n",
      "Epoch 1114/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9114 - val_loss: 574.6080 - lr: 1.0000e-05\n",
      "Epoch 1115/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9063 - val_loss: 574.5233 - lr: 1.0000e-05\n",
      "Epoch 1116/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.9012 - val_loss: 574.4797 - lr: 1.0000e-05\n",
      "Epoch 1117/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8992 - val_loss: 574.5067 - lr: 1.0000e-05\n",
      "Epoch 1118/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8985 - val_loss: 574.5219 - lr: 1.0000e-05\n",
      "Epoch 1119/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8974 - val_loss: 574.5529 - lr: 1.0000e-05\n",
      "Epoch 1120/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8975 - val_loss: 574.5873 - lr: 1.0000e-05\n",
      "Epoch 1121/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8978 - val_loss: 574.5598 - lr: 1.0000e-05\n",
      "Epoch 1122/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8967 - val_loss: 574.5660 - lr: 1.0000e-05\n",
      "Epoch 1123/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8951 - val_loss: 574.5806 - lr: 1.0000e-05\n",
      "Epoch 1124/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8917 - val_loss: 574.5339 - lr: 1.0000e-05\n",
      "Epoch 1125/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8897 - val_loss: 574.5816 - lr: 1.0000e-05\n",
      "Epoch 1126/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8912 - val_loss: 574.5974 - lr: 1.0000e-05\n",
      "Epoch 1127/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8897 - val_loss: 574.5272 - lr: 1.0000e-05\n",
      "Epoch 1128/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8867 - val_loss: 574.5237 - lr: 1.0000e-05\n",
      "Epoch 1129/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8852 - val_loss: 574.5824 - lr: 1.0000e-05\n",
      "Epoch 1130/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8855 - val_loss: 574.5812 - lr: 1.0000e-05\n",
      "Epoch 1131/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8848 - val_loss: 574.5926 - lr: 1.0000e-05\n",
      "Epoch 1132/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8839 - val_loss: 574.6257 - lr: 1.0000e-05\n",
      "Epoch 1133/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8824 - val_loss: 574.5974 - lr: 1.0000e-05\n",
      "Epoch 1134/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8816 - val_loss: 574.6015 - lr: 1.0000e-05\n",
      "Epoch 1135/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8805 - val_loss: 574.6170 - lr: 1.0000e-05\n",
      "Epoch 1136/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8773 - val_loss: 574.5844 - lr: 1.0000e-05\n",
      "Epoch 1137/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8750 - val_loss: 574.6321 - lr: 1.0000e-05\n",
      "Epoch 1138/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 101.8774 - val_loss: 574.6406 - lr: 1.0000e-05\n",
      "Epoch 1139/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 101.8755 - val_loss: 574.5684 - lr: 1.0000e-05\n",
      "Epoch 1140/5000\n",
      "3/3 [==============================] - 7s 2s/step - loss: 101.8727 - val_loss: 574.5667 - lr: 1.0000e-05\n",
      "Epoch 1141/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8710 - val_loss: 574.6332 - lr: 1.0000e-05\n",
      "Epoch 1142/5000\n",
      "3/3 [==============================] - 5s 2s/step - loss: 101.8706 - val_loss: 574.6454 - lr: 1.0000e-05\n",
      "Epoch 1143/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8704 - val_loss: 574.6604 - lr: 1.0000e-05\n",
      "Epoch 1144/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8702 - val_loss: 574.6967 - lr: 1.0000e-05\n",
      "Epoch 1145/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8686 - val_loss: 574.6744 - lr: 1.0000e-05\n",
      "Epoch 1146/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8665 - val_loss: 574.6815 - lr: 1.0000e-05\n",
      "Epoch 1147/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8656 - val_loss: 574.6736 - lr: 1.0000e-05\n",
      "Epoch 1148/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8641 - val_loss: 574.6204 - lr: 1.0000e-05\n",
      "Epoch 1149/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8621 - val_loss: 574.6884 - lr: 1.0000e-05\n",
      "Epoch 1150/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8628 - val_loss: 574.6795 - lr: 1.0000e-05\n",
      "Epoch 1151/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8627 - val_loss: 574.6024 - lr: 1.0000e-05\n",
      "Epoch 1152/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8625 - val_loss: 574.7028 - lr: 1.0000e-05\n",
      "Epoch 1153/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8620 - val_loss: 574.7209 - lr: 1.0000e-05\n",
      "Epoch 1154/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8569 - val_loss: 574.6912 - lr: 1.0000e-05\n",
      "Epoch 1155/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8535 - val_loss: 574.6833 - lr: 1.0000e-05\n",
      "Epoch 1156/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8540 - val_loss: 574.6866 - lr: 1.0000e-05\n",
      "Epoch 1157/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8578 - val_loss: 574.7541 - lr: 1.0000e-05\n",
      "Epoch 1158/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8563 - val_loss: 574.7151 - lr: 1.0000e-05\n",
      "Epoch 1159/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8520 - val_loss: 574.7405 - lr: 1.0000e-05\n",
      "Epoch 1160/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8494 - val_loss: 574.7738 - lr: 1.0000e-05\n",
      "Epoch 1161/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8499 - val_loss: 574.7493 - lr: 1.0000e-05\n",
      "Epoch 1162/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8531 - val_loss: 574.7657 - lr: 1.0000e-05\n",
      "Epoch 1163/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8525 - val_loss: 574.7686 - lr: 1.0000e-05\n",
      "Epoch 1164/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8489 - val_loss: 574.8190 - lr: 1.0000e-05\n",
      "Epoch 1165/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8479 - val_loss: 574.8029 - lr: 1.0000e-05\n",
      "Epoch 1166/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8461 - val_loss: 574.7305 - lr: 1.0000e-05\n",
      "Epoch 1167/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8453 - val_loss: 574.8452 - lr: 1.0000e-05\n",
      "Epoch 1168/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8455 - val_loss: 574.8291 - lr: 1.0000e-05\n",
      "Epoch 1169/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8414 - val_loss: 574.7688 - lr: 1.0000e-05\n",
      "Epoch 1170/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8375 - val_loss: 574.8478 - lr: 1.0000e-05\n",
      "Epoch 1171/5000\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8397 - val_loss: 574.8438 - lr: 1.0000e-05\n",
      "Epoch 1172/5000\n",
      "3/3 [==============================] - ETA: 0s - loss: 101.8403\n",
      "Epoch 1172: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "3/3 [==============================] - 4s 1s/step - loss: 101.8403 - val_loss: 574.7617 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1332968e0>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup instance of nbeatsblock\n",
    "nbeats_block_layer = NBeatsBlock(input_size=INPUT_SIZE,\n",
    "                                 theta_size=THETA_SIZE,\n",
    "                                 horizon=HORIZON,\n",
    "                                 n_neurons=N_NEURONS,\n",
    "                                 n_layers=N_LAYERS,\n",
    "                                 name=\"InitialBlock\")\n",
    "\n",
    "# create input to stack layer\n",
    "stack_input = layers.Input(shape=(INPUT_SIZE), name=\"stack_input\")\n",
    "\n",
    "# create initial backcast and forecast input\n",
    "residuals, forecast = nbeats_block_layer(stack_input)\n",
    "\n",
    "# create stacks of block layers\n",
    "for i, _ in enumerate(range(N_STACKS - 1)):\n",
    "    backcast, block_forecast = NBeatsBlock(input_size=INPUT_SIZE,\n",
    "                                           theta_size=THETA_SIZE,\n",
    "                                           horizon=HORIZON,\n",
    "                                           n_neurons=N_NEURONS,\n",
    "                                           n_layers=N_LAYERS,\n",
    "                                           name=f\"NBeatsBlock_{i}\")(residuals)\n",
    "    residuals = layers.subtract([residuals, backcast], name=f\"subtract_{i}\")\n",
    "    forecast = layers.add([forecast, block_forecast], name=f\"add_{i}\")\n",
    "\n",
    "model_7 = tf.keras.Model(inputs=stack_input,\n",
    "                         outputs=forecast,\n",
    "                         name=\"model_7_nbeats\")\n",
    "\n",
    "model_7.compile(loss=\"mae\", optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "model_7.fit(train_dataset,\n",
    "            epochs=N_EPOCHS,\n",
    "            validation_data=test_dataset,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                 patience=200,\n",
    "                                                 restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                                     patience=100,\n",
    "                                                     verbose=1)\n",
    "            ])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_8 Ensemble Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below cell is the function to create ensemble models. but my pc would take ages to run that so im skipping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_models(horizon=HORIZON, \n",
    "                        train_data=train_dataset,\n",
    "                        test_data=test_dataset,\n",
    "                        num_iter=10, \n",
    "                        num_epochs=100, \n",
    "                        loss_fns=[\"mae\", \"mse\", \"mape\"]):\n",
    "  \"\"\"\n",
    "  Returns a list of num_iter models each trained on MAE, MSE and MAPE loss.\n",
    "\n",
    "  For example, if num_iter=10, a list of 30 trained models will be returned:\n",
    "  10 * len([\"mae\", \"mse\", \"mape\"]).\n",
    "  \"\"\"\n",
    "  # Make empty list for trained ensemble models\n",
    "  ensemble_models = []\n",
    "\n",
    "  # Create num_iter number of models per loss function\n",
    "  for i in range(num_iter):\n",
    "    # Build and fit a new model with a different loss function\n",
    "    for loss_function in loss_fns:\n",
    "      print(f\"Optimizing model by reducing: {loss_function} for {num_epochs} epochs, model number: {i}\")\n",
    "\n",
    "      # Construct a simple model (similar to model_1)\n",
    "      model = tf.keras.Sequential([\n",
    "        # Initialize layers with normal (Gaussian) distribution so we can use the models for prediction\n",
    "        # interval estimation later: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal\n",
    "        layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"), \n",
    "        layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"),\n",
    "        layers.Dense(HORIZON)                                 \n",
    "      ])\n",
    "\n",
    "      # Compile simple model with current loss function\n",
    "      model.compile(loss=loss_function,\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    metrics=[\"mae\", \"mse\"])\n",
    "      \n",
    "      # Fit model\n",
    "      model.fit(train_data,\n",
    "                epochs=num_epochs,\n",
    "                verbose=0,\n",
    "                validation_data=test_data,\n",
    "                # Add callbacks to prevent training from going/stalling for too long\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                            patience=200,\n",
    "                                                            restore_best_weights=True),\n",
    "                           tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                                                patience=100,\n",
    "                                                                verbose=1)])\n",
    "      \n",
    "      # Append fitted model to list of ensemble models\n",
    "      ensemble_models.append(model)\n",
    "\n",
    "  return ensemble_models # return list of trained models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
